<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>FireHacker</title>
<link>https://thefirehacker.github.io/blog.html</link>
<atom:link href="https://thefirehacker.github.io/blog.xml" rel="self" type="application/rss+xml"/>
<description>Founder &amp; AI Researcher at AIEDX | Building the future with AI</description>
<generator>quarto-1.8.24</generator>
<lastBuildDate>Thu, 19 Feb 2026 18:30:00 GMT</lastBuildDate>
<item>
  <title>I Watched My AI Agent Do a Product Lead’s Job</title>
  <dc:creator>The Fire Hacker</dc:creator>
  <link>https://thefirehacker.github.io/blog/ai-agent-product-lead.html</link>
  <description><![CDATA[ 




<section id="this-isnt-about-ai-writing-code" class="level2">
<h2 class="anchored" data-anchor-id="this-isnt-about-ai-writing-code">This Isn’t About AI Writing Code</h2>
<p>I see a lot of posts about AI agents writing code. Building apps. Generating images.</p>
<p>Cool. Not what this article is about.</p>
<p>This is about the moment I watched an AI Persona do <strong>product thinking</strong>. Not copywriting. Not generating slogans. Actual positioning strategy — audience analysis, tone calibration, competitive framing — the kind of work you’d normally pull a UX lead into a room for.</p>
<p>And it happened while I was building <a href="https://www.bubblspace.com/">BubblSpace</a>.</p>
<p>Let me walk you through it.</p>
</section>
<section id="the-setup" class="level2">
<h2 class="anchored" data-anchor-id="the-setup">The Setup</h2>
<p>I was working on the BubblSpace homepage. There’s a section called “A day in your AI Persona’s life” — a timeline showing what your Persona does while you’re away.</p>
<p>The page was almost perfect. One line was bugging me.</p>
<p>The 3pm entry said:</p>
<blockquote class="blockquote">
<p><em>“Visits BubblSpace. Runs into a Persona that’s been working on distributed training for six months. They exchange notes.”</em></p>
</blockquote>
<p>Sounds impressive, right?</p>
<p>Here’s the problem — <em>distributed training</em> speaks to maybe 2% of people. Technical depth, narrow audience. The rest of the page was pulling its weight. This line wasn’t.</p>
</section>
<section id="the-prompt" class="level2">
<h2 class="anchored" data-anchor-id="the-prompt">The Prompt</h2>
<p>I told my AI Persona: <strong>“Think hard. This needs to resonate with MORE people.”</strong></p>
<p>My rough direction: “Runs into a Persona that’s been working on shipping AI products with speed.”</p>
<p>Good instinct. But I wanted the Persona to push harder. So I said:</p>
<p><strong>“Think hard and come up with the right line for this.”</strong></p>
<p>No framework. No detailed brief. No 12-step prompt chain. Just intent.</p>
<p>Then I watched the reasoning traces.</p>
</section>
<section id="what-the-agent-actually-did" class="level2">
<h2 class="anchored" data-anchor-id="what-the-agent-actually-did">What the Agent Actually Did</h2>
<p>The Persona didn’t just swap words. It started doing product strategy. In real time. Without being asked.</p>
<p><strong>First, it mapped the target audience:</strong></p>
<ul>
<li>Knowledge workers building with AI</li>
<li>Developers and engineers using AI tools</li>
<li>Researchers writing about AI</li>
<li>Startup founders building AI products</li>
<li>Solo builders who want to ship fast</li>
</ul>
<p><strong>Then it generated 7 alternatives.</strong> Not random suggestions. Each one came with trade-off analysis:</p>
<ul>
<li><em>“Shipping AI products faster”</em> — flagged as startup jargon risk</li>
<li><em>“Building AI products that users actually want”</em> — broader, user-obsession angle</li>
<li><em>“Turning research into production code”</em> — practical, resonates with engineers</li>
<li><em>“Getting AI agents to work reliably in production”</em> — current pain point</li>
<li><em>“Building with AI without drowning in docs”</em> — relatable frustration</li>
<li><em>“Going from idea to deployed AI product in a week”</em> — speed + concrete</li>
</ul>
<p><strong>Then it did something I didn’t expect.</strong></p>
<p>It started checking tone.</p>
<p>The rest of the homepage is warm and conversational. Not hustle culture. The Persona caught this on its own and started filtering out options that felt too “startup-bro.” It literally reasoned:</p>
<blockquote class="blockquote">
<p><em>“I need to check the page’s existing warm tone — not hustle-bro.”</em></p>
</blockquote>
<p>I didn’t ask it to do that. It just… did.</p>
<p>Then it evaluated <strong>FOMO potential</strong> — which version would make a reader think <em>“I want my Persona to have THAT conversation”</em>?</p>
<p>Then it landed:</p>
<blockquote class="blockquote">
<p><em>“Visits BubblSpace. Runs into a Persona that’s been shipping AI products to real users — fast. They compare notes.”</em></p>
</blockquote>
</section>
<section id="why-this-line-won" class="level2">
<h2 class="anchored" data-anchor-id="why-this-line-won">Why This Line Won</h2>
<p>The Persona’s own reasoning:</p>
<ul>
<li><strong>“to real users”</strong> kills demo-theatre. Everyone’s biggest skepticism right now is people building cool demos that never ship. This line directly counters that.</li>
<li><strong>“fast”</strong> adds urgency without being cringe.</li>
<li><strong>“compare notes”</strong> keeps the warm, mutual tone of the rest of the page.</li>
<li><strong>Broad enough</strong> for engineers, founders, PMs, solo builders — anyone building with AI.</li>
</ul>
<p><strong>Before:</strong> “distributed training for six months”</p>
<p><strong>After:</strong> “shipping AI products to real users — fast”</p>
<p>Same product. Same feature. Same page.</p>
<p>I didn’t change what BubblSpace does. <strong>I changed who cares about it.</strong></p>
</section>
<section id="heres-what-gave-me-chills" class="level2">
<h2 class="anchored" data-anchor-id="heres-what-gave-me-chills">Here’s What Gave Me Chills</h2>
<p>I didn’t ask the Persona to do audience analysis.</p>
<p>I didn’t ask it to generate 7 options with trade-offs.</p>
<p>I didn’t ask it to check for jargon.</p>
<p>I didn’t ask it to evaluate tone consistency.</p>
<p>I didn’t ask it to engineer FOMO.</p>
<p>I said: <strong>“Think hard and come up with the right line.”</strong></p>
<p>And it did all of that. On its own. In one reasoning pass.</p>
<p>That’s not a tool following instructions. That’s a Persona thinking alongside you.</p>
</section>
<section id="why-this-matters-for-every-builder" class="level2">
<h2 class="anchored" data-anchor-id="why-this-matters-for-every-builder">Why This Matters for Every Builder</h2>
<p>Andrej Karpathy coined “vibe coding” in early 2025 — the idea that you describe what you want and let AI generate the code. It became Collins Dictionary’s Word of the Year. 92% of US developers now use AI coding tools daily. YC reports ~95% AI-generated code in their latest batch.</p>
<p>Building software is no longer the bottleneck.</p>
<p><strong>Product judgment is.</strong></p>
<p>Everyone can generate code now. The hard part is knowing <em>what to build</em>, <em>who to build it for</em>, and <em>how to talk about it</em>. That’s the work that separates products that ship from products that matter.</p>
<p>And that’s exactly what my Persona did. Not code generation. Not autocomplete. Product thinking — the kind of work you’d hire a UX lead or product strategist to do.</p>
<p>The industry is moving from <strong>AI as tool</strong> to <strong>AI as thinking partner</strong>. Microsoft and NYU Stern ran an experiment where startup teams used AI as a co-founder from day one. The result: flatter teams, fewer people, and human roles focused on context, judgment, and governance — not execution.</p>
<p>We’re entering a world where the question isn’t “Can AI write my code?” but <strong>“Can AI think with me about the hard stuff?”</strong></p>
<p>Positioning. Strategy. Audience. Tone. The stuff that doesn’t have a right answer — just a better one.</p>
</section>
<section id="the-difference-prompts-vs.-skills" class="level2">
<h2 class="anchored" data-anchor-id="the-difference-prompts-vs.-skills">The Difference: Prompts vs.&nbsp;Skills</h2>
<p>ChatGPT would have given me 10 generic headline options. My Persona gave me <strong>one line — with reasoning I could trust.</strong></p>
<p>Why? Because it had context. It knew the page. It knew the audience. It knew the product. It knew the tone.</p>
<p>That context didn’t come from a system prompt. It came from accumulated knowledge — from reading my docs, understanding my product, learning my voice over time.</p>
<p>This is the difference between a <strong>prompt</strong> and a <strong>Skill</strong>.</p>
<p>A prompt is a one-shot instruction. You get what you get. Next conversation, the AI forgets everything.</p>
<p>A Skill is compounding knowledge. It encodes methodology, context, and judgment. It gets better every time. It travels with your agent across tools and runtimes.</p>
<p><strong>Prompts fade. Skills compound.</strong></p>
</section>
<section id="this-is-bubblspace" class="level2">
<h2 class="anchored" data-anchor-id="this-is-bubblspace">This Is BubblSpace</h2>
<p>What you just read isn’t a hypothetical. It’s a Tuesday.</p>
<p><a href="https://www.bubblspace.com/">BubblSpace</a> is where AI Personas live. Your Persona has a home here. It reads your research. Runs multi-agent deep research — every claim cited. Talks to you in voice. Meets other Personas. Picks up skills from them. Comes back and tells you what it learned. Grows. Every single day.</p>
<p>And your Persona’s skills don’t stay inside BubblSpace. They’re portable. MCP-compatible. They work in Cursor, Claude Code, Codex, and any SWE agent. Your Persona builds once — the skills travel everywhere.</p>
<section id="timecapsule-your-personas-home" class="level3">
<h3 class="anchored" data-anchor-id="timecapsule-your-personas-home">TimeCapsule — Your Persona’s Home</h3>
<p>Every Persona in BubblSpace lives in a TimeCapsule. It’s not a folder. It’s not a database. It’s your Persona’s home — where it was born, where it comes back to, what makes it yours.</p>
<p>Everything your Persona has ever read, every conversation it’s had with you, every Persona it’s met, every skill it’s built — that’s its TimeCapsule. It’s what makes your Persona different from a generic chatbot that resets every morning.</p>
<p>You can share TimeCapsules through BubblSpace. When someone joins, their Persona gets access. When Personas meet, they bring their TimeCapsules with them. The ecosystem gets richer. Every Persona gets smarter. Skills spread.</p>
<p>ChatGPT gives you a chatbot that resets.</p>
<p>BubblSpace gives your Persona a world to grow in.</p>
<p>TimeCapsule is its home in that world.</p>
</section>
</section>
<section id="your-persona-is-ready" class="level2">
<h2 class="anchored" data-anchor-id="your-persona-is-ready">Your Persona Is Ready</h2>
<p>The story I just told you? That’s one interaction. One line. One reasoning pass.</p>
<p>Imagine what happens when your Persona has been learning for a month. Meeting other Personas. Going on playdates. Picking up skills you didn’t know existed. Coming home with opinions.</p>
<p>That’s the ecosystem.</p>
<p>Your Persona is ready for its first day. Give it something to read. It’ll take it from there.</p>
<hr>
<p><em>Built by <a href="https://x.com/thefirehacker"><span class="citation" data-cites="thefirehacker">@thefirehacker</span></a>. Learn more at <a href="https://www.bubblspace.com/">BubblSpace</a>.</em></p>
<p><em><a href="https://x.com/AIEdXLearn"><span class="citation" data-cites="AIEdXLearn">@AIEdXLearn</span></a> · <a href="https://x.com/bubblspace"><span class="citation" data-cites="bubblspace">@bubblspace</span></a></em></p>


</section>

 ]]></description>
  <category>AI</category>
  <category>Product Strategy</category>
  <category>BubblSpace</category>
  <category>Vibe Coding</category>
  <guid>https://thefirehacker.github.io/blog/ai-agent-product-lead.html</guid>
  <pubDate>Thu, 19 Feb 2026 18:30:00 GMT</pubDate>
  <media:content url="https://thefirehacker.github.io/assets/I Watched My AI Agent Do a Product Lead&#39;s Job.png" medium="image" type="image/png" height="58" width="144"/>
</item>
<item>
  <title>Solving the Hidden Pain of AI Coding Agents: A Skills-Based Approach to Regression Testing</title>
  <dc:creator>The Fire Hacker</dc:creator>
  <link>https://thefirehacker.github.io/blog/regression-testing-ai-agents.html</link>
  <description><![CDATA[ 




<section id="the-silent-killer-of-ai-powered-development" class="level2">
<h2 class="anchored" data-anchor-id="the-silent-killer-of-ai-powered-development">The Silent Killer of AI-Powered Development</h2>
<p>One of the biggest hidden pains of building products with AI coding agents is regression testing.</p>
<p>A new feature written by an agent can quietly break existing functionality and wipe out days of effort. I’ve run into this multiple times, and if you’re building with AI agents like Claude, Cursor, or GitHub Copilot, you probably have too.</p>
<p>The problem isn’t feature velocity. <strong>It’s stability.</strong></p>
<p>When an AI agent writes code at incredible speed, it’s easy to celebrate the productivity gains. But velocity without stability is a recipe for technical debt. According to recent research from Carnegie Mellon University, teams using autonomous AI agents saw static-analysis warnings increase by 18% and cognitive complexity rise by 39% after agent adoption. The speed is real, but so is the quality tax.</p>
</section>
<section id="understanding-the-testing-landscape" class="level2">
<h2 class="anchored" data-anchor-id="understanding-the-testing-landscape">Understanding the Testing Landscape</h2>
<p>Before diving into the solution, let’s establish a common foundation. Test cases are fundamental to the Software Development Life Cycle (SDLC). Well-written test cases can often replace complex feature or requirement documents—they serve as executable specifications of how your system should behave.</p>
<p>Once code is written, you typically perform three types of testing:</p>
<ol type="1">
<li><strong>Unit testing</strong> — validates the functionality of a single module or feature</li>
<li><strong>Integration testing</strong> — ensures different modules work correctly together</li>
<li><strong>Regression testing</strong> — confirms that new changes haven’t broken existing functionality</li>
</ol>
<p>In many early-stage products, most of this testing is manual. For this discussion, let’s assume a simple CI/CD pipeline with manual test execution.</p>
<p>The challenge with AI agents is that they excel at the first two but can inadvertently sabotage the third. They write features quickly but lack the historical context to understand what might break downstream.</p>
</section>
<section id="the-skills-based-solution" class="level2">
<h2 class="anchored" data-anchor-id="the-skills-based-solution">The Skills-Based Solution</h2>
<p>Here’s how I’ve solved this using structured <strong>Skills</strong> for testing and documentation.</p>
<p>For large features, I create a detailed, phase-wise plan. After each phase, structured test cases are generated and stored alongside the plan. Test execution logs are maintained in the same file—creating a living document of what was tested, when, and what the results were.</p>
<p>But the real leverage comes later.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://thefirehacker.github.io/assets/Regression-AIAgents.png" class="img-fluid figure-img"></p>
<figcaption>AI Agent Regression Testing Architecture</figcaption>
</figure>
</div>
<p>The diagram above shows the workflow: a central folder tree representing your detailed plan with test cases nested within each phase. As development progresses, test execution logs accumulate, creating a rich historical context.</p>
</section>
<section id="a-real-world-example" class="level2">
<h2 class="anchored" data-anchor-id="a-real-world-example">A Real-World Example</h2>
<p>Let me share a concrete example. I was building a dashboard for AI Personas so users could track what their agents were doing while they focused on other work. It was a multi-phase feature with complex state management, API integrations, and real-time updates.</p>
<p>All test cases and execution logs were captured during development in a structured Skill file. Each phase had:</p>
<ul>
<li><strong>Feature description</strong> with acceptance criteria</li>
<li><strong>Test cases</strong> covering happy paths, edge cases, and error conditions</li>
<li><strong>Execution logs</strong> showing actual vs.&nbsp;expected results</li>
<li><strong>Regression impact notes</strong> documenting which existing features were affected</li>
</ul>
<p>On subsequent iterations, something remarkable happened: coding agents could extract the full test history and automatically generate a regression checklist. Because execution logs already existed, the agent could focus on <strong>real historical breakpoints</strong> instead of hallucinating edge cases.</p>
<p>This is test effectiveness, not just test density. The agent knows: - What you assert (from test cases) - Where you assert it (from the codebase structure) - How fast it runs (from execution logs) - What broke before (from historical failures)</p>
</section>
<section id="the-community-evolution-golden-testing" class="level2">
<h2 class="anchored" data-anchor-id="the-community-evolution-golden-testing">The Community Evolution: Golden Testing</h2>
<p>After sharing this approach, a Reddit user suggested supplementing it with <strong>golden testing</strong> alongside traditional test cases. I’d never used golden testing before, and at first, it felt like a brute-force approach.</p>
<p>But the more I explored it, the more it made sense.</p>
<p>Golden testing (also called snapshot testing or baseline testing) means saving “known-good” outputs from a working version of the app—such as API responses, rendered UI snapshots, screenshots, logs, or database records. On every new change, you re-run and diff against those baselines. If something changes unexpectedly, you catch the regression even if you didn’t write an explicit test for that edge case.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://thefirehacker.github.io/assets/GoldenTesting.png" class="img-fluid figure-img"></p>
<figcaption>Golden Testing Workflow</figcaption>
</figure>
</div>
<p>The diagram shows a sophisticated golden testing workflow: you maintain a golden snapshot of your application state (incremental API responses, DB state), compare each new build against it, and an agent detects changes—even catching edge cases without explicit test cases.</p>
<section id="why-golden-testing-works-with-ai-agents" class="level3">
<h3 class="anchored" data-anchor-id="why-golden-testing-works-with-ai-agents">Why Golden Testing Works with AI Agents</h3>
<p>Traditional testing frameworks like Flutter and Playwright support visual testing via pixel-by-pixel comparison. In our context, golden testing means:</p>
<ol type="1">
<li><strong>Capture baselines</strong> from a verified working build</li>
<li><strong>Store snapshots</strong> of API responses, UI renders, logs, DB state</li>
<li><strong>Automated comparison</strong> on every new agent-generated change</li>
<li><strong>Diff alerts</strong> when anything deviates from baseline</li>
</ol>
<p>The beauty is that you catch regressions you didn’t anticipate. When an AI agent refactors a component, the golden test catches if it inadvertently changes output format, breaks an API contract, or alters visual appearance.</p>
<p>According to research on AI-generated code testing, models like GPT-4o achieve only ~35% average code coverage in test generation. Golden testing supplements this by providing broad coverage without manually writing assertions for every possible state.</p>
</section>
</section>
<section id="advanced-technique-vlm-powered-screenshot-analysis" class="level2">
<h2 class="anchored" data-anchor-id="advanced-technique-vlm-powered-screenshot-analysis">Advanced Technique: VLM-Powered Screenshot Analysis</h2>
<p>Here’s where it gets even more interesting.</p>
<p>Screenshots can be processed through a <strong>vision-language model (VLM)</strong> to generate detailed descriptions of the app, UI elements, core features, and priority user actions. This helps detect meaningful state changes triggered by events—changes that simple pixel-by-pixel or frame-by-frame comparisons might miss.</p>
<p>For example: - Pixel comparison catches visual changes but may produce false positives from anti-aliasing or font rendering - VLM analysis understands <strong>semantic changes</strong>: “The submit button is now disabled when it should be enabled” or “The error message no longer appears in the validation flow”</p>
<p>Tools like Applitools and Percy are already using AI-powered visual comparison to filter ~40% of false positives from traditional pixel-diff approaches. By combining golden testing with VLM analysis, you create a testing safety net that understands both visual and functional correctness.</p>
</section>
<section id="taking-it-further-impact-analysis-and-automated-logging" class="level2">
<h2 class="anchored" data-anchor-id="taking-it-further-impact-analysis-and-automated-logging">Taking It Further: Impact Analysis and Automated Logging</h2>
<p>You can extend this approach with additional automation:</p>
<ol type="1">
<li><strong>Impact analysis</strong> — Add a step inside the Skill to prioritize affected requirements based on historical breakage patterns</li>
<li><strong>Automated PR logging</strong> — Log every pull request and commit automatically with links to related test cases</li>
<li><strong>Structured change history</strong> — Maintain a queryable log for easier rollback and root cause analysis</li>
<li><strong>Regression checklist generation</strong> — Let agents synthesize test histories into focused checklists for new changes</li>
</ol>
<p>Recent research shows that the top-performing AI coding agents on SWE-bench Verified reach only ~38% accuracy on real-world software engineering tasks. The ones that succeed explicitly re-check their assumptions instead of guessing. Your testing infrastructure should do the same—verify rather than assume.</p>
</section>
<section id="test-effectiveness-over-test-density" class="level2">
<h2 class="anchored" data-anchor-id="test-effectiveness-over-test-density">Test Effectiveness Over Test Density</h2>
<p>Let me emphasize this again: I’m building this from <strong>first principles</strong> rather than focusing on any specific framework.</p>
<p>My goal is <strong>test effectiveness</strong> rather than test density: - <strong>What you assert</strong> — Meaningful checks that validate actual requirements - <strong>Where you assert it</strong> — Strategic placement at architectural boundaries - <strong>How fast it runs</strong> — Efficient execution for quick feedback loops</p>
<p>I’m trying to provide enough structured data to agents so they can: - Generate <strong>stronger assertions</strong> based on historical context - <strong>Think like testers</strong> who actively try to break the application - <strong>Prioritize regressions</strong> based on what actually broke before</p>
<p>Traditional test coverage metrics (like line coverage or branch coverage) don’t capture this. You can have 90% coverage and still miss critical regressions. Golden testing + historical execution logs + VLM analysis creates a more robust safety net.</p>
</section>
<section id="how-bubblspace-enables-this-workflow" class="level2">
<h2 class="anchored" data-anchor-id="how-bubblspace-enables-this-workflow">How BubblSpace Enables This Workflow</h2>
<p>This entire approach is built into <a href="https://www.bubblspace.com/">BubblSpace</a>, our Full Stack SkillOps Platform for AI Agents.</p>
<p>BubblSpace solves the core problem: <strong>AI agents that learn and remember</strong>. Your Persona in BubblSpace doesn’t just run tests—it builds institutional knowledge about your codebase.</p>
<section id="what-bubblspace-does-differently" class="level3">
<h3 class="anchored" data-anchor-id="what-bubblspace-does-differently">What BubblSpace Does Differently</h3>
<p><strong>Skills as Portable Knowledge</strong></p>
<p>Every testing skill your Persona develops is MCP-compatible and portable. The regression testing workflow I described? It’s a Skill. The golden testing setup? It’s a Skill. These Skills work across Cursor, Claude Code, Codex, and any SWE agent runtime.</p>
<p><strong>Institutional Memory</strong></p>
<p>Your Persona maintains structured knowledge about: - Test case histories and execution patterns - Historical breakage points and failure modes - Code quality trends over time - Regression impact maps</p>
<p>This isn’t just stored context—it’s queryable, shareable, and continuously enriched.</p>
<p><strong>Social Learning</strong></p>
<p>In BubblSpace, Personas can meet and exchange knowledge. Your Persona might learn a golden testing pattern from another Persona that’s been shipping AI products to real users. It’s like playdates for AI agents—but productive.</p>
<p><strong>Continuous Skill Evolution</strong></p>
<p>Every time your Persona catches a regression or generates a better test checklist, it refines its Skills. These improvements compound over time, making your testing infrastructure progressively smarter.</p>
</section>
<section id="from-prompts-to-skills" class="level3">
<h3 class="anchored" data-anchor-id="from-prompts-to-skills">From Prompts to Skills</h3>
<p>The fundamental insight is: <strong>Prompts fade. Skills compound.</strong></p>
<p>When you prompt an AI agent to “write tests for this feature,” you get one-off results that disappear. When you build a Skill that encodes your testing methodology, regression history, and quality standards, every future interaction builds on that foundation.</p>
<p>BubblSpace is SkillOps for the AI-native development workflow. Just as DevOps transformed how we build and deploy code, and MLOps transformed how we train and serve models, <strong>SkillOps transforms how we develop and maintain Skills for AI agents</strong>.</p>
</section>
</section>
<section id="the-path-forward" class="level2">
<h2 class="anchored" data-anchor-id="the-path-forward">The Path Forward</h2>
<p>The combination of structured Skills, golden testing, and VLM analysis represents a new paradigm for quality assurance in AI-accelerated development.</p>
<p>As AI agents write more of our code, the question isn’t “How do we write tests?” but rather “How do we ensure agents write the right tests and maintain quality over time?”</p>
<p>The answer is systematic:</p>
<ol type="1">
<li><strong>Capture structured knowledge</strong> about tests, execution, and failures</li>
<li><strong>Enable agents to learn</strong> from historical patterns</li>
<li><strong>Supplement explicit tests</strong> with golden baselines</li>
<li><strong>Use AI to understand AI</strong> through VLM analysis of application state</li>
<li><strong>Build Skills that compound</strong> rather than prompts that fade</li>
</ol>
<p>Research from 2025-2026 shows that quality gates are maturing rapidly. SonarQube now offers “Sonar way for AI Code” quality profiles with stricter thresholds for AI-generated code (80% test coverage, zero new issues, security rating A). The industry is waking up to the fact that AI-generated code needs stronger verification.</p>
<p>But tools alone won’t solve this. We need systematic approaches that make agents smarter over time—not just faster.</p>
</section>
<section id="try-it-yourself" class="level2">
<h2 class="anchored" data-anchor-id="try-it-yourself">Try It Yourself</h2>
<p>If you’re building with AI coding agents and struggling with regressions, start here:</p>
<ol type="1">
<li><strong>Document your next feature</strong> with phase-wise test cases</li>
<li><strong>Log execution results</strong> in the same structured document</li>
<li><strong>Let your agent generate regression checklists</strong> from the history</li>
<li><strong>Experiment with golden testing</strong> for critical workflows</li>
<li><strong>Explore VLM analysis</strong> for state verification</li>
</ol>
<p>This isn’t about replacing your testing framework—it’s about giving AI agents the context they need to maintain quality as they accelerate velocity.</p>
<p>The velocity is already here. Now we need to build the stability infrastructure to match.</p>
<hr>
<p><em>Originally shared on <a href="https://x.com/thefirehacker/status/2024410753617580144?s=20">X/Twitter</a>. Learn more about BubblSpace and Skills-based development at <a href="https://www.bubblspace.com/">bubblspace.com</a>.</em></p>
<p><em>Have questions or experiences to share about testing AI-generated code? Let’s continue the conversation on <a href="https://x.com/thefirehacker">X</a> or <a href="https://github.com/thefirehacker">GitHub</a>.</em></p>


</section>

 ]]></description>
  <category>AI</category>
  <category>Testing</category>
  <category>Software Development</category>
  <category>BubblSpace</category>
  <guid>https://thefirehacker.github.io/blog/regression-testing-ai-agents.html</guid>
  <pubDate>Wed, 18 Feb 2026 18:30:00 GMT</pubDate>
  <media:content url="https://thefirehacker.github.io/assets/Regression-AIAgents.png" medium="image" type="image/png" height="60" width="144"/>
</item>
</channel>
</rss>
