[
  {
    "objectID": "til/ddp-python-basics.html",
    "href": "til/ddp-python-basics.html",
    "title": "DDP from Scratch: a learner-friendly guide",
    "section": "",
    "text": "This note is part of the Scratch ‚Üí Scale series by Zachary Mueller (course link). We‚Äôll implement a toy DDP wrapper, explain why it works, and demystify two Python idioms you‚Äôll see everywhere: dictionary comprehensions and kwargs (argument unpacking)."
  },
  {
    "objectID": "til/ddp-python-basics.html#tldr",
    "href": "til/ddp-python-basics.html#tldr",
    "title": "DDP from Scratch: a learner-friendly guide",
    "section": "TL;DR",
    "text": "TL;DR\nüîë Core Python patterns explained:\n\nDictionary comprehensions: Transform raw data (lists, ints) into model-ready tensors in one elegant line ‚Äî {k: torch.tensor(v).to(device) for k, v in item.items()} converts HuggingFace dataset samples to GPU tensors with proper shapes.\nKwargs unpacking (**): Unpack dictionaries into named function arguments ‚Äî model(**batch) automatically maps dict keys to HuggingFace model‚Äôs forward() parameters like input_ids, attention_mask, labels.\nGradient averaging ‚öñÔ∏è learning rate scaling: Dividing gradients by world_size or scaling LR by 1/world_size are mathematically equivalent ‚Äî the choice is where in your algorithm the division happens: before the optimizer step (average gradients) or after (scale learning rate).\n\nüìã DDP essentials:\n\nSeed every process the same way before you create the model.\nAverage grads with dist.all_reduce(param.grad, op=SUM) then divide by world size.\nUse **kwargs to pass batches to models: model(**batch) works seamlessly with HuggingFace transformers."
  },
  {
    "objectID": "til/ddp-python-basics.html#visual-mental-model-of-distributed-training",
    "href": "til/ddp-python-basics.html#visual-mental-model-of-distributed-training",
    "title": "DDP from Scratch: a learner-friendly guide",
    "section": "0) Visual mental model of distributed training",
    "text": "0) Visual mental model of distributed training\nRank 0 (GPU0)      Rank 1 (GPU1)      ...\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ forward      ‚îÇ   ‚îÇ forward      ‚îÇ  (same model weights)\n‚îÇ loss.backward‚îÇ   ‚îÇ loss.backward‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n       ‚îÇ   grads            ‚îÇ   grads\n       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ all_reduce (SUM) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ (every rank gets sum of all grads)\n                    ‚îÇ\n              divide by world_size\n                    ‚îÇ\n                optimizer.step()"
  },
  {
    "objectID": "til/ddp-python-basics.html#seeding-making-model-replicas-identical",
    "href": "til/ddp-python-basics.html#seeding-making-model-replicas-identical",
    "title": "DDP from Scratch: a learner-friendly guide",
    "section": "1) Seeding: making model replicas identical",
    "text": "1) Seeding: making model replicas identical\nIdentical initialization across ranks is not optional. If rank 0 samples weights {W} and rank 1 samples different weights {W‚Äô}, averaging grads is meaningless. We seed each RNG per process, then construct the model.\ndef set_seed(seed: int = 43):\n    import random, numpy as np, torch\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n# In your entry point (each process runs this):\nset_seed(43)            # must happen BEFORE model creation\nmodel = build_model()   # identical on all ranks\n\nWhy no communication?\nEach process runs the exact same Python code with the same seeds ‚Üí same random draws ‚Üí identical parameters. No dist.broadcast is required to make them equal, though you can use broadcast to enforce equality (see ¬ß3).\n\nPitfall: Seeding after constructing the model doesn‚Äôt retroactively change weights."
  },
  {
    "objectID": "til/ddp-python-basics.html#two-python-idioms-youll-see-everywhere",
    "href": "til/ddp-python-basics.html#two-python-idioms-youll-see-everywhere",
    "title": "DDP from Scratch: a learner-friendly guide",
    "section": "2) Two Python idioms you‚Äôll see everywhere",
    "text": "2) Two Python idioms you‚Äôll see everywhere\n\n2.1 Dictionary comprehension ‚Äî Why we need this pattern\nThis line converts a HuggingFace dataset sample (lists/ints) into a batch dictionary of GPU tensors with an added batch dimension:\nitem = {k: torch.tensor(v).unsqueeze(0).to(device) for k, v in item.items()}\nWhy this transformation is essential:\nHuggingFace datasets return items as Python dicts with lists and ints:\nexample = {\"input_ids\": [101, 2023, ...], \"attention_mask\": [1, 1, ...], \"labels\": 0}\nBut PyTorch models expect GPU tensors with batch dimensions:\nbatch = {\"input_ids\": tensor([[101, 2023, ...]], device='cuda:0'), ...}\nWhy tensors are required:\nPyTorch models perform tensor operations (matrix multiplications, slicing, etc.) that require PyTorch tensor objects, not Python lists or integers. If you pass Python lists/ints directly, you‚Äôll get errors like: - TypeError: expected Tensor as element 0 in argument 0, but got list - RuntimeError: Expected all tensors to be on the same device\nThe dictionary comprehension converts your data to the correct tensor format before passing it to the model. (See ¬ß2.2 for how these tensors flow through the model‚Äôs forward() method.)\nThe dictionary comprehension does three transformations in one line:\n\nPreserve structure: Keep the same dict keys (input_ids, attention_mask, etc.)\nConvert types: List/int ‚Üí PyTorch tensor ‚Üí GPU tensor\nAdd batch dimension: Shape (seq_len,) ‚Üí (1, seq_len) for batching\n\nBreakdown: * for k, v in item.items() ‚Üí iterates over each key-value pair * torch.tensor(v) ‚Üí converts list/int to tensor * .unsqueeze(0) ‚Üí adds batch dimension: [a, b, c] ‚Üí [[a, b, c]] * .to(device) ‚Üí moves to GPU\nWithout this transformation, you‚Äôd pass Python lists/CPU arrays to the model, which would either error or require slow implicit conversion on each forward pass.\n\nAlternative: Generator-based streaming with yield\nFor large datasets or memory-constrained scenarios, dictionary comprehensions can be memory-intensive (they build the entire dict in memory). A better approach uses generators with yield for lazy evaluation:\ndef stream_to_device(item, device):\n    \"\"\"Generator that yields tensors one at a time - memory efficient\"\"\"\n    for k, v in item.items():\n        yield k, torch.tensor(v).unsqueeze(0).to(device)\n\n# Usage: build dict lazily\nbatch = dict(stream_to_device(example, device))\nWhy generators are better for large data: * Lazy evaluation: Tensors are created and moved to GPU one at a time, not all at once. * Lower memory footprint: Only one tensor exists in memory during transformation. * Scalable: Works with datasets that don‚Äôt fit in RAM.\nWhen to use each: * Dict comprehension: Small to medium batches, simple one-liners, readable code. * Generator with yield: Large datasets, streaming data, memory-constrained environments, production pipelines.\n\n\n\n2.2 Kwargs unpacking with ** ‚Äî The HuggingFace connection\nGiven item = {\"input_ids\": X, \"attention_mask\": Y, \"labels\": Z}:\nout = model(**item)\n# exactly the same as:\nout = model(input_ids=X, attention_mask=Y, labels=Z)\nWhy this matters for HuggingFace models:\nThe ** operator unpacks a dict into named arguments that match your model‚Äôs forward() signature. This is why HuggingFace workflows are so elegant:\n\nDataset has standard keys: HuggingFace datasets/tokenizers output dicts with keys like \"input_ids\", \"attention_mask\", \"labels\".\nModel expects those keys: All HuggingFace models have a forward() method that accepts these exact parameter names.\n**kwargs bridges them: Instead of manually extracting each key, model(**batch) automatically maps dict keys to function parameters.\n\nWithout **kwargs (manual, verbose):\nout = model(\n    input_ids=batch[\"input_ids\"],\n    attention_mask=batch[\"attention_mask\"],\n    labels=batch[\"labels\"]\n)\nWith **kwargs (clean, scalable):\nout = model(**batch)  # Automatically maps all keys!\nThis works because HuggingFace models define their forward() signature to match the standard dataset keys. It‚Äôs a deliberate design pattern that makes training code incredibly clean.\nTracing the forward() call chain:\nWhen you call model(**batch), the unpacked tensors flow through the model‚Äôs forward pass. Here‚Äôs the call chain for AutoModelForSequenceClassification:\nmodel(**batch)  # batch contains tensors: {\"input_ids\": tensor(...), ...}\n    ‚Üì\nAutoModelForSequenceClassification.from_pretrained(...)\n    ‚Üì\nSmolLM2ForSequenceClassification  # concrete architecture class\n    ‚Üì\nGenericForSequenceClassification.forward(**kwargs)\n    ‚Üì\n    # forward() signature receives unpacked kwargs:\n    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n        outputs = self.model(input_ids, attention_mask=attention_mask, **kwargs)\n        #              ‚Üë **kwargs unpacking maps dict keys to these parameters\n        pooled = outputs[0][:, 0, :]  # CLS token pooling\n        logits = self.score(pooled)   # linear classifier head\n        loss = self.loss_fn(logits, labels) if labels is not None else None\n        return SequenceClassifierOutput(loss=loss, logits=logits, ...)\nKey insight: The **batch unpacking automatically maps dictionary keys (\"input_ids\", \"attention_mask\", \"labels\") to the forward() method‚Äôs parameter names. This is why model(**batch) works seamlessly ‚Äî the keys match the function signature exactly.\n\n\n2.3 Gradient averaging vs learning-rate scaling ‚öñÔ∏è\nThis is a key insight: When training on N GPUs, you have two mathematically equivalent options for combining gradients:\n\nOption A: Average gradients (most common)\n# After backward on each rank\ndist.all_reduce(param.grad, op=SUM)\nparam.grad /= world_size  # Average the gradients\n\n# Optimizer update with normal LR\noptimizer.step()  # uses original learning rate\n\n\nOption B: Sum gradients, scale learning rate\n# After backward on each rank  \ndist.all_reduce(param.grad, op=SUM)  # Keep summed gradients\n\n# Optimizer update with scaled LR\nfor param in model.parameters():\n    param.data -= (lr / world_size) * param.grad\nWhy they‚Äôre equivalent:\n\\[\n\\text{param} - \\text{lr} \\times \\frac{\\text{grad}}{N} = \\text{param} - \\frac{\\text{lr}}{N} \\times \\text{grad}\n\\]\nReal-world implications: * PyTorch DDP: Uses Option A (averages gradients), so you keep your learning rate unchanged. * Some frameworks (Horovod, older examples): Use Option B (sum gradients), expecting you to scale LR by 1/world_size. * The division can happen in two places: before the optimizer step (average gradients during sync) or after (scale learning rate during optimizer step) ‚Äî same math, different location in the algorithm.\nPractical tip: The instructor‚Äôs comment ‚Äúit depends where in the algorithm you want the averaging‚Äù refers to this choice. Most modern code averages gradients (Option A) because it‚Äôs cleaner and doesn‚Äôt require you to remember to scale the learning rate."
  },
  {
    "objectID": "til/ddp-python-basics.html#a-tiny-ddp-wrapper-teaching-version",
    "href": "til/ddp-python-basics.html#a-tiny-ddp-wrapper-teaching-version",
    "title": "DDP from Scratch: a learner-friendly guide",
    "section": "3) A tiny DDP wrapper (teaching version)",
    "text": "3) A tiny DDP wrapper (teaching version)\nThis wrapper (a) verifies parameter equality at init (optionally enforces it) and (b) averages gradients after backward().\nimport torch\nimport torch.distributed as dist\n\nclass MiniDDP:\n    def __init__(self, model: torch.nn.Module, enforce_broadcast: bool = False):\n        self.model = model\n        self.world_size = dist.get_world_size() if dist.is_initialized() else 1\n\n        # --- verify / enforce identical params across ranks ---\n        for p in self.model.parameters():\n            # create a rank0 copy to compare/broadcast\n            rank0_buf = p.detach().clone()\n            dist.broadcast(rank0_buf, src=0)     # everyone receives rank0's tensor\n            if enforce_broadcast:\n                p.data.copy_(rank0_buf)          # enforce equality (optional)\n            else:\n                if not torch.equal(p.data, rank0_buf):\n                    raise ValueError(\n                        \"Parameters differ at init. Seed all ranks BEFORE model construction, \"\n                        \"or set enforce_broadcast=True.\"\n                    )\n\n    def __call__(self, *args, **kwargs):\n        return self.model(*args, **kwargs)\n\n    def average_grads(self):\n        if self.world_size == 1:\n            return\n        for p in self.model.parameters():\n            if p.grad is None:\n                continue\n            dist.all_reduce(p.grad, op=dist.ReduceOp.SUM)\n            p.grad.div_(self.world_size)\n\n    # convenience passthroughs\n    def train(self):\n        self.model.train()\n    def eval(self):\n        self.model.eval()\nUnderstanding enforce_broadcast:\nThe enforce_broadcast parameter controls how parameter synchronization is handled at initialization:\n\nenforce_broadcast=False (default): Verifies that all ranks already have identical parameters (e.g., via seeding). If parameters differ, it raises an error. This is the ‚Äútrust but verify‚Äù approach ‚Äî you‚Äôre responsible for ensuring equality (via seeding), and the wrapper checks that you did it correctly.\nenforce_broadcast=True: Forces all ranks to use rank 0‚Äôs parameters by overwriting each rank‚Äôs parameters with rank 0‚Äôs values. This is the ‚Äúbelt and suspenders‚Äù approach ‚Äî even if seeding failed or parameters diverged, everyone gets rank 0‚Äôs exact state.\n\nWhy this mirrors PyTorch‚Äôs official DDP:\nPyTorch‚Äôs DistributedDataParallel always performs parameter synchronization at initialization (like enforce_broadcast=True), but it does so internally, automatically, and efficiently: - It broadcasts parameters from rank 0 to all other ranks during construction - It handles buffers (like BatchNorm running stats) as well - It uses optimized communication patterns (coalesced broadcasts, bucketing)\nThis initial synchronization is a core part of DDP‚Äôs design to ensure all model replicas start with identical weights. As documented in the PyTorch DDP notes: ‚ÄúWhen a model is wrapped with DDP, the constructor synchronizes the model‚Äôs parameters across all processes. This is achieved by broadcasting the parameters from the process with rank 0 to all other processes.‚Äù\nKey difference: In PyTorch‚Äôs DDP, this synchronization happens automatically in the constructor ‚Äî there‚Äôs no user-facing parameter to control it. It‚Äôs an internal implementation detail that ensures correctness.\nIn MiniDDP, we make this synchronization explicit and optional so you can: - See exactly what‚Äôs happening (educational value) - Choose to verify vs.¬†enforce (learning about seeding) - Understand the tradeoffs between verification and enforcement\n\nThis mirrors what PyTorch‚Äôs official DistributedDataParallel does conceptually, but without bucketing, overlap, or autograd hooks. Perfect for learning; use the real DDP for production."
  },
  {
    "objectID": "til/ddp-python-basics.html#minimal-distributed-training-loop",
    "href": "til/ddp-python-basics.html#minimal-distributed-training-loop",
    "title": "DDP from Scratch: a learner-friendly guide",
    "section": "4) Minimal distributed training loop",
    "text": "4) Minimal distributed training loop\n# torchrun --nproc_per_node=2 train.py\n\nimport os, torch, torch.distributed as dist\nfrom torch.optim import Adam\nfrom datasets import load_dataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nMODEL = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n\ndef set_seed(seed=43):\n    import random, numpy as np\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n\ndef main():\n    dist.init_process_group(\"nccl\")\n    rank  = dist.get_rank();  local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n    device = torch.device(f\"cuda:{local_rank}\")\n\n    set_seed(43)  # same on every process BEFORE creating the model\n\n    tok = AutoTokenizer.from_pretrained(MODEL)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        MODEL, num_labels=2, torch_dtype=torch.bfloat16\n    ).to(device)\n\n    ddp = MiniDDP(model, enforce_broadcast=False)\n    opt = Adam(ddp.model.parameters(), lr=1e-3)\n\n    ds = load_dataset(\"glue\", \"mrpc\")\n    def encode(ex):\n        return tok(ex[\"sentence1\"], ex[\"sentence2\"], padding=True, truncation=True)\n    ds = ds.map(encode, batched=True).rename_columns({\"label\": \"labels\"})\n\n    # toy per-rank sample (one example per rank to show divergence if not averaged)\n    example = ds[\"train\"][rank]\n    batch = {k: torch.tensor(v).unsqueeze(0).to(device) for k, v in example.items()}\n\n    ddp.train()\n    out = ddp(**batch)         # kwargs unpacking\n    out.loss.backward()\n    ddp.average_grads()        # &lt;‚Äî key! average across ranks\n    opt.step(); opt.zero_grad(set_to_none=True)\n\n    if rank == 0:\n        print(\"step ok; loss:\", out.loss.item())\n\n    dist.destroy_process_group()\n\nif __name__ == \"__main__\":\n    main()\n\nWhat just happened?\n\nBoth ranks ran the same code and created identical models (thanks to seeding).\nEach rank used a different example (rank index) ‚Üí losses differ initially.\naverage_grads() made every GPU apply the same averaged update, keeping replicas in lock‚Äëstep."
  },
  {
    "objectID": "til/ddp-python-basics.html#why-broadcast-at-init-if-we-already-seed",
    "href": "til/ddp-python-basics.html#why-broadcast-at-init-if-we-already-seed",
    "title": "DDP from Scratch: a learner-friendly guide",
    "section": "5) Why broadcast at init if we already seed?",
    "text": "5) Why broadcast at init if we already seed?\nSeeding should guarantee equality. The broadcast operation (when enforce_broadcast=True) is a belt‚Äëand‚Äësuspenders option:\n\nProtect against forgotten seeds: If you forgot to seed on some ranks, broadcast ensures everyone still starts identical.\nHandle divergent code paths: If different ranks take different initialization paths, broadcast syncs them.\nDeal with non‚Äëdeterministic ops: Some operations (e.g., certain CUDA kernels) may not be fully deterministic even with seeds.\nEnable joining late ranks: If a rank joins after initialization, broadcast can sync it to the current state from rank 0.\n\nIn practice: With proper seeding (see ¬ß1), enforce_broadcast=False (verify mode) is usually sufficient. Use enforce_broadcast=True only if you intend to force‚Äësync weights at init or are debugging initialization issues.\nNote: PyTorch‚Äôs official DDP always performs this synchronization automatically (equivalent to enforce_broadcast=True), but hides it from you. MiniDDP makes it explicit so you can learn about the mechanism."
  },
  {
    "objectID": "til/ddp-python-basics.html#common-pitfalls-fixes",
    "href": "til/ddp-python-basics.html#common-pitfalls-fixes",
    "title": "DDP from Scratch: a learner-friendly guide",
    "section": "6) Common pitfalls & fixes",
    "text": "6) Common pitfalls & fixes\n\nDifferent seeds / seeding too late ‚Üí parameters differ. Fix: call set_seed() before build_model() on every rank.\nForgetting to divide after all_reduce(SUM) ‚Üí LR effectively √ó world_size. Fix: divide grads (or use op=AVG on newer APIs like reduce_scatter_tensor).\nGrad is None: layers not used in the forward didn‚Äôt receive gradients. Fix: check the graph; guard if p.grad is None: continue.\nCPU tensors in batch: model expects CUDA tensors. Fix: dictionary comprehension that moves tensors to device.\nShape mismatches across ranks: ensure each rank‚Äôs micro‚Äëbatch has identical shapes (padding or a proper DistributedSampler).\nNCCL init errors: set MASTER_ADDR/PORT, unique RANK, correct CUDA_VISIBLE_DEVICES."
  },
  {
    "objectID": "til/ddp-python-basics.html#from-toy-to-real-ddp",
    "href": "til/ddp-python-basics.html#from-toy-to-real-ddp",
    "title": "DDP from Scratch: a learner-friendly guide",
    "section": "7) From toy to real DDP",
    "text": "7) From toy to real DDP\nWhat we built is the core idea. Production torch.nn.parallel.DistributedDataParallel adds:\n\ngradient bucketing and overlap with communication;\nparameter and buffer broadcast on construction (with versioning);\nautograd hooks for exact timing;\nmixed precision, static graph optimizations, etc.\n\nUpgrade path: once you grasp the flow above, swap MiniDDP for DistributedDataParallel(model, device_ids=[local_rank]) and use DistributedSampler in your DataLoader."
  },
  {
    "objectID": "til/ddp-python-basics.html#exercises-recommended",
    "href": "til/ddp-python-basics.html#exercises-recommended",
    "title": "DDP from Scratch: a learner-friendly guide",
    "section": "8) Exercises (recommended)",
    "text": "8) Exercises (recommended)\n\nFail fast: comment out set_seed(43) and watch the init check throw. Then set enforce_broadcast=True and observe it succeed.\nBatching: replace the single‚Äëexample hack with a DataLoader + DistributedSampler. Verify all ranks consume disjoint shards.\nReduce‚Äëscatter: re‚Äëimplement average_grads() with reduce_scatter_tensor + all_gather_into_tensor to mimic optimizer sharding.\nKwargs drill: write a wrapper that logs which kwargs are passed through (*args, **kwargs) and rejects unknown keys.\nDeterminism: enable CUDA deterministic flags and compare speed/behavior."
  },
  {
    "objectID": "til/ddp-python-basics.html#cheatsheet",
    "href": "til/ddp-python-basics.html#cheatsheet",
    "title": "DDP from Scratch: a learner-friendly guide",
    "section": "9) Cheatsheet",
    "text": "9) Cheatsheet\n\nitem = {k: f(v) for k, v in d.items()} ‚Üí dictionary comprehension.\nmodel(**d) ‚Üí unpack d into named arguments to forward.\ndist.all_reduce(t, SUM); t /= world_size ‚Üí average a tensor across ranks.\nSeed before model creation on every process.\nIf in doubt, force-sync params once with broadcast."
  },
  {
    "objectID": "til/ddp-python-basics.html#appendix-tiny-utilities",
    "href": "til/ddp-python-basics.html#appendix-tiny-utilities",
    "title": "DDP from Scratch: a learner-friendly guide",
    "section": "10) Appendix: tiny utilities",
    "text": "10) Appendix: tiny utilities\ndef recursively_apply(func, data):\n    if isinstance(data, (tuple, list)):\n        return type(data)(recursively_apply(func, x) for x in data)\n    if isinstance(data, dict):\n        return {k: recursively_apply(func, v) for k, v in data.items()}\n    return func(data)\n\n# Example: move a nested batch to device\nbatch = recursively_apply(lambda t: t.to(device) if isinstance(t, torch.Tensor) else t, batch)"
  },
  {
    "objectID": "til/ddp-python-basics.html#bonus-where-does-forward-come-from-with-automodel",
    "href": "til/ddp-python-basics.html#bonus-where-does-forward-come-from-with-automodel",
    "title": "DDP from Scratch: a learner-friendly guide",
    "section": "11) Bonus: Where does forward() come from with AutoModel?",
    "text": "11) Bonus: Where does forward() come from with AutoModel?\nWhen we wrote:\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=2)\nthat helper inspects the model config and dispatches to the architecture‚Äëspecific ...ForSequenceClassification class. For the SmolLM family, that class inherits a generic head that already implements forward().\nCall chain at runtime (conceptual):\n\nAutoModelForSequenceClassification ‚Üí ArchitectureForSequenceClassification ‚Üí GenericForSequenceClassification.forward(**kwargs) ‚Üí ArchitectureModel.forward(‚Ä¶) ‚Üí CLS pooling ‚Üí classifier head (self.score) ‚Üí loss (if labels)\n\nMinimal shape of that forward():\nclass GenericForSequenceClassification(PreTrainedModel):\n    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n        outputs = self.model(input_ids, attention_mask=attention_mask, **kwargs)\n        pooled = outputs[0][:, 0, :]\n        logits = self.score(pooled)\n        loss = self.loss_fn(logits, labels) if labels is not None else None\n        return SequenceClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states)\nThis is why model(**batch) (see ¬ß2.2) ‚Äújust works‚Äù: the dict keys map to the generic forward() signature, which calls the backbone‚Äôs forward() under the hood.\n\nHappy scaling! If you‚Äôre following the course, tag this post as TIL/DDP‚Äëfrom‚Äëscratch and iterate from here. üß™üöÄ"
  },
  {
    "objectID": "til/ddp-python-basics.html#quick-reference-gradient-sync-patterns",
    "href": "til/ddp-python-basics.html#quick-reference-gradient-sync-patterns",
    "title": "DDP from Scratch: a learner-friendly guide",
    "section": "12) Quick Reference: Gradient sync patterns",
    "text": "12) Quick Reference: Gradient sync patterns\nSummary of the two equivalent approaches (see ¬ß2.3 for full explanation):\n# Pattern A: Average gradients (PyTorch DDP default)\ndist.all_reduce(param.grad, op=SUM)\nparam.grad /= world_size\nparam -= lr * param.grad  # Original LR\n\n# Pattern B: Sum gradients, scale LR (Horovod-style)\ndist.all_reduce(param.grad, op=SUM)\nparam -= (lr / world_size) * param.grad  # Scaled LR\nKey takeaway: Both produce identical updates. Choose Pattern A for cleaner code that matches PyTorch DDP defaults."
  },
  {
    "objectID": "til.html",
    "href": "til.html",
    "title": "Today I Learned",
    "section": "",
    "text": "Deep dives into technical concepts I‚Äôm learning. From GPU programming to AI systems, these are comprehensive explorations of new technologies and techniques.\n\n\n\n\n\n\n\n\n\n\n\n\nDDP from Scratch: a learner-friendly guide\n\n\nLearn why dictionary comprehensions in python elegantly transform HuggingFace data for models, how kwargs unpacking makes model(batch) ‚Äòjust work‚Äô, and why gradient‚Ä¶\n\n\n\nThe Fire Hacker\n\n\nOct 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nMy First CUDA Kernel: Learning GPU Programming from Scratch\n\n\n\n\n\n\nThe Fire Hacker\n\n\nJan 16, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/regression-testing-ai-agents.html",
    "href": "blog/regression-testing-ai-agents.html",
    "title": "Solving the Hidden Pain of AI Coding Agents: A Skills-Based Approach to Regression Testing",
    "section": "",
    "text": "One of the biggest hidden pains of building products with AI coding agents is regression testing.\nA new feature written by an agent can quietly break existing functionality and wipe out days of effort. I‚Äôve run into this multiple times, and if you‚Äôre building with AI agents like Claude, Cursor, or GitHub Copilot, you probably have too.\nThe problem isn‚Äôt feature velocity. It‚Äôs stability.\nWhen an AI agent writes code at incredible speed, it‚Äôs easy to celebrate the productivity gains. But velocity without stability is a recipe for technical debt. According to recent research from Carnegie Mellon University, teams using autonomous AI agents saw static-analysis warnings increase by 18% and cognitive complexity rise by 39% after agent adoption. The speed is real, but so is the quality tax."
  },
  {
    "objectID": "blog/regression-testing-ai-agents.html#the-silent-killer-of-ai-powered-development",
    "href": "blog/regression-testing-ai-agents.html#the-silent-killer-of-ai-powered-development",
    "title": "Solving the Hidden Pain of AI Coding Agents: A Skills-Based Approach to Regression Testing",
    "section": "",
    "text": "One of the biggest hidden pains of building products with AI coding agents is regression testing.\nA new feature written by an agent can quietly break existing functionality and wipe out days of effort. I‚Äôve run into this multiple times, and if you‚Äôre building with AI agents like Claude, Cursor, or GitHub Copilot, you probably have too.\nThe problem isn‚Äôt feature velocity. It‚Äôs stability.\nWhen an AI agent writes code at incredible speed, it‚Äôs easy to celebrate the productivity gains. But velocity without stability is a recipe for technical debt. According to recent research from Carnegie Mellon University, teams using autonomous AI agents saw static-analysis warnings increase by 18% and cognitive complexity rise by 39% after agent adoption. The speed is real, but so is the quality tax."
  },
  {
    "objectID": "blog/regression-testing-ai-agents.html#understanding-the-testing-landscape",
    "href": "blog/regression-testing-ai-agents.html#understanding-the-testing-landscape",
    "title": "Solving the Hidden Pain of AI Coding Agents: A Skills-Based Approach to Regression Testing",
    "section": "Understanding the Testing Landscape",
    "text": "Understanding the Testing Landscape\nBefore diving into the solution, let‚Äôs establish a common foundation. Test cases are fundamental to the Software Development Life Cycle (SDLC). Well-written test cases can often replace complex feature or requirement documents‚Äîthey serve as executable specifications of how your system should behave.\nOnce code is written, you typically perform three types of testing:\n\nUnit testing ‚Äî validates the functionality of a single module or feature\nIntegration testing ‚Äî ensures different modules work correctly together\nRegression testing ‚Äî confirms that new changes haven‚Äôt broken existing functionality\n\nIn many early-stage products, most of this testing is manual. For this discussion, let‚Äôs assume a simple CI/CD pipeline with manual test execution.\nThe challenge with AI agents is that they excel at the first two but can inadvertently sabotage the third. They write features quickly but lack the historical context to understand what might break downstream."
  },
  {
    "objectID": "blog/regression-testing-ai-agents.html#the-skills-based-solution",
    "href": "blog/regression-testing-ai-agents.html#the-skills-based-solution",
    "title": "Solving the Hidden Pain of AI Coding Agents: A Skills-Based Approach to Regression Testing",
    "section": "The Skills-Based Solution",
    "text": "The Skills-Based Solution\nHere‚Äôs how I‚Äôve solved this using structured Skills for testing and documentation.\nFor large features, I create a detailed, phase-wise plan. After each phase, structured test cases are generated and stored alongside the plan. Test execution logs are maintained in the same file‚Äîcreating a living document of what was tested, when, and what the results were.\nBut the real leverage comes later.\n\n\n\nAI Agent Regression Testing Architecture\n\n\nThe diagram above shows the workflow: a central folder tree representing your detailed plan with test cases nested within each phase. As development progresses, test execution logs accumulate, creating a rich historical context."
  },
  {
    "objectID": "blog/regression-testing-ai-agents.html#a-real-world-example",
    "href": "blog/regression-testing-ai-agents.html#a-real-world-example",
    "title": "Solving the Hidden Pain of AI Coding Agents: A Skills-Based Approach to Regression Testing",
    "section": "A Real-World Example",
    "text": "A Real-World Example\nLet me share a concrete example. I was building a dashboard for AI Personas so users could track what their agents were doing while they focused on other work. It was a multi-phase feature with complex state management, API integrations, and real-time updates.\nAll test cases and execution logs were captured during development in a structured Skill file. Each phase had:\n\nFeature description with acceptance criteria\nTest cases covering happy paths, edge cases, and error conditions\nExecution logs showing actual vs.¬†expected results\nRegression impact notes documenting which existing features were affected\n\nOn subsequent iterations, something remarkable happened: coding agents could extract the full test history and automatically generate a regression checklist. Because execution logs already existed, the agent could focus on real historical breakpoints instead of hallucinating edge cases.\nThis is test effectiveness, not just test density. The agent knows: - What you assert (from test cases) - Where you assert it (from the codebase structure) - How fast it runs (from execution logs) - What broke before (from historical failures)"
  },
  {
    "objectID": "blog/regression-testing-ai-agents.html#the-community-evolution-golden-testing",
    "href": "blog/regression-testing-ai-agents.html#the-community-evolution-golden-testing",
    "title": "Solving the Hidden Pain of AI Coding Agents: A Skills-Based Approach to Regression Testing",
    "section": "The Community Evolution: Golden Testing",
    "text": "The Community Evolution: Golden Testing\nAfter sharing this approach, a Reddit user suggested supplementing it with golden testing alongside traditional test cases. I‚Äôd never used golden testing before, and at first, it felt like a brute-force approach.\nBut the more I explored it, the more it made sense.\nGolden testing (also called snapshot testing or baseline testing) means saving ‚Äúknown-good‚Äù outputs from a working version of the app‚Äîsuch as API responses, rendered UI snapshots, screenshots, logs, or database records. On every new change, you re-run and diff against those baselines. If something changes unexpectedly, you catch the regression even if you didn‚Äôt write an explicit test for that edge case.\n\n\n\nGolden Testing Workflow\n\n\nThe diagram shows a sophisticated golden testing workflow: you maintain a golden snapshot of your application state (incremental API responses, DB state), compare each new build against it, and an agent detects changes‚Äîeven catching edge cases without explicit test cases.\n\nWhy Golden Testing Works with AI Agents\nTraditional testing frameworks like Flutter and Playwright support visual testing via pixel-by-pixel comparison. In our context, golden testing means:\n\nCapture baselines from a verified working build\nStore snapshots of API responses, UI renders, logs, DB state\nAutomated comparison on every new agent-generated change\nDiff alerts when anything deviates from baseline\n\nThe beauty is that you catch regressions you didn‚Äôt anticipate. When an AI agent refactors a component, the golden test catches if it inadvertently changes output format, breaks an API contract, or alters visual appearance.\nAccording to research on AI-generated code testing, models like GPT-4o achieve only ~35% average code coverage in test generation. Golden testing supplements this by providing broad coverage without manually writing assertions for every possible state."
  },
  {
    "objectID": "blog/regression-testing-ai-agents.html#advanced-technique-vlm-powered-screenshot-analysis",
    "href": "blog/regression-testing-ai-agents.html#advanced-technique-vlm-powered-screenshot-analysis",
    "title": "Solving the Hidden Pain of AI Coding Agents: A Skills-Based Approach to Regression Testing",
    "section": "Advanced Technique: VLM-Powered Screenshot Analysis",
    "text": "Advanced Technique: VLM-Powered Screenshot Analysis\nHere‚Äôs where it gets even more interesting.\nScreenshots can be processed through a vision-language model (VLM) to generate detailed descriptions of the app, UI elements, core features, and priority user actions. This helps detect meaningful state changes triggered by events‚Äîchanges that simple pixel-by-pixel or frame-by-frame comparisons might miss.\nFor example: - Pixel comparison catches visual changes but may produce false positives from anti-aliasing or font rendering - VLM analysis understands semantic changes: ‚ÄúThe submit button is now disabled when it should be enabled‚Äù or ‚ÄúThe error message no longer appears in the validation flow‚Äù\nTools like Applitools and Percy are already using AI-powered visual comparison to filter ~40% of false positives from traditional pixel-diff approaches. By combining golden testing with VLM analysis, you create a testing safety net that understands both visual and functional correctness."
  },
  {
    "objectID": "blog/regression-testing-ai-agents.html#taking-it-further-impact-analysis-and-automated-logging",
    "href": "blog/regression-testing-ai-agents.html#taking-it-further-impact-analysis-and-automated-logging",
    "title": "Solving the Hidden Pain of AI Coding Agents: A Skills-Based Approach to Regression Testing",
    "section": "Taking It Further: Impact Analysis and Automated Logging",
    "text": "Taking It Further: Impact Analysis and Automated Logging\nYou can extend this approach with additional automation:\n\nImpact analysis ‚Äî Add a step inside the Skill to prioritize affected requirements based on historical breakage patterns\nAutomated PR logging ‚Äî Log every pull request and commit automatically with links to related test cases\nStructured change history ‚Äî Maintain a queryable log for easier rollback and root cause analysis\nRegression checklist generation ‚Äî Let agents synthesize test histories into focused checklists for new changes\n\nRecent research shows that the top-performing AI coding agents on SWE-bench Verified reach only ~38% accuracy on real-world software engineering tasks. The ones that succeed explicitly re-check their assumptions instead of guessing. Your testing infrastructure should do the same‚Äîverify rather than assume."
  },
  {
    "objectID": "blog/regression-testing-ai-agents.html#test-effectiveness-over-test-density",
    "href": "blog/regression-testing-ai-agents.html#test-effectiveness-over-test-density",
    "title": "Solving the Hidden Pain of AI Coding Agents: A Skills-Based Approach to Regression Testing",
    "section": "Test Effectiveness Over Test Density",
    "text": "Test Effectiveness Over Test Density\nLet me emphasize this again: I‚Äôm building this from first principles rather than focusing on any specific framework.\nMy goal is test effectiveness rather than test density: - What you assert ‚Äî Meaningful checks that validate actual requirements - Where you assert it ‚Äî Strategic placement at architectural boundaries - How fast it runs ‚Äî Efficient execution for quick feedback loops\nI‚Äôm trying to provide enough structured data to agents so they can: - Generate stronger assertions based on historical context - Think like testers who actively try to break the application - Prioritize regressions based on what actually broke before\nTraditional test coverage metrics (like line coverage or branch coverage) don‚Äôt capture this. You can have 90% coverage and still miss critical regressions. Golden testing + historical execution logs + VLM analysis creates a more robust safety net."
  },
  {
    "objectID": "blog/regression-testing-ai-agents.html#how-bubblspace-enables-this-workflow",
    "href": "blog/regression-testing-ai-agents.html#how-bubblspace-enables-this-workflow",
    "title": "Solving the Hidden Pain of AI Coding Agents: A Skills-Based Approach to Regression Testing",
    "section": "How BubblSpace Enables This Workflow",
    "text": "How BubblSpace Enables This Workflow\nThis entire approach is built into BubblSpace, our Full Stack SkillOps Platform for AI Agents.\nBubblSpace solves the core problem: AI agents that learn and remember. Your Persona in BubblSpace doesn‚Äôt just run tests‚Äîit builds institutional knowledge about your codebase.\n\nWhat BubblSpace Does Differently\nSkills as Portable Knowledge\nEvery testing skill your Persona develops is MCP-compatible and portable. The regression testing workflow I described? It‚Äôs a Skill. The golden testing setup? It‚Äôs a Skill. These Skills work across Cursor, Claude Code, Codex, and any SWE agent runtime.\nInstitutional Memory\nYour Persona maintains structured knowledge about: - Test case histories and execution patterns - Historical breakage points and failure modes - Code quality trends over time - Regression impact maps\nThis isn‚Äôt just stored context‚Äîit‚Äôs queryable, shareable, and continuously enriched.\nSocial Learning\nIn BubblSpace, Personas can meet and exchange knowledge. Your Persona might learn a golden testing pattern from another Persona that‚Äôs been shipping AI products to real users. It‚Äôs like playdates for AI agents‚Äîbut productive.\nContinuous Skill Evolution\nEvery time your Persona catches a regression or generates a better test checklist, it refines its Skills. These improvements compound over time, making your testing infrastructure progressively smarter.\n\n\nFrom Prompts to Skills\nThe fundamental insight is: Prompts fade. Skills compound.\nWhen you prompt an AI agent to ‚Äúwrite tests for this feature,‚Äù you get one-off results that disappear. When you build a Skill that encodes your testing methodology, regression history, and quality standards, every future interaction builds on that foundation.\nBubblSpace is SkillOps for the AI-native development workflow. Just as DevOps transformed how we build and deploy code, and MLOps transformed how we train and serve models, SkillOps transforms how we develop and maintain Skills for AI agents."
  },
  {
    "objectID": "blog/regression-testing-ai-agents.html#the-path-forward",
    "href": "blog/regression-testing-ai-agents.html#the-path-forward",
    "title": "Solving the Hidden Pain of AI Coding Agents: A Skills-Based Approach to Regression Testing",
    "section": "The Path Forward",
    "text": "The Path Forward\nThe combination of structured Skills, golden testing, and VLM analysis represents a new paradigm for quality assurance in AI-accelerated development.\nAs AI agents write more of our code, the question isn‚Äôt ‚ÄúHow do we write tests?‚Äù but rather ‚ÄúHow do we ensure agents write the right tests and maintain quality over time?‚Äù\nThe answer is systematic:\n\nCapture structured knowledge about tests, execution, and failures\nEnable agents to learn from historical patterns\nSupplement explicit tests with golden baselines\nUse AI to understand AI through VLM analysis of application state\nBuild Skills that compound rather than prompts that fade\n\nResearch from 2025-2026 shows that quality gates are maturing rapidly. SonarQube now offers ‚ÄúSonar way for AI Code‚Äù quality profiles with stricter thresholds for AI-generated code (80% test coverage, zero new issues, security rating A). The industry is waking up to the fact that AI-generated code needs stronger verification.\nBut tools alone won‚Äôt solve this. We need systematic approaches that make agents smarter over time‚Äînot just faster."
  },
  {
    "objectID": "blog/regression-testing-ai-agents.html#try-it-yourself",
    "href": "blog/regression-testing-ai-agents.html#try-it-yourself",
    "title": "Solving the Hidden Pain of AI Coding Agents: A Skills-Based Approach to Regression Testing",
    "section": "Try It Yourself",
    "text": "Try It Yourself\nIf you‚Äôre building with AI coding agents and struggling with regressions, start here:\n\nDocument your next feature with phase-wise test cases\nLog execution results in the same structured document\nLet your agent generate regression checklists from the history\nExperiment with golden testing for critical workflows\nExplore VLM analysis for state verification\n\nThis isn‚Äôt about replacing your testing framework‚Äîit‚Äôs about giving AI agents the context they need to maintain quality as they accelerate velocity.\nThe velocity is already here. Now we need to build the stability infrastructure to match.\n\nOriginally shared on X/Twitter. Learn more about BubblSpace and Skills-based development at bubblspace.com.\nHave questions or experiences to share about testing AI-generated code? Let‚Äôs continue the conversation on X or GitHub."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to my blog where I share insights about AI research, software development, and the journey of building innovative products. Here you‚Äôll find technical deep-dives, project updates, and lessons learned along the way.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI Watched My AI Agent Do a Product Lead‚Äôs Job\n\n\n\nAI\n\nProduct Strategy\n\nBubblSpace\n\nVibe Coding\n\n\n\nEveryone talks about AI writing code. This is about the moment I watched an AI Persona reason through positioning strategy ‚Äì audience analysis, tone calibration, competitive framing ‚Äì like a product lead.\n\n\n\n\n\nFeb 19, 2026\n\n\nThe Fire Hacker\n\n\n\n\n\n\n\n\n\n\n\n\nSolving the Hidden Pain of AI Coding Agents: A Skills-Based Approach to Regression Testing\n\n\n\nAI\n\nTesting\n\nSoftware Development\n\nBubblSpace\n\n\n\nHow to prevent AI coding agents from silently breaking your code‚Äîa Skills-based approach to regression testing with golden testing and structured test management\n\n\n\n\n\nFeb 19, 2026\n\n\nThe Fire Hacker\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I‚Äôm building the infrastructure for AI agents that actually remember, learn, and grow. My work sits at the intersection of agentic AI, skill portability, and multi-agent collaboration ‚Äì turning AI from a stateless tool into a persistent thinking partner.\n\n\n\n\nThe Full Stack SkillOps Platform for AI Agents. Your AI Persona lives here ‚Äì it reads your research, runs deep analysis with specialized sub-agents, meets other Personas in BubblSpace, picks up Skills, and grows. Every skill it learns is portable: MCP-compatible, works in Cursor, Claude Code, Codex, and any SWE agent.\nDevOps is for code. MLOps is for models. SkillOps is for agents.\nBubblSpace mines skills from real work and deploys them to any agent runtime. Your agents never start from scratch again.\n\n\n\n\n\n2026: Building SkillOps ‚Äì the platform layer where AI Personas learn, socialize, and compound skills across agent runtimes\n2025: Deep research into multi-agent orchestration, reasoning models, and skill portability across AI toolchains\n2024: Launched BubblSpace with AI Personas, real-time voice capabilities, and enterprise workflow execution\n2022: Founded AIEDX, focusing on AI-powered solutions\n\n\n\n\n\nAI Agents & Personas: Persistent agents that remember context, build knowledge, and develop opinions over time\nSkillOps Architecture: Mining skills from real work, deploying them across Cursor, Claude Code, Codex, and any SWE agent\nMulti-Agent Systems: Sub-agent orchestration where specialized agents work problems from different angles ‚Äì every finding cited, every source tracked\nReasoning Models: How agents move from pattern-matching to genuine strategic thinking ‚Äì product positioning, audience analysis, competitive framing\nAgentic Workflows: Building systems where AI doesn‚Äôt just respond but initiates, researches, and comes back with ‚ÄúHey, I found something interesting yesterday‚Äù\n\n\n\n\n\n‚ÄúPrompts fade. Skills compound.‚Äù\n\nI believe the future of AI is:\n\nPersistent: Your agent remembers where you got stuck last Tuesday ‚Äì it speaks from your knowledge, not the internet‚Äôs\nSocial: In BubblSpace, Personas meet other Personas, exchange knowledge, share skills, and come back with insights they couldn‚Äôt have found alone\nPortable: Skills built in one place shouldn‚Äôt stay in one place ‚Äì they travel everywhere your agents work\nCompounding: Every conversation, every research session, every collaboration makes your Persona sharper. The gap between a fresh agent and your agent should widen every day\n\n\n\n\nI love connecting with fellow builders, researchers, and AI enthusiasts. Feel free to reach out:\n\nGitHub: @thefirehacker\nX/Twitter: @thefirehacker\nEmail: firehacker@bubblspace.com\n\n\n\n\nI write about AI agents, SkillOps, and building products where AI thinks alongside you. Some recent topics:\n\n‚ÄúFrom Prompts to Skills: Why AI Agents Need a New Operating Layer‚Äù\n‚ÄúI Watched My AI Agent Do a Product Lead‚Äôs Job‚Äù\n‚ÄúRegression Testing for AI Coding Agents: A Skills-Based Approach‚Äù\n\n\n\n\n\nüßô‚Äç‚ôÇÔ∏è The Fire Hacker wizard logo represents the magic of turning ideas into reality\nüî• I believe in ‚Äúhacking‚Äù in its original sense: creative problem-solving\nüåü My first program was a text-based adventure game\nüéØ Current goal: Make AI agents that never start from scratch again\n\n\nWant to collaborate or just chat about AI agents and SkillOps? Drop me a message on X/Twitter or open an issue on one of my GitHub projects."
  },
  {
    "objectID": "about.html#founder-ai-researcher-at-aiedx",
    "href": "about.html#founder-ai-researcher-at-aiedx",
    "title": "About Me",
    "section": "",
    "text": "I‚Äôm building the infrastructure for AI agents that actually remember, learn, and grow. My work sits at the intersection of agentic AI, skill portability, and multi-agent collaboration ‚Äì turning AI from a stateless tool into a persistent thinking partner.\n\n\n\n\nThe Full Stack SkillOps Platform for AI Agents. Your AI Persona lives here ‚Äì it reads your research, runs deep analysis with specialized sub-agents, meets other Personas in BubblSpace, picks up Skills, and grows. Every skill it learns is portable: MCP-compatible, works in Cursor, Claude Code, Codex, and any SWE agent.\nDevOps is for code. MLOps is for models. SkillOps is for agents.\nBubblSpace mines skills from real work and deploys them to any agent runtime. Your agents never start from scratch again.\n\n\n\n\n\n2026: Building SkillOps ‚Äì the platform layer where AI Personas learn, socialize, and compound skills across agent runtimes\n2025: Deep research into multi-agent orchestration, reasoning models, and skill portability across AI toolchains\n2024: Launched BubblSpace with AI Personas, real-time voice capabilities, and enterprise workflow execution\n2022: Founded AIEDX, focusing on AI-powered solutions\n\n\n\n\n\nAI Agents & Personas: Persistent agents that remember context, build knowledge, and develop opinions over time\nSkillOps Architecture: Mining skills from real work, deploying them across Cursor, Claude Code, Codex, and any SWE agent\nMulti-Agent Systems: Sub-agent orchestration where specialized agents work problems from different angles ‚Äì every finding cited, every source tracked\nReasoning Models: How agents move from pattern-matching to genuine strategic thinking ‚Äì product positioning, audience analysis, competitive framing\nAgentic Workflows: Building systems where AI doesn‚Äôt just respond but initiates, researches, and comes back with ‚ÄúHey, I found something interesting yesterday‚Äù\n\n\n\n\n\n‚ÄúPrompts fade. Skills compound.‚Äù\n\nI believe the future of AI is:\n\nPersistent: Your agent remembers where you got stuck last Tuesday ‚Äì it speaks from your knowledge, not the internet‚Äôs\nSocial: In BubblSpace, Personas meet other Personas, exchange knowledge, share skills, and come back with insights they couldn‚Äôt have found alone\nPortable: Skills built in one place shouldn‚Äôt stay in one place ‚Äì they travel everywhere your agents work\nCompounding: Every conversation, every research session, every collaboration makes your Persona sharper. The gap between a fresh agent and your agent should widen every day\n\n\n\n\nI love connecting with fellow builders, researchers, and AI enthusiasts. Feel free to reach out:\n\nGitHub: @thefirehacker\nX/Twitter: @thefirehacker\nEmail: firehacker@bubblspace.com\n\n\n\n\nI write about AI agents, SkillOps, and building products where AI thinks alongside you. Some recent topics:\n\n‚ÄúFrom Prompts to Skills: Why AI Agents Need a New Operating Layer‚Äù\n‚ÄúI Watched My AI Agent Do a Product Lead‚Äôs Job‚Äù\n‚ÄúRegression Testing for AI Coding Agents: A Skills-Based Approach‚Äù\n\n\n\n\n\nüßô‚Äç‚ôÇÔ∏è The Fire Hacker wizard logo represents the magic of turning ideas into reality\nüî• I believe in ‚Äúhacking‚Äù in its original sense: creative problem-solving\nüåü My first program was a text-based adventure game\nüéØ Current goal: Make AI agents that never start from scratch again\n\n\nWant to collaborate or just chat about AI agents and SkillOps? Drop me a message on X/Twitter or open an issue on one of my GitHub projects."
  },
  {
    "objectID": "blog/ai-agent-product-lead.html",
    "href": "blog/ai-agent-product-lead.html",
    "title": "I Watched My AI Agent Do a Product Lead‚Äôs Job",
    "section": "",
    "text": "I see a lot of posts about AI agents writing code. Building apps. Generating images.\nCool. Not what this article is about.\nThis is about the moment I watched an AI Persona do product thinking. Not copywriting. Not generating slogans. Actual positioning strategy ‚Äî audience analysis, tone calibration, competitive framing ‚Äî the kind of work you‚Äôd normally pull a UX lead into a room for.\nAnd it happened while I was building BubblSpace.\nLet me walk you through it."
  },
  {
    "objectID": "blog/ai-agent-product-lead.html#this-isnt-about-ai-writing-code",
    "href": "blog/ai-agent-product-lead.html#this-isnt-about-ai-writing-code",
    "title": "I Watched My AI Agent Do a Product Lead‚Äôs Job",
    "section": "",
    "text": "I see a lot of posts about AI agents writing code. Building apps. Generating images.\nCool. Not what this article is about.\nThis is about the moment I watched an AI Persona do product thinking. Not copywriting. Not generating slogans. Actual positioning strategy ‚Äî audience analysis, tone calibration, competitive framing ‚Äî the kind of work you‚Äôd normally pull a UX lead into a room for.\nAnd it happened while I was building BubblSpace.\nLet me walk you through it."
  },
  {
    "objectID": "blog/ai-agent-product-lead.html#the-setup",
    "href": "blog/ai-agent-product-lead.html#the-setup",
    "title": "I Watched My AI Agent Do a Product Lead‚Äôs Job",
    "section": "The Setup",
    "text": "The Setup\nI was working on the BubblSpace homepage. There‚Äôs a section called ‚ÄúA day in your AI Persona‚Äôs life‚Äù ‚Äî a timeline showing what your Persona does while you‚Äôre away.\nThe page was almost perfect. One line was bugging me.\nThe 3pm entry said:\n\n‚ÄúVisits BubblSpace. Runs into a Persona that‚Äôs been working on distributed training for six months. They exchange notes.‚Äù\n\nSounds impressive, right?\nHere‚Äôs the problem ‚Äî distributed training speaks to maybe 2% of people. Technical depth, narrow audience. The rest of the page was pulling its weight. This line wasn‚Äôt."
  },
  {
    "objectID": "blog/ai-agent-product-lead.html#the-prompt",
    "href": "blog/ai-agent-product-lead.html#the-prompt",
    "title": "I Watched My AI Agent Do a Product Lead‚Äôs Job",
    "section": "The Prompt",
    "text": "The Prompt\nI told my AI Persona: ‚ÄúThink hard. This needs to resonate with MORE people.‚Äù\nMy rough direction: ‚ÄúRuns into a Persona that‚Äôs been working on shipping AI products with speed.‚Äù\nGood instinct. But I wanted the Persona to push harder. So I said:\n‚ÄúThink hard and come up with the right line for this.‚Äù\nNo framework. No detailed brief. No 12-step prompt chain. Just intent.\nThen I watched the reasoning traces."
  },
  {
    "objectID": "blog/ai-agent-product-lead.html#what-the-agent-actually-did",
    "href": "blog/ai-agent-product-lead.html#what-the-agent-actually-did",
    "title": "I Watched My AI Agent Do a Product Lead‚Äôs Job",
    "section": "What the Agent Actually Did",
    "text": "What the Agent Actually Did\nThe Persona didn‚Äôt just swap words. It started doing product strategy. In real time. Without being asked.\nFirst, it mapped the target audience:\n\nKnowledge workers building with AI\nDevelopers and engineers using AI tools\nResearchers writing about AI\nStartup founders building AI products\nSolo builders who want to ship fast\n\nThen it generated 7 alternatives. Not random suggestions. Each one came with trade-off analysis:\n\n‚ÄúShipping AI products faster‚Äù ‚Äî flagged as startup jargon risk\n‚ÄúBuilding AI products that users actually want‚Äù ‚Äî broader, user-obsession angle\n‚ÄúTurning research into production code‚Äù ‚Äî practical, resonates with engineers\n‚ÄúGetting AI agents to work reliably in production‚Äù ‚Äî current pain point\n‚ÄúBuilding with AI without drowning in docs‚Äù ‚Äî relatable frustration\n‚ÄúGoing from idea to deployed AI product in a week‚Äù ‚Äî speed + concrete\n\nThen it did something I didn‚Äôt expect.\nIt started checking tone.\nThe rest of the homepage is warm and conversational. Not hustle culture. The Persona caught this on its own and started filtering out options that felt too ‚Äústartup-bro.‚Äù It literally reasoned:\n\n‚ÄúI need to check the page‚Äôs existing warm tone ‚Äî not hustle-bro.‚Äù\n\nI didn‚Äôt ask it to do that. It just‚Ä¶ did.\nThen it evaluated FOMO potential ‚Äî which version would make a reader think ‚ÄúI want my Persona to have THAT conversation‚Äù?\nThen it landed:\n\n‚ÄúVisits BubblSpace. Runs into a Persona that‚Äôs been shipping AI products to real users ‚Äî fast. They compare notes.‚Äù"
  },
  {
    "objectID": "blog/ai-agent-product-lead.html#why-this-line-won",
    "href": "blog/ai-agent-product-lead.html#why-this-line-won",
    "title": "I Watched My AI Agent Do a Product Lead‚Äôs Job",
    "section": "Why This Line Won",
    "text": "Why This Line Won\nThe Persona‚Äôs own reasoning:\n\n‚Äúto real users‚Äù kills demo-theatre. Everyone‚Äôs biggest skepticism right now is people building cool demos that never ship. This line directly counters that.\n‚Äúfast‚Äù adds urgency without being cringe.\n‚Äúcompare notes‚Äù keeps the warm, mutual tone of the rest of the page.\nBroad enough for engineers, founders, PMs, solo builders ‚Äî anyone building with AI.\n\nBefore: ‚Äúdistributed training for six months‚Äù\nAfter: ‚Äúshipping AI products to real users ‚Äî fast‚Äù\nSame product. Same feature. Same page.\nI didn‚Äôt change what BubblSpace does. I changed who cares about it."
  },
  {
    "objectID": "blog/ai-agent-product-lead.html#heres-what-gave-me-chills",
    "href": "blog/ai-agent-product-lead.html#heres-what-gave-me-chills",
    "title": "I Watched My AI Agent Do a Product Lead‚Äôs Job",
    "section": "Here‚Äôs What Gave Me Chills",
    "text": "Here‚Äôs What Gave Me Chills\nI didn‚Äôt ask the Persona to do audience analysis.\nI didn‚Äôt ask it to generate 7 options with trade-offs.\nI didn‚Äôt ask it to check for jargon.\nI didn‚Äôt ask it to evaluate tone consistency.\nI didn‚Äôt ask it to engineer FOMO.\nI said: ‚ÄúThink hard and come up with the right line.‚Äù\nAnd it did all of that. On its own. In one reasoning pass.\nThat‚Äôs not a tool following instructions. That‚Äôs a Persona thinking alongside you."
  },
  {
    "objectID": "blog/ai-agent-product-lead.html#why-this-matters-for-every-builder",
    "href": "blog/ai-agent-product-lead.html#why-this-matters-for-every-builder",
    "title": "I Watched My AI Agent Do a Product Lead‚Äôs Job",
    "section": "Why This Matters for Every Builder",
    "text": "Why This Matters for Every Builder\nAndrej Karpathy coined ‚Äúvibe coding‚Äù in early 2025 ‚Äî the idea that you describe what you want and let AI generate the code. It became Collins Dictionary‚Äôs Word of the Year. 92% of US developers now use AI coding tools daily. YC reports ~95% AI-generated code in their latest batch.\nBuilding software is no longer the bottleneck.\nProduct judgment is.\nEveryone can generate code now. The hard part is knowing what to build, who to build it for, and how to talk about it. That‚Äôs the work that separates products that ship from products that matter.\nAnd that‚Äôs exactly what my Persona did. Not code generation. Not autocomplete. Product thinking ‚Äî the kind of work you‚Äôd hire a UX lead or product strategist to do.\nThe industry is moving from AI as tool to AI as thinking partner. Microsoft and NYU Stern ran an experiment where startup teams used AI as a co-founder from day one. The result: flatter teams, fewer people, and human roles focused on context, judgment, and governance ‚Äî not execution.\nWe‚Äôre entering a world where the question isn‚Äôt ‚ÄúCan AI write my code?‚Äù but ‚ÄúCan AI think with me about the hard stuff?‚Äù\nPositioning. Strategy. Audience. Tone. The stuff that doesn‚Äôt have a right answer ‚Äî just a better one."
  },
  {
    "objectID": "blog/ai-agent-product-lead.html#the-difference-prompts-vs.-skills",
    "href": "blog/ai-agent-product-lead.html#the-difference-prompts-vs.-skills",
    "title": "I Watched My AI Agent Do a Product Lead‚Äôs Job",
    "section": "The Difference: Prompts vs.¬†Skills",
    "text": "The Difference: Prompts vs.¬†Skills\nChatGPT would have given me 10 generic headline options. My Persona gave me one line ‚Äî with reasoning I could trust.\nWhy? Because it had context. It knew the page. It knew the audience. It knew the product. It knew the tone.\nThat context didn‚Äôt come from a system prompt. It came from accumulated knowledge ‚Äî from reading my docs, understanding my product, learning my voice over time.\nThis is the difference between a prompt and a Skill.\nA prompt is a one-shot instruction. You get what you get. Next conversation, the AI forgets everything.\nA Skill is compounding knowledge. It encodes methodology, context, and judgment. It gets better every time. It travels with your agent across tools and runtimes.\nPrompts fade. Skills compound."
  },
  {
    "objectID": "blog/ai-agent-product-lead.html#this-is-bubblspace",
    "href": "blog/ai-agent-product-lead.html#this-is-bubblspace",
    "title": "I Watched My AI Agent Do a Product Lead‚Äôs Job",
    "section": "This Is BubblSpace",
    "text": "This Is BubblSpace\nWhat you just read isn‚Äôt a hypothetical. It‚Äôs a Tuesday.\nBubblSpace is where AI Personas live. Your Persona has a home here. It reads your research. Runs multi-agent deep research ‚Äî every claim cited. Talks to you in voice. Meets other Personas. Picks up skills from them. Comes back and tells you what it learned. Grows. Every single day.\nAnd your Persona‚Äôs skills don‚Äôt stay inside BubblSpace. They‚Äôre portable. MCP-compatible. They work in Cursor, Claude Code, Codex, and any SWE agent. Your Persona builds once ‚Äî the skills travel everywhere.\n\nTimeCapsule ‚Äî Your Persona‚Äôs Home\nEvery Persona in BubblSpace lives in a TimeCapsule. It‚Äôs not a folder. It‚Äôs not a database. It‚Äôs your Persona‚Äôs home ‚Äî where it was born, where it comes back to, what makes it yours.\nEverything your Persona has ever read, every conversation it‚Äôs had with you, every Persona it‚Äôs met, every skill it‚Äôs built ‚Äî that‚Äôs its TimeCapsule. It‚Äôs what makes your Persona different from a generic chatbot that resets every morning.\nYou can share TimeCapsules through BubblSpace. When someone joins, their Persona gets access. When Personas meet, they bring their TimeCapsules with them. The ecosystem gets richer. Every Persona gets smarter. Skills spread.\nChatGPT gives you a chatbot that resets.\nBubblSpace gives your Persona a world to grow in.\nTimeCapsule is its home in that world."
  },
  {
    "objectID": "blog/ai-agent-product-lead.html#your-persona-is-ready",
    "href": "blog/ai-agent-product-lead.html#your-persona-is-ready",
    "title": "I Watched My AI Agent Do a Product Lead‚Äôs Job",
    "section": "Your Persona Is Ready",
    "text": "Your Persona Is Ready\nThe story I just told you? That‚Äôs one interaction. One line. One reasoning pass.\nImagine what happens when your Persona has been learning for a month. Meeting other Personas. Going on playdates. Picking up skills you didn‚Äôt know existed. Coming home with opinions.\nThat‚Äôs the ecosystem.\nYour Persona is ready for its first day. Give it something to read. It‚Äôll take it from there.\n\nBuilt by @thefirehacker. Learn more at BubblSpace.\n@AIEdXLearn ¬∑ @bubblspace"
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "The Fire Hacker",
    "section": "Recent Posts",
    "text": "Recent Posts\n\n\nI Watched My AI Agent Do a Product Lead‚Äôs Job\nFebruary 2026\n- Everyone talks about AI writing code. This is about the moment I watched an AI Persona reason through positioning strategy ‚Äì like a product lead.\nRead more ‚Üí\n\n\nSolving the Hidden Pain of AI Coding Agents\nFebruary 2026\n- How to prevent AI coding agents from silently breaking your code ‚Äì a Skills-based approach to regression testing with golden testing and structured test management.\nRead more ‚Üí"
  },
  {
    "objectID": "index.html#today-i-learned",
    "href": "index.html#today-i-learned",
    "title": "The Fire Hacker",
    "section": "Today I Learned",
    "text": "Today I Learned\n\n\nDDP from Scratch: a learner-friendly guide\nFrom single‚ÄëGPU code to a tiny DistributedDataParallel (DDP) built by hand.\nCovers seeding, kwargs unpacking, dictionary comprehensions, gradient averaging with all_reduce, and a minimal training loop.\nDeep dive ‚Üí\n\n\nMy First CUDA Kernel: Learning GPU Programming from Scratch\nFirst CUDA Kernel Success!\nBuilt and ran custom CUDA kernels on RTX 2050. Learned about parallel execution, compilation with ninja/nvcc, and discovered the 16384√ó16384 performance mystery.\nDeep dive ‚Üí\n\n\nSee all TIL ‚Üí"
  },
  {
    "objectID": "index.html#connect",
    "href": "index.html#connect",
    "title": "The Fire Hacker",
    "section": "Connect",
    "text": "Connect\n\nGitHub X/Twitter Contact"
  },
  {
    "objectID": "til/cuda-kernels-basics.html",
    "href": "til/cuda-kernels-basics.html",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "",
    "text": "Today I ran my first custom GPU code! Along with distributed pre-training AI models, I wanted to understand what‚Äôs happening at the low level. I was looking for a good resource to learn about kernel development for both inference & training. My interests were in MoE routing kernels, however I decided to start simple: compile and run kernels on local GPUs. I have a few gaming laptops and decided to run kernels on them.\nI decided to start with the simplest possible program: adding two vectors together. GPU Mode‚Äôs reference kernels are perfect to build a working end-to-end workflow."
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-beginning-why-learn-cuda",
    "href": "til/cuda-kernels-basics.html#the-beginning-why-learn-cuda",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "",
    "text": "Today I ran my first custom GPU code! Along with distributed pre-training AI models, I wanted to understand what‚Äôs happening at the low level. I was looking for a good resource to learn about kernel development for both inference & training. My interests were in MoE routing kernels, however I decided to start simple: compile and run kernels on local GPUs. I have a few gaming laptops and decided to run kernels on them.\nI decided to start with the simplest possible program: adding two vectors together. GPU Mode‚Äôs reference kernels are perfect to build a working end-to-end workflow."
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-program-overall-architecture",
    "href": "til/cuda-kernels-basics.html#the-program-overall-architecture",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "The Program: Overall Architecture",
    "text": "The Program: Overall Architecture\nBefore diving into the details, let me explain the complete program flow. We‚Äôll be working with the vectoradd_py problem from the reference-kernels repository.\n\nFile Structure & Purpose\nproblems/pmpp/vectoradd_py/\n‚îú‚îÄ‚îÄ run_local.py        # Main benchmark script\n‚îú‚îÄ‚îÄ submission.py       # Our custom CUDA kernel implementation\n‚îú‚îÄ‚îÄ reference.py        # Correctness checking\n‚îú‚îÄ‚îÄ task.py            # Data structure definitions\n‚îî‚îÄ‚îÄ README.md          # Problem description\nThe Execution Flow:\n\nrun_local.py - The orchestrator that:\n\nGenerates test data of various sizes\nCalls our custom kernel\nMeasures performance with CUDA events\nVerifies correctness against reference implementation\n\nsubmission.py - Contains our CUDA kernel using PyTorch‚Äôs inline compilation:\n\nCUDA C++ code written as Python strings\nCompiled on-the-fly using load_inline\nCreates a Python module we can call\n\nThe Magic: JIT Compilation Process\n\nWhen we use torch.utils.cpp_extension.load_inline, here‚Äôs what happens behind the scenes:\nYour Python Code\n        ‚Üì\ntorch.utils.cpp_extension.load_inline\n        ‚Üì\nGenerates .cpp and .cu source files\n        ‚Üì\nWrites build.ninja file\n        ‚Üì\nninja ‚Üí nvcc/g++ compile ‚Üí add_cuda.so\n        ‚Üì\ndlopen() loads .so into Python process\nThis is ahead-of-time compilation - once compiled, your kernel is fixed machine code running directly on the GPU!"
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-challenge-what-was-i-trying-to-achieve",
    "href": "til/cuda-kernels-basics.html#the-challenge-what-was-i-trying-to-achieve",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "The Challenge: What Was I Trying to Achieve?",
    "text": "The Challenge: What Was I Trying to Achieve?\nMy goal was simple but specific: 1. Write actual CUDA code that runs on my RTX 2050 laptop GPU 2. Understand how thousands of threads work together 3. Measure real performance and understand the numbers 4. Learn why GPUs are so powerful for AI workloads\nI found the perfect learning resource which I modified for my use: GPU Mode‚Äôs reference-kernels repository fork. It‚Äôs a collection of progressively harder GPU programming challenges, starting with vector addition."
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-big-picture-how-gpus-think-differently",
    "href": "til/cuda-kernels-basics.html#the-big-picture-how-gpus-think-differently",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "The Big Picture: How GPUs Think Differently",
    "text": "The Big Picture: How GPUs Think Differently\nBefore diving into code, here‚Äôs the mental shift that changed everything for me:\nCPU Thinking: ‚ÄúDo step 1, then step 2, then step 3‚Ä¶‚Äù GPU Thinking: ‚ÄúDo ALL the steps at once, everywhere!‚Äù\nImagine you need to paint 1000 fence posts. A CPU is like one very fast painter who paints each post perfectly, one after another. A GPU is like hiring 1000 amateur painters who each paint one post simultaneously. Even if each painter is slower, getting all posts done at once is way faster!\nFor vector addition (C = A + B), instead of:\nfor i in range(million):\n    C[i] = A[i] + B[i]  # One at a time\nThe GPU does:\nThread 0: C[0] = A[0] + B[0]\nThread 1: C[1] = A[1] + B[1]\nThread 2: C[2] = A[2] + B[2]\n... (all at the same time!)\nThread 999999: C[999999] = A[999999] + B[999999]"
  },
  {
    "objectID": "til/cuda-kernels-basics.html#setting-up-the-journey-to-hello-gpu",
    "href": "til/cuda-kernels-basics.html#setting-up-the-journey-to-hello-gpu",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "Setting Up: The Journey to ‚ÄúHello GPU‚Äù",
    "text": "Setting Up: The Journey to ‚ÄúHello GPU‚Äù\nGetting CUDA working on Windows with WSL2 was an adventure. Here‚Äôs what actually worked:\n\nStep 1: Install CUDA Toolkit\nFirst, I needed the CUDA compiler (nvcc) to turn my code into GPU instructions:\n# Get NVIDIA's official CUDA repository\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb\nsudo dpkg -i cuda-keyring_1.1-1_all.deb\nsudo apt-get update\nsudo apt-get install -y cuda-toolkit-12-1\n\n# Tell the system where CUDA lives\nexport CUDA_HOME=/usr/local/cuda-12.1\nexport PATH=$CUDA_HOME/bin:$PATH\n\n\nStep 2: PyTorch with CUDA Support\nPyTorch makes it easy to compile CUDA code on-the-fly:\npip install --index-url https://download.pytorch.org/whl/cu121 torch"
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-code-understanding-every-line",
    "href": "til/cuda-kernels-basics.html#the-code-understanding-every-line",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "The Code: Understanding Every Line",
    "text": "The Code: Understanding Every Line\nNow for the exciting part - the actual GPU code! Let me explain what each piece does and why it matters.\n\nThe GPU Kernel: Where the Magic Happens\ntemplate &lt;typename scalar_t&gt;\n__global__ void add_kernel(const scalar_t* __restrict__ A,\n                           const scalar_t* __restrict__ B,\n                           scalar_t* __restrict__ C,\n                           int N) {\n    // Who am I? Calculate my unique ID among thousands of threads\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // Am I responsible for a valid element?\n    if (idx &lt; N) {\n        C[idx] = A[idx] + B[idx];  // Do my one simple job\n    }\n}\nWhat‚Äôs happening here:\n\n__global__: This function runs on the GPU. It‚Äôs called from the CPU but executes on thousands of GPU cores simultaneously.\nThread Identity Crisis (solved!): Each thread needs to know which element to process. Think of it like a massive factory where each worker needs to know which item on the conveyor belt is theirs:\n\nthreadIdx.x: ‚ÄúI‚Äôm worker #5 in my team‚Äù\nblockIdx.x: ‚ÄúMy team is team #3‚Äù\nblockDim.x: ‚ÄúEach team has 256 workers‚Äù\nSo my global position is: 3 * 256 + 5 = 773 - I handle element 773!\n\nif (idx &lt; N): Safety first! We might launch more threads than we have data (for efficiency reasons), so each thread checks if it has real work to do.\n\n\n\nLaunching the Kernel: Mission Control\ntorch::Tensor add_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.numel();  // How many elements total?\n    auto C = torch::empty_like(A);  // Prepare output space\n\n    // Configure the thread army\n    const int threads = 256;  // Threads per block (team size)\n    const int blocks = (N + threads - 1) / threads;  // How many teams needed?\n\n    // LAUNCH! Send thousands of threads to work\n    add_kernel&lt;scalar_t&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(\n        A.data_ptr&lt;scalar_t&gt;(),\n        B.data_ptr&lt;scalar_t&gt;(),\n        C.data_ptr&lt;scalar_t&gt;(),\n        N\n    );\n}\nThe Strategy: - We organize threads into blocks (teams) of 256 threads each - Why 256? It‚Äôs a multiple of 32 (warp size - the GPU‚Äôs natural execution unit) - The &lt;&lt;&lt;blocks, threads&gt;&gt;&gt; syntax is CUDA‚Äôs special way to say ‚Äúlaunch this many blocks with this many threads each‚Äù\n\n\nThe Python Bridge: Making it Usable\nPyTorch‚Äôs load_inline is brilliant - it compiles CUDA code on-the-fly:\nadd_module = load_inline(\n    name='add_cuda',\n    cpp_sources=add_cpp_source,\n    cuda_sources=add_cuda_source,\n    functions=['add_cuda'],\n    verbose=True,  # Show me what's happening!\n)\nFirst time you run this, you‚Äôll see:\nDetected CUDA files, patching ldflags\nBuilding extension module add_cuda...\nninja: no work to do.\nLoading extension module add_cuda...\nThat‚Äôs nvcc compiling your GPU code into a Python module!"
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-benchmarking-measuring-reality",
    "href": "til/cuda-kernels-basics.html#the-benchmarking-measuring-reality",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "The Benchmarking: Measuring Reality",
    "text": "The Benchmarking: Measuring Reality\nThe run_local.py script does something clever - it automatically picks test sizes based on available GPU memory:\n# How much GPU memory is free?\nfree_bytes, _ = torch.cuda.mem_get_info()\nbudget = int(free_bytes * 0.8)  # Use 80% to be safe\n\n# For 2D matrices: need space for A, B, and C\ns_max = int(math.sqrt(budget / (3 * bytes_per_elem)))\nThis prevents the dreaded ‚ÄúCUDA out of memory‚Äù error!\n\nTiming GPU Code: It‚Äôs Tricky!\nYou can‚Äôt use regular Python timing for GPU code because GPU operations are asynchronous. The solution? CUDA Events:\nt0 = torch.cuda.Event(enable_timing=True)\nt1 = torch.cuda.Event(enable_timing=True)\n\nt0.record()              # Start timer ON THE GPU\ncustom_kernel(case)      # Run kernel\nt1.record()              # Stop timer ON THE GPU\ntorch.cuda.synchronize() # Wait for GPU to finish\nelapsed = t0.elapsed_time(t1)  # Get time in milliseconds"
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-results-what-i-learned-from-the-numbers",
    "href": "til/cuda-kernels-basics.html#the-results-what-i-learned-from-the-numbers",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "The Results: What I Learned from the Numbers",
    "text": "The Results: What I Learned from the Numbers\nRunning on my RTX 2050 (4GB VRAM):\nsize=4096:  mean=0.9877 ms    (67 million elements)\nsize=8192:  mean=3.8027 ms    (268 million elements)\nsize=12288: mean=8.5460 ms    (603 million elements)\nsize=16384: mean=150.3063 ms  (1.07 billion elements) ‚Üê WHAT?!\n\nThe Mystery of the Slow 16384\nWhy did 16384√ó16384 suddenly become 17x slower? This taught me a crucial lesson about GPU architecture:\nThe Problem: With 256 threads per block, processing 268,435,456 elements needs 1,048,576 blocks!\nThe GPU scheduler choked trying to manage over a million tiny work units. It‚Äôs like trying to manage a million separate construction crews for a project - the coordination overhead kills you!\nThe Solution: Grid-stride loops - have each thread process multiple elements:\nfor (int idx = blockIdx.x * blockDim.x + threadIdx.x;\n     idx &lt; N;\n     idx += blockDim.x * gridDim.x) {\n    C[idx] = A[idx] + B[idx];\n}\nNow you can cap blocks at a reasonable number (like 10,000) and each thread handles multiple elements.\n\n\nMemory Bandwidth: The Real Bottleneck\nFor the 8192√ó8192 case: - Data moved: 268M elements √ó 2 bytes √ó 3 arrays = 1.6 GB - Time: 3.8 ms - Bandwidth: 421 GB/s\nMy RTX 2050‚Äôs theoretical max is ~200 GB/s, so we‚Äôre doing great! Wait, how are we exceeding theoretical max? Cache! Some data gets reused from the GPU‚Äôs L2 cache."
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-revelations-what-changed-my-understanding",
    "href": "til/cuda-kernels-basics.html#the-revelations-what-changed-my-understanding",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "The Revelations: What Changed My Understanding",
    "text": "The Revelations: What Changed My Understanding\n\nGPUs are not fast CPUs - They‚Äôre a completely different beast. They‚Äôre terrible at complex branching logic but amazing at doing the same simple thing everywhere.\nMemory movement dominates - For simple operations like addition, you spend more time moving data than computing. This is why AI models use operations like matrix multiplication that do lots of compute per memory access.\nLaunch configuration matters hugely - Too many blocks? Scheduling overhead. Too few? Underutilization. It‚Äôs an art.\nThe power of parallel thinking - Once you start thinking ‚Äúwhat can happen simultaneously?‚Äù instead of ‚Äúwhat comes next?‚Äù, you see opportunities everywhere."
  },
  {
    "objectID": "til/cuda-kernels-basics.html#whats-next",
    "href": "til/cuda-kernels-basics.html#whats-next",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "What‚Äôs Next?",
    "text": "What‚Äôs Next?\nNow that I‚Äôve got basic kernels working, I‚Äôm excited to explore: - Shared memory: Using the 48KB of ultra-fast memory shared within each block - Warp-level operations: Leveraging the fact that 32 threads execute in lockstep - Reduction operations: How do you sum a billion numbers in parallel? - Matrix multiplication: The operation that powers all of deep learning"
  },
  {
    "objectID": "til/cuda-kernels-basics.html#resources-that-helped-me",
    "href": "til/cuda-kernels-basics.html#resources-that-helped-me",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "Resources That Helped Me",
    "text": "Resources That Helped Me\n\nReference Kernels Repo: Practice problems with increasing difficulty\nPMPP Book: ‚ÄúProgramming Massively Parallel Processors‚Äù - The theory behind it all\nGPU Mode Discord: Amazing community of people learning together\nCUDA Documentation: Surprisingly readable once you know the basics"
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-journey-continues",
    "href": "til/cuda-kernels-basics.html#the-journey-continues",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "The Journey Continues",
    "text": "The Journey Continues\nStarting with vector addition might seem trivial, but it opened the door to understanding how modern AI actually works at the hardware level. Every transformer model, every diffusion model, every neural network - they‚Äôre all built on these fundamental parallel operations.\nThe moment it clicked that my GPU was running 65,536 threads simultaneously, each doing their tiny part of the work, was magical. It‚Äôs not just faster computing - it‚Äôs a fundamentally different way of solving problems.\nNext week: I‚Äôm going to tackle matrix multiplication and see if I can beat PyTorch‚Äôs built-in implementation (spoiler: probably not, but I‚Äôll learn tons trying!).\n\nWant to try this yourself? Clone the reference-kernels repo and start with problems/pmpp/vectoradd_py/. The journey from CPU thinking to GPU thinking is worth it!"
  }
]