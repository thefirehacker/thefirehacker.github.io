[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to my blog where I share insights about AI research, software development, and the journey of building innovative products. Here you‚Äôll find technical deep-dives, project updates, and lessons learned along the way.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform\n\n\n\nAI\n\nResearch\n\nEducation\n\nOpen Source\n\n\n\n\n\n\n\n\n\nJan 16, 2025\n\n\nThe Fire Hacker\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I‚Äôm passionate about democratizing AI and building tools that empower individuals and businesses to harness the power of artificial intelligence. My work focuses on making AI accessible, practical, and privacy-preserving.\n\n\n\n\nA revolutionary platform for creating and sharing AI-powered experiences in collaborative spaces. Think of it as Google Docs meets AI playground, where teams can work together with multiple AI models in real-time.\n\n\n\nSmall Language Models designed specifically for preserving and sharing memories through time. This project explores how we can use AI to create meaningful connections with our past and future selves.\n\n\n\n\n\n2025: Focused on AI research initiatives in distributed systems to build Efficient small models\n2024: BuildingBubblSpace: AI Pesronas with real time voice to voice capabiltiles to execute Enterprise Workflows\n2022: Founded AIEDX, focusing on local-first AI solutions\n\n\n\n\n\nSmall Language Models (SLMs): AI models that can run locally or small cluster of GPUs.\nDistributed Computing: Scaling AI across multiple nodes for both training and inference.\nAI-Frames: Open learning with the help of AI.\n\n\n\n\nI believe in giving back to the community. Check out my projects:\n\nTimeCapsule-SLM - Local-first memory preservation\n\n\n\n\n\n‚ÄúThe best AI is the one that runs on your device, respects your privacy, and enhances your capabilities without replacing your creativity.‚Äù\n\nI believe the future of AI is: - Local-first: Your data stays on your device - Collaborative: AI should enhance human collaboration - Accessible: Everyone should benefit from AI advances - Transparent: Understanding how AI makes decisions\n\n\n\nI love connecting with fellow developers, researchers, and AI enthusiasts. Feel free to reach out:\n\nGitHub: @thefirehacker\nX/Twitter: @thefirehacker\nEmail: firehacker@bubblspace.com\n\n\n\n\nI occasionally speak at conferences and write about AI, distributed systems, and building products. Some recent topics:\n\n‚ÄúSmall Models, Big Impact: The Future of Edge AI‚Äù\n‚ÄúBuilding Privacy-First AI Applications‚Äù\n‚ÄúFrom Research to Product: Shipping AI Features Users Love‚Äù\n\n\n\n\n\nüßô‚Äç‚ôÇÔ∏è The Fire Hacker wizard logo represents the magic of turning ideas into reality\nüî• I believe in ‚Äúhacking‚Äù in its original sense: creative problem-solving\nüåü My first program was a text-based adventure game\nüéØ Current goal: Make AI accessible to 1 million users\n\n\nWant to collaborate or just chat about AI? Drop me a message on X/Twitter or open an issue on one of my GitHub projects."
  },
  {
    "objectID": "about.html#founder-ai-researcher-at-aiedx",
    "href": "about.html#founder-ai-researcher-at-aiedx",
    "title": "About Me",
    "section": "",
    "text": "I‚Äôm passionate about democratizing AI and building tools that empower individuals and businesses to harness the power of artificial intelligence. My work focuses on making AI accessible, practical, and privacy-preserving.\n\n\n\n\nA revolutionary platform for creating and sharing AI-powered experiences in collaborative spaces. Think of it as Google Docs meets AI playground, where teams can work together with multiple AI models in real-time.\n\n\n\nSmall Language Models designed specifically for preserving and sharing memories through time. This project explores how we can use AI to create meaningful connections with our past and future selves.\n\n\n\n\n\n2025: Focused on AI research initiatives in distributed systems to build Efficient small models\n2024: BuildingBubblSpace: AI Pesronas with real time voice to voice capabiltiles to execute Enterprise Workflows\n2022: Founded AIEDX, focusing on local-first AI solutions\n\n\n\n\n\nSmall Language Models (SLMs): AI models that can run locally or small cluster of GPUs.\nDistributed Computing: Scaling AI across multiple nodes for both training and inference.\nAI-Frames: Open learning with the help of AI.\n\n\n\n\nI believe in giving back to the community. Check out my projects:\n\nTimeCapsule-SLM - Local-first memory preservation\n\n\n\n\n\n‚ÄúThe best AI is the one that runs on your device, respects your privacy, and enhances your capabilities without replacing your creativity.‚Äù\n\nI believe the future of AI is: - Local-first: Your data stays on your device - Collaborative: AI should enhance human collaboration - Accessible: Everyone should benefit from AI advances - Transparent: Understanding how AI makes decisions\n\n\n\nI love connecting with fellow developers, researchers, and AI enthusiasts. Feel free to reach out:\n\nGitHub: @thefirehacker\nX/Twitter: @thefirehacker\nEmail: firehacker@bubblspace.com\n\n\n\n\nI occasionally speak at conferences and write about AI, distributed systems, and building products. Some recent topics:\n\n‚ÄúSmall Models, Big Impact: The Future of Edge AI‚Äù\n‚ÄúBuilding Privacy-First AI Applications‚Äù\n‚ÄúFrom Research to Product: Shipping AI Features Users Love‚Äù\n\n\n\n\n\nüßô‚Äç‚ôÇÔ∏è The Fire Hacker wizard logo represents the magic of turning ideas into reality\nüî• I believe in ‚Äúhacking‚Äù in its original sense: creative problem-solving\nüåü My first program was a text-based adventure game\nüéØ Current goal: Make AI accessible to 1 million users\n\n\nWant to collaborate or just chat about AI? Drop me a message on X/Twitter or open an issue on one of my GitHub projects."
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "The Fire Hacker",
    "section": "Recent Posts",
    "text": "Recent Posts\n\n\nGetting Started with TimeCapsule-SLM\nJanuary 2025 - Learn how to use the AI-powered research and learning platform that democratizes knowledge discovery. Read more ‚Üí"
  },
  {
    "objectID": "index.html#today-i-learned",
    "href": "index.html#today-i-learned",
    "title": "The Fire Hacker",
    "section": "Today I Learned",
    "text": "Today I Learned\n\n\nCUDA Kernels from Scratch - Successfully compiled and ran my first GPU kernel on RTX 2050! Learned about blocks, threads, and why my 16384√ó16384 matrix was 17x slower. Read more ‚Üí"
  },
  {
    "objectID": "index.html#connect",
    "href": "index.html#connect",
    "title": "The Fire Hacker",
    "section": "Connect",
    "text": "Connect\n\nGitHub X/Twitter Contact"
  },
  {
    "objectID": "til.html",
    "href": "til.html",
    "title": "Today I Learned",
    "section": "",
    "text": "Deep dives into technical concepts I‚Äôm learning. From GPU programming to AI systems, these are comprehensive explorations of new technologies and techniques.\n\n\n\n\n\n\n\n\n\n\n\n\nLearning CUDA Kernels: From Zero to Running Custom GPU Code\n\n\n\n\n\n\nThe Fire Hacker\n\n\nJan 16, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/timecapsule-slm.html",
    "href": "blog/timecapsule-slm.html",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "",
    "text": "TimeCapsule-SLM is an innovative AI-powered research and learning platform that‚Äôs revolutionizing how we discover knowledge and collaborate on research. Built with privacy-first principles and cutting-edge AI technology, it democratizes access to powerful research tools while keeping your data secure and local."
  },
  {
    "objectID": "blog/timecapsule-slm.html#introduction",
    "href": "blog/timecapsule-slm.html#introduction",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "",
    "text": "TimeCapsule-SLM is an innovative AI-powered research and learning platform that‚Äôs revolutionizing how we discover knowledge and collaborate on research. Built with privacy-first principles and cutting-edge AI technology, it democratizes access to powerful research tools while keeping your data secure and local."
  },
  {
    "objectID": "blog/timecapsule-slm.html#what-is-timecapsule-slm",
    "href": "blog/timecapsule-slm.html#what-is-timecapsule-slm",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "What is TimeCapsule-SLM?",
    "text": "What is TimeCapsule-SLM?\nTimeCapsule-SLM combines the power of Small Language Models with advanced research capabilities to create a comprehensive platform for:\n\nResearchers seeking AI-assisted discovery and pattern recognition\nStudents looking for adaptive, personalized learning experiences\nTeachers creating interactive educational content\nTeams collaborating on knowledge discovery\n\nThe platform addresses critical challenges in modern education and research: - Research fragmentation across multiple sources - Inefficient learning workflows - Privacy concerns with cloud-based AI - Limited AI integration in educational settings - Resource constraints in low-bandwidth environments"
  },
  {
    "objectID": "blog/timecapsule-slm.html#core-features",
    "href": "blog/timecapsule-slm.html#core-features",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Core Features",
    "text": "Core Features\n\nüß† DeepResearch TimeCapsule\nTransform your research workflow with multi-agent AI collaboration:\n\nAI-Powered Discovery: Generate novel research ideas and hypotheses\nPattern Recognition: Uncover hidden connections in your data\nMulti-Agent System: Leverage specialized AI agents for different research tasks\nCollaborative Intelligence: Combine human expertise with AI insights\n\n\n\nüé• AI-Frames Interactive Learning\nCreate immersive, adaptive learning experiences:\n\nSequential Learning Paths: Build structured knowledge journeys\nMultimodal Content: Integrate videos, documents, and interactive elements\nAI-Guided Explanations: Get personalized help when you need it\nSelf-Paced Progress: Learn at your own speed with AI support\n\n\n\nüìö In-Browser RAG (Retrieval-Augmented Generation)\nExperience the power of semantic search without compromising privacy:\n\nLocal Vector Store: All processing happens in your browser\nOffline Capability: Works without internet after initial model load\nSemantic Understanding: Find information based on meaning, not just keywords\nPrivacy-First Design: Your documents never leave your device"
  },
  {
    "objectID": "blog/timecapsule-slm.html#getting-started",
    "href": "blog/timecapsule-slm.html#getting-started",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Getting Started",
    "text": "Getting Started\n\nPrerequisites\nBefore installing TimeCapsule-SLM, ensure you have:\n# Node.js 18 or higher\nnode --version\n\n# npm or yarn package manager\nnpm --version\n\n# Git for cloning the repository\ngit --version\n\n\nInstallation\n\nClone the Repository:\n\ngit clone https://github.com/thefirehacker/TimeCapsule-SLM.git\ncd TimeCapsule-SLM\n\nInstall Dependencies:\n\nnpm install\n# or\nyarn install\n\nConfigure Environment:\n\ncp env.example .env.local\nEdit .env.local to configure your AI providers:\n# Optional: Add API keys for cloud models\nOPENAI_API_KEY=your_key_here\n\n# Local model configuration (Ollama)\nOLLAMA_HOST=http://localhost:11434\n\nStart the Development Server:\n\nnpm run dev\n# or\nyarn dev\nVisit http://localhost:3000 to access TimeCapsule-SLM!"
  },
  {
    "objectID": "blog/timecapsule-slm.html#setting-up-local-ai-models",
    "href": "blog/timecapsule-slm.html#setting-up-local-ai-models",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Setting Up Local AI Models",
    "text": "Setting Up Local AI Models\nFor the best privacy and offline experience, use local models with Ollama:\n\nInstall Ollama\n# macOS/Linux\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# Windows - Download from ollama.ai\n\n\nPull Recommended Models\n# For research and general tasks\nollama pull gemma:2b\n\n# For code and technical content\nollama pull qwen2.5:3b\n\n# For creative writing\nollama pull llama3.2:3b"
  },
  {
    "objectID": "blog/timecapsule-slm.html#using-timecapsule-slm",
    "href": "blog/timecapsule-slm.html#using-timecapsule-slm",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Using TimeCapsule-SLM",
    "text": "Using TimeCapsule-SLM\n\nCreating Your First Research Project\n\nInitialize a Knowledge Base:\n\nClick ‚ÄúNew Project‚Äù\nChoose your domain (Research, Education, Personal)\nSelect your preferred AI model\n\nImport Your Documents:\n\nDrag and drop PDFs, Word docs, or text files\nThe system will automatically index and embed them\nAll processing happens locally in your browser\n\nStart Researching:\n\nUse natural language queries to explore your knowledge base\nThe AI will surface relevant information and suggest connections\nGenerate summaries, insights, and new research directions\n\n\n\n\nBuilding AI-Frames for Learning\nCreate interactive learning experiences with AI-Frames:\n// Example AI-Frame configuration\n{\n  \"title\": \"Introduction to Quantum Computing\",\n  \"modules\": [\n    {\n      \"type\": \"video\",\n      \"content\": \"intro-video.mp4\",\n      \"ai_notes\": true\n    },\n    {\n      \"type\": \"interactive\",\n      \"content\": \"qubit-simulator\",\n      \"ai_guidance\": \"adaptive\"\n    },\n    {\n      \"type\": \"quiz\",\n      \"ai_generated\": true,\n      \"difficulty\": \"progressive\"\n    }\n  ]\n}\n\n\nCollaborative Features\nTimeCapsule-SLM supports real-time collaboration:\n\nShared Workspaces: Invite team members to research projects\nLive AI Sessions: Collaborate with AI assistance in real-time\nKnowledge Graphs: Visualize connections discovered by your team\nVersion Control: Track changes and contributions"
  },
  {
    "objectID": "blog/timecapsule-slm.html#architecture-technology",
    "href": "blog/timecapsule-slm.html#architecture-technology",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Architecture & Technology",
    "text": "Architecture & Technology\nTimeCapsule-SLM is built with modern, performant technologies:\n\nFrontend: Next.js 15, React 19, TypeScript\nAI Integration: Support for Ollama, OpenAI, and local models\nDatabase: RxDB for offline-first data persistence\nVector Store: In-browser embeddings with WebAssembly\nAuthentication: NextAuth.js for secure access"
  },
  {
    "objectID": "blog/timecapsule-slm.html#privacy-security",
    "href": "blog/timecapsule-slm.html#privacy-security",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Privacy & Security",
    "text": "Privacy & Security\nYour privacy is our priority:\n\nLocal-First: All sensitive processing happens on your device\nNo Telemetry: We don‚Äôt track your usage or collect data\nOpen Source: Audit the code yourself (Apache 2.0 License)\nEncryption: Local data is encrypted at rest\nControl: You decide what stays local vs.¬†what uses cloud services"
  },
  {
    "objectID": "blog/timecapsule-slm.html#use-cases",
    "href": "blog/timecapsule-slm.html#use-cases",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Use Cases",
    "text": "Use Cases\n\nFor Researchers\n\nLiterature reviews with AI-powered synthesis\nPattern discovery in research data\nHypothesis generation and validation\nCollaborative paper writing\n\n\n\nFor Students\n\nPersonalized study guides\nAI tutoring for complex topics\nInteractive learning paths\nExam preparation with adaptive quizzes\n\n\n\nFor Teachers\n\nCreate engaging course content\nBuild interactive lessons\nTrack student progress\nGenerate assessments automatically\n\n\n\nFor Teams\n\nKnowledge management\nCollaborative research\nTraining materials\nDocumentation with AI assistance"
  },
  {
    "objectID": "blog/timecapsule-slm.html#performance-tips",
    "href": "blog/timecapsule-slm.html#performance-tips",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Performance Tips",
    "text": "Performance Tips\nOptimize TimeCapsule-SLM for your hardware:\n\nModel Selection: Choose smaller models (2-3B parameters) for faster responses\nCaching: Enable browser caching for repeated queries\nBatch Processing: Process multiple documents simultaneously\nGPU Acceleration: Use WebGPU when available for faster inference"
  },
  {
    "objectID": "blog/timecapsule-slm.html#roadmap-future-features",
    "href": "blog/timecapsule-slm.html#roadmap-future-features",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Roadmap & Future Features",
    "text": "Roadmap & Future Features\nWe‚Äôre constantly improving TimeCapsule-SLM:\n\nMobile Apps: iOS and Android applications (Q2 2025)\nVoice Interface: Natural conversation with your knowledge base\nAdvanced Visualizations: 3D knowledge graphs and mind maps\nPlugin System: Extend functionality with custom modules\nFederated Learning: Collaborate without sharing raw data"
  },
  {
    "objectID": "blog/timecapsule-slm.html#contributing",
    "href": "blog/timecapsule-slm.html#contributing",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Contributing",
    "text": "Contributing\nTimeCapsule-SLM is open source and welcomes contributions:\n# Fork the repository\n# Create a feature branch\ngit checkout -b feature/amazing-feature\n\n# Make your changes\n# Run tests\nnpm test\n\n# Submit a pull request\nCheck our contribution guidelines for more details."
  },
  {
    "objectID": "blog/timecapsule-slm.html#community-support",
    "href": "blog/timecapsule-slm.html#community-support",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Community & Support",
    "text": "Community & Support\nJoin our growing community:\n\nGitHub Discussions: Technical questions and feature requests\nDiscord Server: Real-time chat with developers and users\nDocumentation: Comprehensive guides at timecapsule.bubblspace.com\nX/Twitter: Follow @thefirehacker for updates"
  },
  {
    "objectID": "blog/timecapsule-slm.html#conclusion",
    "href": "blog/timecapsule-slm.html#conclusion",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Conclusion",
    "text": "Conclusion\nTimeCapsule-SLM represents a new paradigm in AI-assisted research and learning. By combining powerful AI capabilities with privacy-first design and local-first architecture, we‚Äôre making advanced research tools accessible to everyone.\nWhether you‚Äôre a researcher pushing the boundaries of knowledge, a student seeking personalized learning, or a teacher creating engaging content, TimeCapsule-SLM empowers you to work smarter, not harder.\nStart your journey today and experience the future of AI-powered research and learning!\n\nReady to transform your research and learning workflow? Get started with TimeCapsule-SLM or visit timecapsule.bubblspace.com for more information.\nHave questions or feedback? Reach out on X/Twitter or open an issue on GitHub."
  },
  {
    "objectID": "til/cuda-kernels-basics.html",
    "href": "til/cuda-kernels-basics.html",
    "title": "Learning CUDA Kernels: From Zero to Running Custom GPU Code",
    "section": "",
    "text": "Today I successfully compiled and ran my first custom CUDA kernel on my RTX 2050 gaming laptop! Here‚Äôs everything I learned about GPU programming, from environment setup to understanding performance metrics."
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-journey-running-my-first-cuda-kernel",
    "href": "til/cuda-kernels-basics.html#the-journey-running-my-first-cuda-kernel",
    "title": "Learning CUDA Kernels: From Zero to Running Custom GPU Code",
    "section": "",
    "text": "Today I successfully compiled and ran my first custom CUDA kernel on my RTX 2050 gaming laptop! Here‚Äôs everything I learned about GPU programming, from environment setup to understanding performance metrics."
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-setup-wsl2-cuda-pytorch",
    "href": "til/cuda-kernels-basics.html#the-setup-wsl2-cuda-pytorch",
    "title": "Learning CUDA Kernels: From Zero to Running Custom GPU Code",
    "section": "The Setup: WSL2 + CUDA + PyTorch",
    "text": "The Setup: WSL2 + CUDA + PyTorch\nGetting CUDA to work on Windows through WSL2 was my first challenge. Here‚Äôs the complete path I took:\n\n1. Environment Configuration\n# Install CUDA Toolkit 12.1 (for nvcc compiler)\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb\nsudo dpkg -i cuda-keyring_1.1-1_all.deb\nsudo apt-get update\nsudo apt-get install -y cuda-toolkit-12-1\n\n# Set up environment variables\nexport CUDA_HOME=/usr/local/cuda-12.1\nexport PATH=$CUDA_HOME/bin:$PATH\nexport LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH\n\n\n2. PyTorch with CUDA Support\n# Install PyTorch with CUDA 12.1 support\npip install --index-url https://download.pytorch.org/whl/cu121 torch"
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-project-gpu-modes-reference-kernels",
    "href": "til/cuda-kernels-basics.html#the-project-gpu-modes-reference-kernels",
    "title": "Learning CUDA Kernels: From Zero to Running Custom GPU Code",
    "section": "The Project: GPU-Mode‚Äôs Reference Kernels",
    "text": "The Project: GPU-Mode‚Äôs Reference Kernels\nI worked with the reference-kernels repository, specifically the vectoradd_py problem. This repository contains practice problems for learning GPU kernel optimization."
  },
  {
    "objectID": "til/cuda-kernels-basics.html#understanding-the-cuda-kernel-code",
    "href": "til/cuda-kernels-basics.html#understanding-the-cuda-kernel-code",
    "title": "Learning CUDA Kernels: From Zero to Running Custom GPU Code",
    "section": "Understanding the CUDA Kernel Code",
    "text": "Understanding the CUDA Kernel Code\nLet me break down the submission.py file that implements vector addition on the GPU:\n\nThe CUDA Kernel Function\ntemplate &lt;typename scalar_t&gt;\n__global__ void add_kernel(const scalar_t* __restrict__ A,\n                           const scalar_t* __restrict__ B,\n                           scalar_t* __restrict__ C,\n                           int N) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (idx &lt; N) {\n        C[idx] = A[idx] + B[idx];\n    }\n}\nKey Concepts I Learned:\n\n__global__: This marks a function that runs on the GPU and is called from the CPU (host)\nThread Indexing:\n\nblockIdx.x: Which block this thread belongs to\nblockDim.x: Number of threads per block\nthreadIdx.x: Thread‚Äôs position within its block\nidx = blockIdx.x * blockDim.x + threadIdx.x: Global thread index calculation\n\n__restrict__: Tells compiler these pointers don‚Äôt overlap, enabling optimizations\nBoundary Check: if (idx &lt; N) prevents out-of-bounds memory access since we might launch more threads than elements\n\n\n\nThe Host Code (CPU Side)\ntorch::Tensor add_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.numel();  // Total number of elements\n    auto C = torch::empty_like(A);  // Allocate output tensor\n\n    const int threads = 256;  // Threads per block\n    const int blocks = (N + threads - 1) / threads;  // Number of blocks needed\n\n    AT_DISPATCH_FLOATING_TYPES_AND_HALF(A.scalar_type(), \"add_kernel\", ([&] {\n        add_kernel&lt;scalar_t&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(\n            A.data_ptr&lt;scalar_t&gt;(),\n            B.data_ptr&lt;scalar_t&gt;(),\n            C.data_ptr&lt;scalar_t&gt;(),\n            N\n        );\n    }));\n}\nWhat I Learned Here:\n\nBlock and Thread Configuration:\n\n256 threads per block is a common choice (multiple of 32, the warp size)\nCalculate blocks needed: (N + threads - 1) / threads (ceiling division)\n\nKernel Launch Syntax: &lt;&lt;&lt;blocks, threads&gt;&gt;&gt; is CUDA‚Äôs special syntax for launching kernels\nType Dispatching: AT_DISPATCH_FLOATING_TYPES_AND_HALF handles different data types (float32, float64, float16)\n\n\n\nPyTorch Integration\nThe really cool part is using PyTorch‚Äôs load_inline to compile CUDA code on-the-fly:\nadd_module = load_inline(\n    name='add_cuda',\n    cpp_sources=add_cpp_source,\n    cuda_sources=add_cuda_source,\n    functions=['add_cuda'],\n    verbose=True,\n)\nThis compiles the CUDA code using nvcc behind the scenes and creates a Python module!"
  },
  {
    "objectID": "til/cuda-kernels-basics.html#understanding-the-benchmarking-script",
    "href": "til/cuda-kernels-basics.html#understanding-the-benchmarking-script",
    "title": "Learning CUDA Kernels: From Zero to Running Custom GPU Code",
    "section": "Understanding the Benchmarking Script",
    "text": "Understanding the Benchmarking Script\nThe run_local.py script taught me about proper GPU performance measurement:\n\nKey Components:\n\nAutomatic Size Detection:\n\n# Pick sizes that fit in VRAM (80% of available)\nfree_bytes, _ = torch.cuda.mem_get_info()\nbudget = int(free_bytes * 0.8)\n\nif dims == 2:\n    # For 2D tensors (matrices)\n    s_max = int(math.sqrt(budget / (3 * bytes_per_elem)))\nThis prevents out-of-memory errors by choosing appropriate test sizes based on available VRAM.\n\nCUDA Event Timing:\n\nt0 = torch.cuda.Event(enable_timing=True)\nt1 = torch.cuda.Event(enable_timing=True)\nt0.record()\ncustom_kernel(case)  # Run the kernel\nt1.record()\ntorch.cuda.synchronize()\nelapsed = t0.elapsed_time(t1)  # Time in milliseconds\nWhy CUDA Events? Regular Python timing wouldn‚Äôt work because CUDA operations are asynchronous. CUDA events measure time on the GPU itself.\n\nWarmup Runs: The script does 10 warmup iterations before timing. This is crucial because:\n\nFirst kernel launch includes JIT compilation overhead\nGPU needs to ‚Äúwarm up‚Äù to reach peak performance clocks"
  },
  {
    "objectID": "til/cuda-kernels-basics.html#my-results-on-rtx-2050",
    "href": "til/cuda-kernels-basics.html#my-results-on-rtx-2050",
    "title": "Learning CUDA Kernels: From Zero to Running Custom GPU Code",
    "section": "My Results on RTX 2050",
    "text": "My Results on RTX 2050\nDetected dims=2, free‚âà3.47 GB, using sizes=[4096, 8192, 12288, 16384]\nsize=4096:  mean=0.9877 ms, best=0.9461 ms, worst=1.2061 ms\nsize=8192:  mean=3.8027 ms, best=3.7571 ms, worst=3.9188 ms\nsize=12288: mean=8.5460 ms, best=8.4602 ms, worst=8.8747 ms\nsize=16384: mean=150.3063 ms, best=144.3062 ms, worst=169.5191 ms\n\nPerformance Analysis:\n\nLinear Scaling (mostly): 4096‚Üí8192 (4x elements) took ~4x time, which is expected\nThe 16384 Anomaly: Suddenly 17x slower! This taught me about:\n\nBlock Scheduling Overhead: With 256 threads/block, 16384√ó16384 elements need 1,048,576 blocks!\nSolution: Use grid-stride loops where each thread processes multiple elements\n\nMemory Bandwidth: For simple operations like addition, we‚Äôre memory-bound:\n\n8192√ó8192 elements √ó 2 bytes √ó 3 tensors = 402 MB\n3.76 ms ‚Üí ~107 GB/s effective bandwidth"
  },
  {
    "objectID": "til/cuda-kernels-basics.html#key-lessons-learned",
    "href": "til/cuda-kernels-basics.html#key-lessons-learned",
    "title": "Learning CUDA Kernels: From Zero to Running Custom GPU Code",
    "section": "Key Lessons Learned",
    "text": "Key Lessons Learned\n\nGPU Programming is Different: You think in terms of thousands of parallel threads, not sequential loops\nMemory is King: Most simple kernels are limited by memory bandwidth, not compute\nLaunch Configuration Matters: Too many blocks can cause scheduling overhead\nMeasure Correctly: Always use CUDA events for timing, not CPU timers\nCheck Boundaries: GPU won‚Äôt stop you from accessing invalid memory - it‚Äôll just crash!\nStart Simple: Vector addition is perfect for learning because it‚Äôs simple enough to understand but shows all core concepts"
  },
  {
    "objectID": "til/cuda-kernels-basics.html#next-steps",
    "href": "til/cuda-kernels-basics.html#next-steps",
    "title": "Learning CUDA Kernels: From Zero to Running Custom GPU Code",
    "section": "Next Steps",
    "text": "Next Steps\nNow that I understand the basics, I want to explore: - Shared memory optimization - Warp-level primitives - More complex kernels (matrix multiplication, reductions) - Performance profiling with Nsight"
  },
  {
    "objectID": "til/cuda-kernels-basics.html#resources-for-beginners",
    "href": "til/cuda-kernels-basics.html#resources-for-beginners",
    "title": "Learning CUDA Kernels: From Zero to Running Custom GPU Code",
    "section": "Resources for Beginners",
    "text": "Resources for Beginners\n\nRepository: reference-kernels - Great practice problems\nPMPP Book: ‚ÄúProgramming Massively Parallel Processors‚Äù - The theory behind these kernels\nCUDA Documentation: docs.nvidia.com/cuda - Official reference\nGPU Mode Discord: Community of learners working through these problems together"
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-aha-moment",
    "href": "til/cuda-kernels-basics.html#the-aha-moment",
    "title": "Learning CUDA Kernels: From Zero to Running Custom GPU Code",
    "section": "The ‚ÄúAha!‚Äù Moment",
    "text": "The ‚ÄúAha!‚Äù Moment\nThe biggest realization: A GPU is not just a fast CPU. It‚Äôs a completely different architecture that requires rethinking how we approach problems. Instead of ‚Äúdo this step by step,‚Äù we think ‚Äúdo this everywhere at once.‚Äù\nWhen that mental shift clicked, suddenly the weird syntax and concepts started making sense. Every thread knows its identity (index) and does its small part of the work. Together, thousands of threads solve the problem in parallel.\n\nThis learning journey continues! Follow along as I tackle more complex kernels and optimization techniques."
  }
]