[
  {
    "objectID": "til/ddp-python-basics.html",
    "href": "til/ddp-python-basics.html",
    "title": "DDP from Scratch: a learner-friendly guide",
    "section": "",
    "text": "This note is part of the Scratch ‚Üí Scale series by Zachary Mueller (course link). We‚Äôll implement a toy DDP wrapper, explain why it works, and demystify two Python idioms you‚Äôll see everywhere: dictionary comprehensions and kwargs (argument unpacking)."
  },
  {
    "objectID": "til/ddp-python-basics.html#tldr",
    "href": "til/ddp-python-basics.html#tldr",
    "title": "DDP from Scratch: a learner-friendly guide",
    "section": "TL;DR",
    "text": "TL;DR\nüîë Core Python patterns explained:\n\nDictionary comprehensions: Transform raw data (lists, ints) into model-ready tensors in one elegant line ‚Äî {k: torch.tensor(v).to(device) for k, v in item.items()} converts HuggingFace dataset samples to GPU tensors with proper shapes.\nKwargs unpacking (**): Unpack dictionaries into named function arguments ‚Äî model(**batch) automatically maps dict keys to HuggingFace model‚Äôs forward() parameters like input_ids, attention_mask, labels.\nGradient averaging ‚öñÔ∏è learning rate scaling: Dividing gradients by world_size or scaling LR by 1/world_size are mathematically equivalent ‚Äî the choice is where in your algorithm the division happens: before the optimizer step (average gradients) or after (scale learning rate).\n\nüìã DDP essentials:\n\nSeed every process the same way before you create the model.\nAverage grads with dist.all_reduce(param.grad, op=SUM) then divide by world size.\nUse **kwargs to pass batches to models: model(**batch) works seamlessly with HuggingFace transformers."
  },
  {
    "objectID": "til/ddp-python-basics.html#visual-mental-model-of-distributed-training",
    "href": "til/ddp-python-basics.html#visual-mental-model-of-distributed-training",
    "title": "DDP from Scratch: a learner-friendly guide",
    "section": "0) Visual mental model of distributed training",
    "text": "0) Visual mental model of distributed training\nRank 0 (GPU0)      Rank 1 (GPU1)      ...\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ forward      ‚îÇ   ‚îÇ forward      ‚îÇ  (same model weights)\n‚îÇ loss.backward‚îÇ   ‚îÇ loss.backward‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n       ‚îÇ   grads            ‚îÇ   grads\n       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ all_reduce (SUM) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ (every rank gets sum of all grads)\n                    ‚îÇ\n              divide by world_size\n                    ‚îÇ\n                optimizer.step()"
  },
  {
    "objectID": "til/ddp-python-basics.html#seeding-making-model-replicas-identical",
    "href": "til/ddp-python-basics.html#seeding-making-model-replicas-identical",
    "title": "DDP from Scratch: a learner-friendly guide",
    "section": "1) Seeding: making model replicas identical",
    "text": "1) Seeding: making model replicas identical\nIdentical initialization across ranks is not optional. If rank 0 samples weights {W} and rank 1 samples different weights {W‚Äô}, averaging grads is meaningless. We seed each RNG per process, then construct the model.\ndef set_seed(seed: int = 43):\n    import random, numpy as np, torch\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n# In your entry point (each process runs this):\nset_seed(43)            # must happen BEFORE model creation\nmodel = build_model()   # identical on all ranks\n\nWhy no communication?\nEach process runs the exact same Python code with the same seeds ‚Üí same random draws ‚Üí identical parameters. No dist.broadcast is required to make them equal, though you can use broadcast to enforce equality (see ¬ß3).\n\nPitfall: Seeding after constructing the model doesn‚Äôt retroactively change weights."
  },
  {
    "objectID": "til/ddp-python-basics.html#two-python-idioms-youll-see-everywhere",
    "href": "til/ddp-python-basics.html#two-python-idioms-youll-see-everywhere",
    "title": "DDP from Scratch: a learner-friendly guide",
    "section": "2) Two Python idioms you‚Äôll see everywhere",
    "text": "2) Two Python idioms you‚Äôll see everywhere\n\n2.1 Dictionary comprehension ‚Äî Why we need this pattern\nThis line converts a HuggingFace dataset sample (lists/ints) into a batch dictionary of GPU tensors with an added batch dimension:\nitem = {k: torch.tensor(v).unsqueeze(0).to(device) for k, v in item.items()}\nWhy this transformation is essential:\nHuggingFace datasets return items as Python dicts with lists and ints:\nexample = {\"input_ids\": [101, 2023, ...], \"attention_mask\": [1, 1, ...], \"labels\": 0}\nBut PyTorch models expect GPU tensors with batch dimensions:\nbatch = {\"input_ids\": tensor([[101, 2023, ...]], device='cuda:0'), ...}\nWhy tensors are required:\nPyTorch models perform tensor operations (matrix multiplications, slicing, etc.) that require PyTorch tensor objects, not Python lists or integers. If you pass Python lists/ints directly, you‚Äôll get errors like: - TypeError: expected Tensor as element 0 in argument 0, but got list - RuntimeError: Expected all tensors to be on the same device\nThe dictionary comprehension converts your data to the correct tensor format before passing it to the model. (See ¬ß2.2 for how these tensors flow through the model‚Äôs forward() method.)\nThe dictionary comprehension does three transformations in one line:\n\nPreserve structure: Keep the same dict keys (input_ids, attention_mask, etc.)\nConvert types: List/int ‚Üí PyTorch tensor ‚Üí GPU tensor\nAdd batch dimension: Shape (seq_len,) ‚Üí (1, seq_len) for batching\n\nBreakdown: * for k, v in item.items() ‚Üí iterates over each key-value pair * torch.tensor(v) ‚Üí converts list/int to tensor * .unsqueeze(0) ‚Üí adds batch dimension: [a, b, c] ‚Üí [[a, b, c]] * .to(device) ‚Üí moves to GPU\nWithout this transformation, you‚Äôd pass Python lists/CPU arrays to the model, which would either error or require slow implicit conversion on each forward pass.\n\nAlternative: Generator-based streaming with yield\nFor large datasets or memory-constrained scenarios, dictionary comprehensions can be memory-intensive (they build the entire dict in memory). A better approach uses generators with yield for lazy evaluation:\ndef stream_to_device(item, device):\n    \"\"\"Generator that yields tensors one at a time - memory efficient\"\"\"\n    for k, v in item.items():\n        yield k, torch.tensor(v).unsqueeze(0).to(device)\n\n# Usage: build dict lazily\nbatch = dict(stream_to_device(example, device))\nWhy generators are better for large data: * Lazy evaluation: Tensors are created and moved to GPU one at a time, not all at once. * Lower memory footprint: Only one tensor exists in memory during transformation. * Scalable: Works with datasets that don‚Äôt fit in RAM.\nWhen to use each: * Dict comprehension: Small to medium batches, simple one-liners, readable code. * Generator with yield: Large datasets, streaming data, memory-constrained environments, production pipelines.\n\n\n\n2.2 Kwargs unpacking with ** ‚Äî The HuggingFace connection\nGiven item = {\"input_ids\": X, \"attention_mask\": Y, \"labels\": Z}:\nout = model(**item)\n# exactly the same as:\nout = model(input_ids=X, attention_mask=Y, labels=Z)\nWhy this matters for HuggingFace models:\nThe ** operator unpacks a dict into named arguments that match your model‚Äôs forward() signature. This is why HuggingFace workflows are so elegant:\n\nDataset has standard keys: HuggingFace datasets/tokenizers output dicts with keys like \"input_ids\", \"attention_mask\", \"labels\".\nModel expects those keys: All HuggingFace models have a forward() method that accepts these exact parameter names.\n**kwargs bridges them: Instead of manually extracting each key, model(**batch) automatically maps dict keys to function parameters.\n\nWithout **kwargs (manual, verbose):\nout = model(\n    input_ids=batch[\"input_ids\"],\n    attention_mask=batch[\"attention_mask\"],\n    labels=batch[\"labels\"]\n)\nWith **kwargs (clean, scalable):\nout = model(**batch)  # Automatically maps all keys!\nThis works because HuggingFace models define their forward() signature to match the standard dataset keys. It‚Äôs a deliberate design pattern that makes training code incredibly clean.\nTracing the forward() call chain:\nWhen you call model(**batch), the unpacked tensors flow through the model‚Äôs forward pass. Here‚Äôs the call chain for AutoModelForSequenceClassification:\nmodel(**batch)  # batch contains tensors: {\"input_ids\": tensor(...), ...}\n    ‚Üì\nAutoModelForSequenceClassification.from_pretrained(...)\n    ‚Üì\nSmolLM2ForSequenceClassification  # concrete architecture class\n    ‚Üì\nGenericForSequenceClassification.forward(**kwargs)\n    ‚Üì\n    # forward() signature receives unpacked kwargs:\n    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n        outputs = self.model(input_ids, attention_mask=attention_mask, **kwargs)\n        #              ‚Üë **kwargs unpacking maps dict keys to these parameters\n        pooled = outputs[0][:, 0, :]  # CLS token pooling\n        logits = self.score(pooled)   # linear classifier head\n        loss = self.loss_fn(logits, labels) if labels is not None else None\n        return SequenceClassifierOutput(loss=loss, logits=logits, ...)\nKey insight: The **batch unpacking automatically maps dictionary keys (\"input_ids\", \"attention_mask\", \"labels\") to the forward() method‚Äôs parameter names. This is why model(**batch) works seamlessly ‚Äî the keys match the function signature exactly.\n\n\n2.3 Gradient averaging vs learning-rate scaling ‚öñÔ∏è\nThis is a key insight: When training on N GPUs, you have two mathematically equivalent options for combining gradients:\n\nOption A: Average gradients (most common)\n# After backward on each rank\ndist.all_reduce(param.grad, op=SUM)\nparam.grad /= world_size  # Average the gradients\n\n# Optimizer update with normal LR\noptimizer.step()  # uses original learning rate\n\n\nOption B: Sum gradients, scale learning rate\n# After backward on each rank  \ndist.all_reduce(param.grad, op=SUM)  # Keep summed gradients\n\n# Optimizer update with scaled LR\nfor param in model.parameters():\n    param.data -= (lr / world_size) * param.grad\nWhy they‚Äôre equivalent:\n\\[\n\\text{param} - \\text{lr} \\times \\frac{\\text{grad}}{N} = \\text{param} - \\frac{\\text{lr}}{N} \\times \\text{grad}\n\\]\nReal-world implications: * PyTorch DDP: Uses Option A (averages gradients), so you keep your learning rate unchanged. * Some frameworks (Horovod, older examples): Use Option B (sum gradients), expecting you to scale LR by 1/world_size. * The division can happen in two places: before the optimizer step (average gradients during sync) or after (scale learning rate during optimizer step) ‚Äî same math, different location in the algorithm.\nPractical tip: The instructor‚Äôs comment ‚Äúit depends where in the algorithm you want the averaging‚Äù refers to this choice. Most modern code averages gradients (Option A) because it‚Äôs cleaner and doesn‚Äôt require you to remember to scale the learning rate."
  },
  {
    "objectID": "til/ddp-python-basics.html#a-tiny-ddp-wrapper-teaching-version",
    "href": "til/ddp-python-basics.html#a-tiny-ddp-wrapper-teaching-version",
    "title": "DDP from Scratch: a learner-friendly guide",
    "section": "3) A tiny DDP wrapper (teaching version)",
    "text": "3) A tiny DDP wrapper (teaching version)\nThis wrapper (a) verifies parameter equality at init (optionally enforces it) and (b) averages gradients after backward().\nimport torch\nimport torch.distributed as dist\n\nclass MiniDDP:\n    def __init__(self, model: torch.nn.Module, enforce_broadcast: bool = False):\n        self.model = model\n        self.world_size = dist.get_world_size() if dist.is_initialized() else 1\n\n        # --- verify / enforce identical params across ranks ---\n        for p in self.model.parameters():\n            # create a rank0 copy to compare/broadcast\n            rank0_buf = p.detach().clone()\n            dist.broadcast(rank0_buf, src=0)     # everyone receives rank0's tensor\n            if enforce_broadcast:\n                p.data.copy_(rank0_buf)          # enforce equality (optional)\n            else:\n                if not torch.equal(p.data, rank0_buf):\n                    raise ValueError(\n                        \"Parameters differ at init. Seed all ranks BEFORE model construction, \"\n                        \"or set enforce_broadcast=True.\"\n                    )\n\n    def __call__(self, *args, **kwargs):\n        return self.model(*args, **kwargs)\n\n    def average_grads(self):\n        if self.world_size == 1:\n            return\n        for p in self.model.parameters():\n            if p.grad is None:\n                continue\n            dist.all_reduce(p.grad, op=dist.ReduceOp.SUM)\n            p.grad.div_(self.world_size)\n\n    # convenience passthroughs\n    def train(self):\n        self.model.train()\n    def eval(self):\n        self.model.eval()\nUnderstanding enforce_broadcast:\nThe enforce_broadcast parameter controls how parameter synchronization is handled at initialization:\n\nenforce_broadcast=False (default): Verifies that all ranks already have identical parameters (e.g., via seeding). If parameters differ, it raises an error. This is the ‚Äútrust but verify‚Äù approach ‚Äî you‚Äôre responsible for ensuring equality (via seeding), and the wrapper checks that you did it correctly.\nenforce_broadcast=True: Forces all ranks to use rank 0‚Äôs parameters by overwriting each rank‚Äôs parameters with rank 0‚Äôs values. This is the ‚Äúbelt and suspenders‚Äù approach ‚Äî even if seeding failed or parameters diverged, everyone gets rank 0‚Äôs exact state.\n\nWhy this mirrors PyTorch‚Äôs official DDP:\nPyTorch‚Äôs DistributedDataParallel always performs parameter synchronization at initialization (like enforce_broadcast=True), but it does so internally, automatically, and efficiently: - It broadcasts parameters from rank 0 to all other ranks during construction - It handles buffers (like BatchNorm running stats) as well - It uses optimized communication patterns (coalesced broadcasts, bucketing)\nThis initial synchronization is a core part of DDP‚Äôs design to ensure all model replicas start with identical weights. As documented in the PyTorch DDP notes: ‚ÄúWhen a model is wrapped with DDP, the constructor synchronizes the model‚Äôs parameters across all processes. This is achieved by broadcasting the parameters from the process with rank 0 to all other processes.‚Äù\nKey difference: In PyTorch‚Äôs DDP, this synchronization happens automatically in the constructor ‚Äî there‚Äôs no user-facing parameter to control it. It‚Äôs an internal implementation detail that ensures correctness.\nIn MiniDDP, we make this synchronization explicit and optional so you can: - See exactly what‚Äôs happening (educational value) - Choose to verify vs.¬†enforce (learning about seeding) - Understand the tradeoffs between verification and enforcement\n\nThis mirrors what PyTorch‚Äôs official DistributedDataParallel does conceptually, but without bucketing, overlap, or autograd hooks. Perfect for learning; use the real DDP for production."
  },
  {
    "objectID": "til/ddp-python-basics.html#minimal-distributed-training-loop",
    "href": "til/ddp-python-basics.html#minimal-distributed-training-loop",
    "title": "DDP from Scratch: a learner-friendly guide",
    "section": "4) Minimal distributed training loop",
    "text": "4) Minimal distributed training loop\n# torchrun --nproc_per_node=2 train.py\n\nimport os, torch, torch.distributed as dist\nfrom torch.optim import Adam\nfrom datasets import load_dataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nMODEL = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n\ndef set_seed(seed=43):\n    import random, numpy as np\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n\ndef main():\n    dist.init_process_group(\"nccl\")\n    rank  = dist.get_rank();  local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n    device = torch.device(f\"cuda:{local_rank}\")\n\n    set_seed(43)  # same on every process BEFORE creating the model\n\n    tok = AutoTokenizer.from_pretrained(MODEL)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        MODEL, num_labels=2, torch_dtype=torch.bfloat16\n    ).to(device)\n\n    ddp = MiniDDP(model, enforce_broadcast=False)\n    opt = Adam(ddp.model.parameters(), lr=1e-3)\n\n    ds = load_dataset(\"glue\", \"mrpc\")\n    def encode(ex):\n        return tok(ex[\"sentence1\"], ex[\"sentence2\"], padding=True, truncation=True)\n    ds = ds.map(encode, batched=True).rename_columns({\"label\": \"labels\"})\n\n    # toy per-rank sample (one example per rank to show divergence if not averaged)\n    example = ds[\"train\"][rank]\n    batch = {k: torch.tensor(v).unsqueeze(0).to(device) for k, v in example.items()}\n\n    ddp.train()\n    out = ddp(**batch)         # kwargs unpacking\n    out.loss.backward()\n    ddp.average_grads()        # &lt;‚Äî key! average across ranks\n    opt.step(); opt.zero_grad(set_to_none=True)\n\n    if rank == 0:\n        print(\"step ok; loss:\", out.loss.item())\n\n    dist.destroy_process_group()\n\nif __name__ == \"__main__\":\n    main()\n\nWhat just happened?\n\nBoth ranks ran the same code and created identical models (thanks to seeding).\nEach rank used a different example (rank index) ‚Üí losses differ initially.\naverage_grads() made every GPU apply the same averaged update, keeping replicas in lock‚Äëstep."
  },
  {
    "objectID": "til/ddp-python-basics.html#why-broadcast-at-init-if-we-already-seed",
    "href": "til/ddp-python-basics.html#why-broadcast-at-init-if-we-already-seed",
    "title": "DDP from Scratch: a learner-friendly guide",
    "section": "5) Why broadcast at init if we already seed?",
    "text": "5) Why broadcast at init if we already seed?\nSeeding should guarantee equality. The broadcast operation (when enforce_broadcast=True) is a belt‚Äëand‚Äësuspenders option:\n\nProtect against forgotten seeds: If you forgot to seed on some ranks, broadcast ensures everyone still starts identical.\nHandle divergent code paths: If different ranks take different initialization paths, broadcast syncs them.\nDeal with non‚Äëdeterministic ops: Some operations (e.g., certain CUDA kernels) may not be fully deterministic even with seeds.\nEnable joining late ranks: If a rank joins after initialization, broadcast can sync it to the current state from rank 0.\n\nIn practice: With proper seeding (see ¬ß1), enforce_broadcast=False (verify mode) is usually sufficient. Use enforce_broadcast=True only if you intend to force‚Äësync weights at init or are debugging initialization issues.\nNote: PyTorch‚Äôs official DDP always performs this synchronization automatically (equivalent to enforce_broadcast=True), but hides it from you. MiniDDP makes it explicit so you can learn about the mechanism."
  },
  {
    "objectID": "til/ddp-python-basics.html#common-pitfalls-fixes",
    "href": "til/ddp-python-basics.html#common-pitfalls-fixes",
    "title": "DDP from Scratch: a learner-friendly guide",
    "section": "6) Common pitfalls & fixes",
    "text": "6) Common pitfalls & fixes\n\nDifferent seeds / seeding too late ‚Üí parameters differ. Fix: call set_seed() before build_model() on every rank.\nForgetting to divide after all_reduce(SUM) ‚Üí LR effectively √ó world_size. Fix: divide grads (or use op=AVG on newer APIs like reduce_scatter_tensor).\nGrad is None: layers not used in the forward didn‚Äôt receive gradients. Fix: check the graph; guard if p.grad is None: continue.\nCPU tensors in batch: model expects CUDA tensors. Fix: dictionary comprehension that moves tensors to device.\nShape mismatches across ranks: ensure each rank‚Äôs micro‚Äëbatch has identical shapes (padding or a proper DistributedSampler).\nNCCL init errors: set MASTER_ADDR/PORT, unique RANK, correct CUDA_VISIBLE_DEVICES."
  },
  {
    "objectID": "til/ddp-python-basics.html#from-toy-to-real-ddp",
    "href": "til/ddp-python-basics.html#from-toy-to-real-ddp",
    "title": "DDP from Scratch: a learner-friendly guide",
    "section": "7) From toy to real DDP",
    "text": "7) From toy to real DDP\nWhat we built is the core idea. Production torch.nn.parallel.DistributedDataParallel adds:\n\ngradient bucketing and overlap with communication;\nparameter and buffer broadcast on construction (with versioning);\nautograd hooks for exact timing;\nmixed precision, static graph optimizations, etc.\n\nUpgrade path: once you grasp the flow above, swap MiniDDP for DistributedDataParallel(model, device_ids=[local_rank]) and use DistributedSampler in your DataLoader."
  },
  {
    "objectID": "til/ddp-python-basics.html#exercises-recommended",
    "href": "til/ddp-python-basics.html#exercises-recommended",
    "title": "DDP from Scratch: a learner-friendly guide",
    "section": "8) Exercises (recommended)",
    "text": "8) Exercises (recommended)\n\nFail fast: comment out set_seed(43) and watch the init check throw. Then set enforce_broadcast=True and observe it succeed.\nBatching: replace the single‚Äëexample hack with a DataLoader + DistributedSampler. Verify all ranks consume disjoint shards.\nReduce‚Äëscatter: re‚Äëimplement average_grads() with reduce_scatter_tensor + all_gather_into_tensor to mimic optimizer sharding.\nKwargs drill: write a wrapper that logs which kwargs are passed through (*args, **kwargs) and rejects unknown keys.\nDeterminism: enable CUDA deterministic flags and compare speed/behavior."
  },
  {
    "objectID": "til/ddp-python-basics.html#cheatsheet",
    "href": "til/ddp-python-basics.html#cheatsheet",
    "title": "DDP from Scratch: a learner-friendly guide",
    "section": "9) Cheatsheet",
    "text": "9) Cheatsheet\n\nitem = {k: f(v) for k, v in d.items()} ‚Üí dictionary comprehension.\nmodel(**d) ‚Üí unpack d into named arguments to forward.\ndist.all_reduce(t, SUM); t /= world_size ‚Üí average a tensor across ranks.\nSeed before model creation on every process.\nIf in doubt, force-sync params once with broadcast."
  },
  {
    "objectID": "til/ddp-python-basics.html#appendix-tiny-utilities",
    "href": "til/ddp-python-basics.html#appendix-tiny-utilities",
    "title": "DDP from Scratch: a learner-friendly guide",
    "section": "10) Appendix: tiny utilities",
    "text": "10) Appendix: tiny utilities\ndef recursively_apply(func, data):\n    if isinstance(data, (tuple, list)):\n        return type(data)(recursively_apply(func, x) for x in data)\n    if isinstance(data, dict):\n        return {k: recursively_apply(func, v) for k, v in data.items()}\n    return func(data)\n\n# Example: move a nested batch to device\nbatch = recursively_apply(lambda t: t.to(device) if isinstance(t, torch.Tensor) else t, batch)"
  },
  {
    "objectID": "til/ddp-python-basics.html#bonus-where-does-forward-come-from-with-automodel",
    "href": "til/ddp-python-basics.html#bonus-where-does-forward-come-from-with-automodel",
    "title": "DDP from Scratch: a learner-friendly guide",
    "section": "11) Bonus: Where does forward() come from with AutoModel?",
    "text": "11) Bonus: Where does forward() come from with AutoModel?\nWhen we wrote:\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=2)\nthat helper inspects the model config and dispatches to the architecture‚Äëspecific ...ForSequenceClassification class. For the SmolLM family, that class inherits a generic head that already implements forward().\nCall chain at runtime (conceptual):\n\nAutoModelForSequenceClassification ‚Üí ArchitectureForSequenceClassification ‚Üí GenericForSequenceClassification.forward(**kwargs) ‚Üí ArchitectureModel.forward(‚Ä¶) ‚Üí CLS pooling ‚Üí classifier head (self.score) ‚Üí loss (if labels)\n\nMinimal shape of that forward():\nclass GenericForSequenceClassification(PreTrainedModel):\n    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n        outputs = self.model(input_ids, attention_mask=attention_mask, **kwargs)\n        pooled = outputs[0][:, 0, :]\n        logits = self.score(pooled)\n        loss = self.loss_fn(logits, labels) if labels is not None else None\n        return SequenceClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states)\nThis is why model(**batch) (see ¬ß2.2) ‚Äújust works‚Äù: the dict keys map to the generic forward() signature, which calls the backbone‚Äôs forward() under the hood.\n\nHappy scaling! If you‚Äôre following the course, tag this post as TIL/DDP‚Äëfrom‚Äëscratch and iterate from here. üß™üöÄ"
  },
  {
    "objectID": "til/ddp-python-basics.html#quick-reference-gradient-sync-patterns",
    "href": "til/ddp-python-basics.html#quick-reference-gradient-sync-patterns",
    "title": "DDP from Scratch: a learner-friendly guide",
    "section": "12) Quick Reference: Gradient sync patterns",
    "text": "12) Quick Reference: Gradient sync patterns\nSummary of the two equivalent approaches (see ¬ß2.3 for full explanation):\n# Pattern A: Average gradients (PyTorch DDP default)\ndist.all_reduce(param.grad, op=SUM)\nparam.grad /= world_size\nparam -= lr * param.grad  # Original LR\n\n# Pattern B: Sum gradients, scale LR (Horovod-style)\ndist.all_reduce(param.grad, op=SUM)\nparam -= (lr / world_size) * param.grad  # Scaled LR\nKey takeaway: Both produce identical updates. Choose Pattern A for cleaner code that matches PyTorch DDP defaults."
  },
  {
    "objectID": "til.html",
    "href": "til.html",
    "title": "Today I Learned",
    "section": "",
    "text": "Deep dives into technical concepts I‚Äôm learning. From GPU programming to AI systems, these are comprehensive explorations of new technologies and techniques.\n\n\n\n\n\n\n\n\n\n\n\n\nDDP from Scratch: a learner-friendly guide\n\n\nLearn why dictionary comprehensions in python elegantly transform HuggingFace data for models, how kwargs unpacking makes model(batch) ‚Äòjust work‚Äô, and why gradient‚Ä¶\n\n\n\nThe Fire Hacker\n\n\nOct 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nMy First CUDA Kernel: Learning GPU Programming from Scratch\n\n\n\n\n\n\nThe Fire Hacker\n\n\nJan 16, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/timecapsule-slm.html",
    "href": "blog/timecapsule-slm.html",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "",
    "text": "TimeCapsule-SLM is an innovative AI-powered research and learning platform that‚Äôs revolutionizing how we discover knowledge and collaborate on research. Built with privacy-first principles and cutting-edge AI technology, it democratizes access to powerful research tools while keeping your data secure and local."
  },
  {
    "objectID": "blog/timecapsule-slm.html#introduction",
    "href": "blog/timecapsule-slm.html#introduction",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "",
    "text": "TimeCapsule-SLM is an innovative AI-powered research and learning platform that‚Äôs revolutionizing how we discover knowledge and collaborate on research. Built with privacy-first principles and cutting-edge AI technology, it democratizes access to powerful research tools while keeping your data secure and local."
  },
  {
    "objectID": "blog/timecapsule-slm.html#what-is-timecapsule-slm",
    "href": "blog/timecapsule-slm.html#what-is-timecapsule-slm",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "What is TimeCapsule-SLM?",
    "text": "What is TimeCapsule-SLM?\nTimeCapsule-SLM combines the power of Small Language Models with advanced research capabilities to create a comprehensive platform for:\n\nResearchers seeking AI-assisted discovery and pattern recognition\nStudents looking for adaptive, personalized learning experiences\nTeachers creating interactive educational content\nTeams collaborating on knowledge discovery\n\nThe platform addresses critical challenges in modern education and research: - Research fragmentation across multiple sources - Inefficient learning workflows - Privacy concerns with cloud-based AI - Limited AI integration in educational settings - Resource constraints in low-bandwidth environments"
  },
  {
    "objectID": "blog/timecapsule-slm.html#core-features",
    "href": "blog/timecapsule-slm.html#core-features",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Core Features",
    "text": "Core Features\n\nüß† DeepResearch TimeCapsule\nTransform your research workflow with multi-agent AI collaboration:\n\nAI-Powered Discovery: Generate novel research ideas and hypotheses\nPattern Recognition: Uncover hidden connections in your data\nMulti-Agent System: Leverage specialized AI agents for different research tasks\nCollaborative Intelligence: Combine human expertise with AI insights\n\n\n\nüé• AI-Frames Interactive Learning\nCreate immersive, adaptive learning experiences:\n\nSequential Learning Paths: Build structured knowledge journeys\nMultimodal Content: Integrate videos, documents, and interactive elements\nAI-Guided Explanations: Get personalized help when you need it\nSelf-Paced Progress: Learn at your own speed with AI support\n\n\n\nüìö In-Browser RAG (Retrieval-Augmented Generation)\nExperience the power of semantic search without compromising privacy:\n\nLocal Vector Store: All processing happens in your browser\nOffline Capability: Works without internet after initial model load\nSemantic Understanding: Find information based on meaning, not just keywords\nPrivacy-First Design: Your documents never leave your device"
  },
  {
    "objectID": "blog/timecapsule-slm.html#getting-started",
    "href": "blog/timecapsule-slm.html#getting-started",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Getting Started",
    "text": "Getting Started\n\nPrerequisites\nBefore installing TimeCapsule-SLM, ensure you have:\n# Node.js 18 or higher\nnode --version\n\n# npm or yarn package manager\nnpm --version\n\n# Git for cloning the repository\ngit --version\n\n\nInstallation\n\nClone the Repository:\n\ngit clone https://github.com/thefirehacker/TimeCapsule-SLM.git\ncd TimeCapsule-SLM\n\nInstall Dependencies:\n\nnpm install\n# or\nyarn install\n\nConfigure Environment:\n\ncp env.example .env.local\nEdit .env.local to configure your AI providers:\n# Optional: Add API keys for cloud models\nOPENAI_API_KEY=your_key_here\n\n# Local model configuration (Ollama)\nOLLAMA_HOST=http://localhost:11434\n\nStart the Development Server:\n\nnpm run dev\n# or\nyarn dev\nVisit http://localhost:3000 to access TimeCapsule-SLM!"
  },
  {
    "objectID": "blog/timecapsule-slm.html#setting-up-local-ai-models",
    "href": "blog/timecapsule-slm.html#setting-up-local-ai-models",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Setting Up Local AI Models",
    "text": "Setting Up Local AI Models\nFor the best privacy and offline experience, use local models with Ollama:\n\nInstall Ollama\n# macOS/Linux\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# Windows - Download from ollama.ai\n\n\nPull Recommended Models\n# For research and general tasks\nollama pull gemma:2b\n\n# For code and technical content\nollama pull qwen2.5:3b\n\n# For creative writing\nollama pull llama3.2:3b"
  },
  {
    "objectID": "blog/timecapsule-slm.html#using-timecapsule-slm",
    "href": "blog/timecapsule-slm.html#using-timecapsule-slm",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Using TimeCapsule-SLM",
    "text": "Using TimeCapsule-SLM\n\nCreating Your First Research Project\n\nInitialize a Knowledge Base:\n\nClick ‚ÄúNew Project‚Äù\nChoose your domain (Research, Education, Personal)\nSelect your preferred AI model\n\nImport Your Documents:\n\nDrag and drop PDFs, Word docs, or text files\nThe system will automatically index and embed them\nAll processing happens locally in your browser\n\nStart Researching:\n\nUse natural language queries to explore your knowledge base\nThe AI will surface relevant information and suggest connections\nGenerate summaries, insights, and new research directions\n\n\n\n\nBuilding AI-Frames for Learning\nCreate interactive learning experiences with AI-Frames:\n// Example AI-Frame configuration\n{\n  \"title\": \"Introduction to Quantum Computing\",\n  \"modules\": [\n    {\n      \"type\": \"video\",\n      \"content\": \"intro-video.mp4\",\n      \"ai_notes\": true\n    },\n    {\n      \"type\": \"interactive\",\n      \"content\": \"qubit-simulator\",\n      \"ai_guidance\": \"adaptive\"\n    },\n    {\n      \"type\": \"quiz\",\n      \"ai_generated\": true,\n      \"difficulty\": \"progressive\"\n    }\n  ]\n}\n\n\nCollaborative Features\nTimeCapsule-SLM supports real-time collaboration:\n\nShared Workspaces: Invite team members to research projects\nLive AI Sessions: Collaborate with AI assistance in real-time\nKnowledge Graphs: Visualize connections discovered by your team\nVersion Control: Track changes and contributions"
  },
  {
    "objectID": "blog/timecapsule-slm.html#architecture-technology",
    "href": "blog/timecapsule-slm.html#architecture-technology",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Architecture & Technology",
    "text": "Architecture & Technology\nTimeCapsule-SLM is built with modern, performant technologies:\n\nFrontend: Next.js 15, React 19, TypeScript\nAI Integration: Support for Ollama, OpenAI, and local models\nDatabase: RxDB for offline-first data persistence\nVector Store: In-browser embeddings with WebAssembly\nAuthentication: NextAuth.js for secure access"
  },
  {
    "objectID": "blog/timecapsule-slm.html#privacy-security",
    "href": "blog/timecapsule-slm.html#privacy-security",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Privacy & Security",
    "text": "Privacy & Security\nYour privacy is our priority:\n\nLocal-First: All sensitive processing happens on your device\nNo Telemetry: We don‚Äôt track your usage or collect data\nOpen Source: Audit the code yourself (Apache 2.0 License)\nEncryption: Local data is encrypted at rest\nControl: You decide what stays local vs.¬†what uses cloud services"
  },
  {
    "objectID": "blog/timecapsule-slm.html#use-cases",
    "href": "blog/timecapsule-slm.html#use-cases",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Use Cases",
    "text": "Use Cases\n\nFor Researchers\n\nLiterature reviews with AI-powered synthesis\nPattern discovery in research data\nHypothesis generation and validation\nCollaborative paper writing\n\n\n\nFor Students\n\nPersonalized study guides\nAI tutoring for complex topics\nInteractive learning paths\nExam preparation with adaptive quizzes\n\n\n\nFor Teachers\n\nCreate engaging course content\nBuild interactive lessons\nTrack student progress\nGenerate assessments automatically\n\n\n\nFor Teams\n\nKnowledge management\nCollaborative research\nTraining materials\nDocumentation with AI assistance"
  },
  {
    "objectID": "blog/timecapsule-slm.html#performance-tips",
    "href": "blog/timecapsule-slm.html#performance-tips",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Performance Tips",
    "text": "Performance Tips\nOptimize TimeCapsule-SLM for your hardware:\n\nModel Selection: Choose smaller models (2-3B parameters) for faster responses\nCaching: Enable browser caching for repeated queries\nBatch Processing: Process multiple documents simultaneously\nGPU Acceleration: Use WebGPU when available for faster inference"
  },
  {
    "objectID": "blog/timecapsule-slm.html#roadmap-future-features",
    "href": "blog/timecapsule-slm.html#roadmap-future-features",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Roadmap & Future Features",
    "text": "Roadmap & Future Features\nWe‚Äôre constantly improving TimeCapsule-SLM:\n\nMobile Apps: iOS and Android applications (Q2 2025)\nVoice Interface: Natural conversation with your knowledge base\nAdvanced Visualizations: 3D knowledge graphs and mind maps\nPlugin System: Extend functionality with custom modules\nFederated Learning: Collaborate without sharing raw data"
  },
  {
    "objectID": "blog/timecapsule-slm.html#contributing",
    "href": "blog/timecapsule-slm.html#contributing",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Contributing",
    "text": "Contributing\nTimeCapsule-SLM is open source and welcomes contributions:\n# Fork the repository\n# Create a feature branch\ngit checkout -b feature/amazing-feature\n\n# Make your changes\n# Run tests\nnpm test\n\n# Submit a pull request\nCheck our contribution guidelines for more details."
  },
  {
    "objectID": "blog/timecapsule-slm.html#community-support",
    "href": "blog/timecapsule-slm.html#community-support",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Community & Support",
    "text": "Community & Support\nJoin our growing community:\n\nGitHub Discussions: Technical questions and feature requests\nDiscord Server: Real-time chat with developers and users\nDocumentation: Comprehensive guides at timecapsule.bubblspace.com\nX/Twitter: Follow @thefirehacker for updates"
  },
  {
    "objectID": "blog/timecapsule-slm.html#conclusion",
    "href": "blog/timecapsule-slm.html#conclusion",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Conclusion",
    "text": "Conclusion\nTimeCapsule-SLM represents a new paradigm in AI-assisted research and learning. By combining powerful AI capabilities with privacy-first design and local-first architecture, we‚Äôre making advanced research tools accessible to everyone.\nWhether you‚Äôre a researcher pushing the boundaries of knowledge, a student seeking personalized learning, or a teacher creating engaging content, TimeCapsule-SLM empowers you to work smarter, not harder.\nStart your journey today and experience the future of AI-powered research and learning!\n\nReady to transform your research and learning workflow? Get started with TimeCapsule-SLM or visit timecapsule.bubblspace.com for more information.\nHave questions or feedback? Reach out on X/Twitter or open an issue on GitHub."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I‚Äôm passionate about democratizing AI and building tools that empower individuals and businesses to harness the power of artificial intelligence. My work focuses on making AI accessible, practical, and privacy-preserving.\n\n\n\n\nA revolutionary platform for creating and sharing AI-powered experiences in collaborative spaces. Think of it as Google Docs meets AI playground, where teams can work together with multiple AI models in real-time.\n\n\n\nSmall Language Models designed specifically for preserving and sharing memories through time. This project explores how we can use AI to create meaningful connections with our past and future selves.\n\n\n\n\n\n2025: Focused on AI research initiatives in distributed systems to build Efficient small models\n2024: BuildingBubblSpace: AI Pesronas with real time voice to voice capabiltiles to execute Enterprise Workflows\n2022: Founded AIEDX, focusing on local-first AI solutions\n\n\n\n\n\nSmall Language Models (SLMs): AI models that can run locally or small cluster of GPUs.\nDistributed Computing: Scaling AI across multiple nodes for both training and inference.\nAI-Frames: Open learning with the help of AI.\n\n\n\n\nI believe in giving back to the community. Check out my projects:\n\nTimeCapsule-SLM - Local-first memory preservation\n\n\n\n\n\n‚ÄúThe best AI is the one that runs on your device, respects your privacy, and enhances your capabilities without replacing your creativity.‚Äù\n\nI believe the future of AI is: - Local-first: Your data stays on your device - Collaborative: AI should enhance human collaboration - Accessible: Everyone should benefit from AI advances - Transparent: Understanding how AI makes decisions\n\n\n\nI love connecting with fellow developers, researchers, and AI enthusiasts. Feel free to reach out:\n\nGitHub: @thefirehacker\nX/Twitter: @thefirehacker\nEmail: firehacker@bubblspace.com\n\n\n\n\nI occasionally speak at conferences and write about AI, distributed systems, and building products. Some recent topics:\n\n‚ÄúSmall Models, Big Impact: The Future of Edge AI‚Äù\n‚ÄúBuilding Privacy-First AI Applications‚Äù\n‚ÄúFrom Research to Product: Shipping AI Features Users Love‚Äù\n\n\n\n\n\nüßô‚Äç‚ôÇÔ∏è The Fire Hacker wizard logo represents the magic of turning ideas into reality\nüî• I believe in ‚Äúhacking‚Äù in its original sense: creative problem-solving\nüåü My first program was a text-based adventure game\nüéØ Current goal: Make AI accessible to 1 million users\n\n\nWant to collaborate or just chat about AI? Drop me a message on X/Twitter or open an issue on one of my GitHub projects."
  },
  {
    "objectID": "about.html#founder-ai-researcher-at-aiedx",
    "href": "about.html#founder-ai-researcher-at-aiedx",
    "title": "About Me",
    "section": "",
    "text": "I‚Äôm passionate about democratizing AI and building tools that empower individuals and businesses to harness the power of artificial intelligence. My work focuses on making AI accessible, practical, and privacy-preserving.\n\n\n\n\nA revolutionary platform for creating and sharing AI-powered experiences in collaborative spaces. Think of it as Google Docs meets AI playground, where teams can work together with multiple AI models in real-time.\n\n\n\nSmall Language Models designed specifically for preserving and sharing memories through time. This project explores how we can use AI to create meaningful connections with our past and future selves.\n\n\n\n\n\n2025: Focused on AI research initiatives in distributed systems to build Efficient small models\n2024: BuildingBubblSpace: AI Pesronas with real time voice to voice capabiltiles to execute Enterprise Workflows\n2022: Founded AIEDX, focusing on local-first AI solutions\n\n\n\n\n\nSmall Language Models (SLMs): AI models that can run locally or small cluster of GPUs.\nDistributed Computing: Scaling AI across multiple nodes for both training and inference.\nAI-Frames: Open learning with the help of AI.\n\n\n\n\nI believe in giving back to the community. Check out my projects:\n\nTimeCapsule-SLM - Local-first memory preservation\n\n\n\n\n\n‚ÄúThe best AI is the one that runs on your device, respects your privacy, and enhances your capabilities without replacing your creativity.‚Äù\n\nI believe the future of AI is: - Local-first: Your data stays on your device - Collaborative: AI should enhance human collaboration - Accessible: Everyone should benefit from AI advances - Transparent: Understanding how AI makes decisions\n\n\n\nI love connecting with fellow developers, researchers, and AI enthusiasts. Feel free to reach out:\n\nGitHub: @thefirehacker\nX/Twitter: @thefirehacker\nEmail: firehacker@bubblspace.com\n\n\n\n\nI occasionally speak at conferences and write about AI, distributed systems, and building products. Some recent topics:\n\n‚ÄúSmall Models, Big Impact: The Future of Edge AI‚Äù\n‚ÄúBuilding Privacy-First AI Applications‚Äù\n‚ÄúFrom Research to Product: Shipping AI Features Users Love‚Äù\n\n\n\n\n\nüßô‚Äç‚ôÇÔ∏è The Fire Hacker wizard logo represents the magic of turning ideas into reality\nüî• I believe in ‚Äúhacking‚Äù in its original sense: creative problem-solving\nüåü My first program was a text-based adventure game\nüéØ Current goal: Make AI accessible to 1 million users\n\n\nWant to collaborate or just chat about AI? Drop me a message on X/Twitter or open an issue on one of my GitHub projects."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to my blog where I share insights about AI research, software development, and the journey of building innovative products. Here you‚Äôll find technical deep-dives, project updates, and lessons learned along the way.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform\n\n\n\nAI\n\nResearch\n\nEducation\n\nOpen Source\n\n\n\n\n\n\n\n\n\nJan 16, 2025\n\n\nThe Fire Hacker\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "The Fire Hacker",
    "section": "Recent Posts",
    "text": "Recent Posts\n\n\nGetting Started with TimeCapsule-SLM\nJanuary 2025 - Learn how to use the AI-powered research and learning platform that democratizes knowledge discovery. Read more ‚Üí"
  },
  {
    "objectID": "index.html#today-i-learned",
    "href": "index.html#today-i-learned",
    "title": "The Fire Hacker",
    "section": "Today I Learned",
    "text": "Today I Learned\n\n\nDDP from Scratch: a learner-friendly guide\nFrom single‚ÄëGPU code to a tiny DistributedDataParallel (DDP) built by hand.\nCovers seeding, kwargs unpacking, dictionary comprehensions, gradient averaging with all_reduce, and a minimal training loop.\nDeep dive ‚Üí\n\n\nMy First CUDA Kernel: Learning GPU Programming from Scratch\nFirst CUDA Kernel Success!\nBuilt and ran custom CUDA kernels on RTX 2050. Learned about parallel execution, compilation with ninja/nvcc, and discovered the 16384√ó16384 performance mystery.\nDeep dive ‚Üí\n\n\nSee all TIL ‚Üí"
  },
  {
    "objectID": "index.html#connect",
    "href": "index.html#connect",
    "title": "The Fire Hacker",
    "section": "Connect",
    "text": "Connect\n\nGitHub X/Twitter Contact"
  },
  {
    "objectID": "til/cuda-kernels-basics.html",
    "href": "til/cuda-kernels-basics.html",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "",
    "text": "Today I ran my first custom GPU code! Along with distributed pre-training AI models, I wanted to understand what‚Äôs happening at the low level. I was looking for a good resource to learn about kernel development for both inference & training. My interests were in MoE routing kernels, however I decided to start simple: compile and run kernels on local GPUs. I have a few gaming laptops and decided to run kernels on them.\nI decided to start with the simplest possible program: adding two vectors together. GPU Mode‚Äôs reference kernels are perfect to build a working end-to-end workflow."
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-beginning-why-learn-cuda",
    "href": "til/cuda-kernels-basics.html#the-beginning-why-learn-cuda",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "",
    "text": "Today I ran my first custom GPU code! Along with distributed pre-training AI models, I wanted to understand what‚Äôs happening at the low level. I was looking for a good resource to learn about kernel development for both inference & training. My interests were in MoE routing kernels, however I decided to start simple: compile and run kernels on local GPUs. I have a few gaming laptops and decided to run kernels on them.\nI decided to start with the simplest possible program: adding two vectors together. GPU Mode‚Äôs reference kernels are perfect to build a working end-to-end workflow."
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-program-overall-architecture",
    "href": "til/cuda-kernels-basics.html#the-program-overall-architecture",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "The Program: Overall Architecture",
    "text": "The Program: Overall Architecture\nBefore diving into the details, let me explain the complete program flow. We‚Äôll be working with the vectoradd_py problem from the reference-kernels repository.\n\nFile Structure & Purpose\nproblems/pmpp/vectoradd_py/\n‚îú‚îÄ‚îÄ run_local.py        # Main benchmark script\n‚îú‚îÄ‚îÄ submission.py       # Our custom CUDA kernel implementation\n‚îú‚îÄ‚îÄ reference.py        # Correctness checking\n‚îú‚îÄ‚îÄ task.py            # Data structure definitions\n‚îî‚îÄ‚îÄ README.md          # Problem description\nThe Execution Flow:\n\nrun_local.py - The orchestrator that:\n\nGenerates test data of various sizes\nCalls our custom kernel\nMeasures performance with CUDA events\nVerifies correctness against reference implementation\n\nsubmission.py - Contains our CUDA kernel using PyTorch‚Äôs inline compilation:\n\nCUDA C++ code written as Python strings\nCompiled on-the-fly using load_inline\nCreates a Python module we can call\n\nThe Magic: JIT Compilation Process\n\nWhen we use torch.utils.cpp_extension.load_inline, here‚Äôs what happens behind the scenes:\nYour Python Code\n        ‚Üì\ntorch.utils.cpp_extension.load_inline\n        ‚Üì\nGenerates .cpp and .cu source files\n        ‚Üì\nWrites build.ninja file\n        ‚Üì\nninja ‚Üí nvcc/g++ compile ‚Üí add_cuda.so\n        ‚Üì\ndlopen() loads .so into Python process\nThis is ahead-of-time compilation - once compiled, your kernel is fixed machine code running directly on the GPU!"
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-challenge-what-was-i-trying-to-achieve",
    "href": "til/cuda-kernels-basics.html#the-challenge-what-was-i-trying-to-achieve",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "The Challenge: What Was I Trying to Achieve?",
    "text": "The Challenge: What Was I Trying to Achieve?\nMy goal was simple but specific: 1. Write actual CUDA code that runs on my RTX 2050 laptop GPU 2. Understand how thousands of threads work together 3. Measure real performance and understand the numbers 4. Learn why GPUs are so powerful for AI workloads\nI found the perfect learning resource which I modified for my use: GPU Mode‚Äôs reference-kernels repository fork. It‚Äôs a collection of progressively harder GPU programming challenges, starting with vector addition."
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-big-picture-how-gpus-think-differently",
    "href": "til/cuda-kernels-basics.html#the-big-picture-how-gpus-think-differently",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "The Big Picture: How GPUs Think Differently",
    "text": "The Big Picture: How GPUs Think Differently\nBefore diving into code, here‚Äôs the mental shift that changed everything for me:\nCPU Thinking: ‚ÄúDo step 1, then step 2, then step 3‚Ä¶‚Äù GPU Thinking: ‚ÄúDo ALL the steps at once, everywhere!‚Äù\nImagine you need to paint 1000 fence posts. A CPU is like one very fast painter who paints each post perfectly, one after another. A GPU is like hiring 1000 amateur painters who each paint one post simultaneously. Even if each painter is slower, getting all posts done at once is way faster!\nFor vector addition (C = A + B), instead of:\nfor i in range(million):\n    C[i] = A[i] + B[i]  # One at a time\nThe GPU does:\nThread 0: C[0] = A[0] + B[0]\nThread 1: C[1] = A[1] + B[1]\nThread 2: C[2] = A[2] + B[2]\n... (all at the same time!)\nThread 999999: C[999999] = A[999999] + B[999999]"
  },
  {
    "objectID": "til/cuda-kernels-basics.html#setting-up-the-journey-to-hello-gpu",
    "href": "til/cuda-kernels-basics.html#setting-up-the-journey-to-hello-gpu",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "Setting Up: The Journey to ‚ÄúHello GPU‚Äù",
    "text": "Setting Up: The Journey to ‚ÄúHello GPU‚Äù\nGetting CUDA working on Windows with WSL2 was an adventure. Here‚Äôs what actually worked:\n\nStep 1: Install CUDA Toolkit\nFirst, I needed the CUDA compiler (nvcc) to turn my code into GPU instructions:\n# Get NVIDIA's official CUDA repository\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb\nsudo dpkg -i cuda-keyring_1.1-1_all.deb\nsudo apt-get update\nsudo apt-get install -y cuda-toolkit-12-1\n\n# Tell the system where CUDA lives\nexport CUDA_HOME=/usr/local/cuda-12.1\nexport PATH=$CUDA_HOME/bin:$PATH\n\n\nStep 2: PyTorch with CUDA Support\nPyTorch makes it easy to compile CUDA code on-the-fly:\npip install --index-url https://download.pytorch.org/whl/cu121 torch"
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-code-understanding-every-line",
    "href": "til/cuda-kernels-basics.html#the-code-understanding-every-line",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "The Code: Understanding Every Line",
    "text": "The Code: Understanding Every Line\nNow for the exciting part - the actual GPU code! Let me explain what each piece does and why it matters.\n\nThe GPU Kernel: Where the Magic Happens\ntemplate &lt;typename scalar_t&gt;\n__global__ void add_kernel(const scalar_t* __restrict__ A,\n                           const scalar_t* __restrict__ B,\n                           scalar_t* __restrict__ C,\n                           int N) {\n    // Who am I? Calculate my unique ID among thousands of threads\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // Am I responsible for a valid element?\n    if (idx &lt; N) {\n        C[idx] = A[idx] + B[idx];  // Do my one simple job\n    }\n}\nWhat‚Äôs happening here:\n\n__global__: This function runs on the GPU. It‚Äôs called from the CPU but executes on thousands of GPU cores simultaneously.\nThread Identity Crisis (solved!): Each thread needs to know which element to process. Think of it like a massive factory where each worker needs to know which item on the conveyor belt is theirs:\n\nthreadIdx.x: ‚ÄúI‚Äôm worker #5 in my team‚Äù\nblockIdx.x: ‚ÄúMy team is team #3‚Äù\nblockDim.x: ‚ÄúEach team has 256 workers‚Äù\nSo my global position is: 3 * 256 + 5 = 773 - I handle element 773!\n\nif (idx &lt; N): Safety first! We might launch more threads than we have data (for efficiency reasons), so each thread checks if it has real work to do.\n\n\n\nLaunching the Kernel: Mission Control\ntorch::Tensor add_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.numel();  // How many elements total?\n    auto C = torch::empty_like(A);  // Prepare output space\n\n    // Configure the thread army\n    const int threads = 256;  // Threads per block (team size)\n    const int blocks = (N + threads - 1) / threads;  // How many teams needed?\n\n    // LAUNCH! Send thousands of threads to work\n    add_kernel&lt;scalar_t&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(\n        A.data_ptr&lt;scalar_t&gt;(),\n        B.data_ptr&lt;scalar_t&gt;(),\n        C.data_ptr&lt;scalar_t&gt;(),\n        N\n    );\n}\nThe Strategy: - We organize threads into blocks (teams) of 256 threads each - Why 256? It‚Äôs a multiple of 32 (warp size - the GPU‚Äôs natural execution unit) - The &lt;&lt;&lt;blocks, threads&gt;&gt;&gt; syntax is CUDA‚Äôs special way to say ‚Äúlaunch this many blocks with this many threads each‚Äù\n\n\nThe Python Bridge: Making it Usable\nPyTorch‚Äôs load_inline is brilliant - it compiles CUDA code on-the-fly:\nadd_module = load_inline(\n    name='add_cuda',\n    cpp_sources=add_cpp_source,\n    cuda_sources=add_cuda_source,\n    functions=['add_cuda'],\n    verbose=True,  # Show me what's happening!\n)\nFirst time you run this, you‚Äôll see:\nDetected CUDA files, patching ldflags\nBuilding extension module add_cuda...\nninja: no work to do.\nLoading extension module add_cuda...\nThat‚Äôs nvcc compiling your GPU code into a Python module!"
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-benchmarking-measuring-reality",
    "href": "til/cuda-kernels-basics.html#the-benchmarking-measuring-reality",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "The Benchmarking: Measuring Reality",
    "text": "The Benchmarking: Measuring Reality\nThe run_local.py script does something clever - it automatically picks test sizes based on available GPU memory:\n# How much GPU memory is free?\nfree_bytes, _ = torch.cuda.mem_get_info()\nbudget = int(free_bytes * 0.8)  # Use 80% to be safe\n\n# For 2D matrices: need space for A, B, and C\ns_max = int(math.sqrt(budget / (3 * bytes_per_elem)))\nThis prevents the dreaded ‚ÄúCUDA out of memory‚Äù error!\n\nTiming GPU Code: It‚Äôs Tricky!\nYou can‚Äôt use regular Python timing for GPU code because GPU operations are asynchronous. The solution? CUDA Events:\nt0 = torch.cuda.Event(enable_timing=True)\nt1 = torch.cuda.Event(enable_timing=True)\n\nt0.record()              # Start timer ON THE GPU\ncustom_kernel(case)      # Run kernel\nt1.record()              # Stop timer ON THE GPU\ntorch.cuda.synchronize() # Wait for GPU to finish\nelapsed = t0.elapsed_time(t1)  # Get time in milliseconds"
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-results-what-i-learned-from-the-numbers",
    "href": "til/cuda-kernels-basics.html#the-results-what-i-learned-from-the-numbers",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "The Results: What I Learned from the Numbers",
    "text": "The Results: What I Learned from the Numbers\nRunning on my RTX 2050 (4GB VRAM):\nsize=4096:  mean=0.9877 ms    (67 million elements)\nsize=8192:  mean=3.8027 ms    (268 million elements)\nsize=12288: mean=8.5460 ms    (603 million elements)\nsize=16384: mean=150.3063 ms  (1.07 billion elements) ‚Üê WHAT?!\n\nThe Mystery of the Slow 16384\nWhy did 16384√ó16384 suddenly become 17x slower? This taught me a crucial lesson about GPU architecture:\nThe Problem: With 256 threads per block, processing 268,435,456 elements needs 1,048,576 blocks!\nThe GPU scheduler choked trying to manage over a million tiny work units. It‚Äôs like trying to manage a million separate construction crews for a project - the coordination overhead kills you!\nThe Solution: Grid-stride loops - have each thread process multiple elements:\nfor (int idx = blockIdx.x * blockDim.x + threadIdx.x;\n     idx &lt; N;\n     idx += blockDim.x * gridDim.x) {\n    C[idx] = A[idx] + B[idx];\n}\nNow you can cap blocks at a reasonable number (like 10,000) and each thread handles multiple elements.\n\n\nMemory Bandwidth: The Real Bottleneck\nFor the 8192√ó8192 case: - Data moved: 268M elements √ó 2 bytes √ó 3 arrays = 1.6 GB - Time: 3.8 ms - Bandwidth: 421 GB/s\nMy RTX 2050‚Äôs theoretical max is ~200 GB/s, so we‚Äôre doing great! Wait, how are we exceeding theoretical max? Cache! Some data gets reused from the GPU‚Äôs L2 cache."
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-revelations-what-changed-my-understanding",
    "href": "til/cuda-kernels-basics.html#the-revelations-what-changed-my-understanding",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "The Revelations: What Changed My Understanding",
    "text": "The Revelations: What Changed My Understanding\n\nGPUs are not fast CPUs - They‚Äôre a completely different beast. They‚Äôre terrible at complex branching logic but amazing at doing the same simple thing everywhere.\nMemory movement dominates - For simple operations like addition, you spend more time moving data than computing. This is why AI models use operations like matrix multiplication that do lots of compute per memory access.\nLaunch configuration matters hugely - Too many blocks? Scheduling overhead. Too few? Underutilization. It‚Äôs an art.\nThe power of parallel thinking - Once you start thinking ‚Äúwhat can happen simultaneously?‚Äù instead of ‚Äúwhat comes next?‚Äù, you see opportunities everywhere."
  },
  {
    "objectID": "til/cuda-kernels-basics.html#whats-next",
    "href": "til/cuda-kernels-basics.html#whats-next",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "What‚Äôs Next?",
    "text": "What‚Äôs Next?\nNow that I‚Äôve got basic kernels working, I‚Äôm excited to explore: - Shared memory: Using the 48KB of ultra-fast memory shared within each block - Warp-level operations: Leveraging the fact that 32 threads execute in lockstep - Reduction operations: How do you sum a billion numbers in parallel? - Matrix multiplication: The operation that powers all of deep learning"
  },
  {
    "objectID": "til/cuda-kernels-basics.html#resources-that-helped-me",
    "href": "til/cuda-kernels-basics.html#resources-that-helped-me",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "Resources That Helped Me",
    "text": "Resources That Helped Me\n\nReference Kernels Repo: Practice problems with increasing difficulty\nPMPP Book: ‚ÄúProgramming Massively Parallel Processors‚Äù - The theory behind it all\nGPU Mode Discord: Amazing community of people learning together\nCUDA Documentation: Surprisingly readable once you know the basics"
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-journey-continues",
    "href": "til/cuda-kernels-basics.html#the-journey-continues",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "The Journey Continues",
    "text": "The Journey Continues\nStarting with vector addition might seem trivial, but it opened the door to understanding how modern AI actually works at the hardware level. Every transformer model, every diffusion model, every neural network - they‚Äôre all built on these fundamental parallel operations.\nThe moment it clicked that my GPU was running 65,536 threads simultaneously, each doing their tiny part of the work, was magical. It‚Äôs not just faster computing - it‚Äôs a fundamentally different way of solving problems.\nNext week: I‚Äôm going to tackle matrix multiplication and see if I can beat PyTorch‚Äôs built-in implementation (spoiler: probably not, but I‚Äôll learn tons trying!).\n\nWant to try this yourself? Clone the reference-kernels repo and start with problems/pmpp/vectoradd_py/. The journey from CPU thinking to GPU thinking is worth it!"
  }
]