[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to my blog where I share insights about AI research, software development, and the journey of building innovative products. Here you‚Äôll find technical deep-dives, project updates, and lessons learned along the way.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform\n\n\n\nAI\n\nResearch\n\nEducation\n\nOpen Source\n\n\n\n\n\n\n\n\n\nJan 16, 2025\n\n\nThe Fire Hacker\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I‚Äôm passionate about democratizing AI and building tools that empower individuals and businesses to harness the power of artificial intelligence. My work focuses on making AI accessible, practical, and privacy-preserving.\n\n\n\n\nA revolutionary platform for creating and sharing AI-powered experiences in collaborative spaces. Think of it as Google Docs meets AI playground, where teams can work together with multiple AI models in real-time.\n\n\n\nSmall Language Models designed specifically for preserving and sharing memories through time. This project explores how we can use AI to create meaningful connections with our past and future selves.\n\n\n\n\n\n2025: Focused on AI research initiatives in distributed systems to build Efficient small models\n2024: BuildingBubblSpace: AI Pesronas with real time voice to voice capabiltiles to execute Enterprise Workflows\n2022: Founded AIEDX, focusing on local-first AI solutions\n\n\n\n\n\nSmall Language Models (SLMs): AI models that can run locally or small cluster of GPUs.\nDistributed Computing: Scaling AI across multiple nodes for both training and inference.\nAI-Frames: Open learning with the help of AI.\n\n\n\n\nI believe in giving back to the community. Check out my projects:\n\nTimeCapsule-SLM - Local-first memory preservation\n\n\n\n\n\n‚ÄúThe best AI is the one that runs on your device, respects your privacy, and enhances your capabilities without replacing your creativity.‚Äù\n\nI believe the future of AI is: - Local-first: Your data stays on your device - Collaborative: AI should enhance human collaboration - Accessible: Everyone should benefit from AI advances - Transparent: Understanding how AI makes decisions\n\n\n\nI love connecting with fellow developers, researchers, and AI enthusiasts. Feel free to reach out:\n\nGitHub: @thefirehacker\nX/Twitter: @thefirehacker\nEmail: firehacker@bubblspace.com\n\n\n\n\nI occasionally speak at conferences and write about AI, distributed systems, and building products. Some recent topics:\n\n‚ÄúSmall Models, Big Impact: The Future of Edge AI‚Äù\n‚ÄúBuilding Privacy-First AI Applications‚Äù\n‚ÄúFrom Research to Product: Shipping AI Features Users Love‚Äù\n\n\n\n\n\nüßô‚Äç‚ôÇÔ∏è The Fire Hacker wizard logo represents the magic of turning ideas into reality\nüî• I believe in ‚Äúhacking‚Äù in its original sense: creative problem-solving\nüåü My first program was a text-based adventure game\nüéØ Current goal: Make AI accessible to 1 million users\n\n\nWant to collaborate or just chat about AI? Drop me a message on X/Twitter or open an issue on one of my GitHub projects."
  },
  {
    "objectID": "about.html#founder-ai-researcher-at-aiedx",
    "href": "about.html#founder-ai-researcher-at-aiedx",
    "title": "About Me",
    "section": "",
    "text": "I‚Äôm passionate about democratizing AI and building tools that empower individuals and businesses to harness the power of artificial intelligence. My work focuses on making AI accessible, practical, and privacy-preserving.\n\n\n\n\nA revolutionary platform for creating and sharing AI-powered experiences in collaborative spaces. Think of it as Google Docs meets AI playground, where teams can work together with multiple AI models in real-time.\n\n\n\nSmall Language Models designed specifically for preserving and sharing memories through time. This project explores how we can use AI to create meaningful connections with our past and future selves.\n\n\n\n\n\n2025: Focused on AI research initiatives in distributed systems to build Efficient small models\n2024: BuildingBubblSpace: AI Pesronas with real time voice to voice capabiltiles to execute Enterprise Workflows\n2022: Founded AIEDX, focusing on local-first AI solutions\n\n\n\n\n\nSmall Language Models (SLMs): AI models that can run locally or small cluster of GPUs.\nDistributed Computing: Scaling AI across multiple nodes for both training and inference.\nAI-Frames: Open learning with the help of AI.\n\n\n\n\nI believe in giving back to the community. Check out my projects:\n\nTimeCapsule-SLM - Local-first memory preservation\n\n\n\n\n\n‚ÄúThe best AI is the one that runs on your device, respects your privacy, and enhances your capabilities without replacing your creativity.‚Äù\n\nI believe the future of AI is: - Local-first: Your data stays on your device - Collaborative: AI should enhance human collaboration - Accessible: Everyone should benefit from AI advances - Transparent: Understanding how AI makes decisions\n\n\n\nI love connecting with fellow developers, researchers, and AI enthusiasts. Feel free to reach out:\n\nGitHub: @thefirehacker\nX/Twitter: @thefirehacker\nEmail: firehacker@bubblspace.com\n\n\n\n\nI occasionally speak at conferences and write about AI, distributed systems, and building products. Some recent topics:\n\n‚ÄúSmall Models, Big Impact: The Future of Edge AI‚Äù\n‚ÄúBuilding Privacy-First AI Applications‚Äù\n‚ÄúFrom Research to Product: Shipping AI Features Users Love‚Äù\n\n\n\n\n\nüßô‚Äç‚ôÇÔ∏è The Fire Hacker wizard logo represents the magic of turning ideas into reality\nüî• I believe in ‚Äúhacking‚Äù in its original sense: creative problem-solving\nüåü My first program was a text-based adventure game\nüéØ Current goal: Make AI accessible to 1 million users\n\n\nWant to collaborate or just chat about AI? Drop me a message on X/Twitter or open an issue on one of my GitHub projects."
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "The Fire Hacker",
    "section": "Recent Posts",
    "text": "Recent Posts\n\n\nGetting Started with TimeCapsule-SLM\nJanuary 2025 - Learn how to use the AI-powered research and learning platform that democratizes knowledge discovery. Read more ‚Üí"
  },
  {
    "objectID": "index.html#today-i-learned",
    "href": "index.html#today-i-learned",
    "title": "The Fire Hacker",
    "section": "Today I Learned",
    "text": "Today I Learned\n\n\nGPU Programming Breakthrough\nFirst CUDA Kernel Success!\nBuilt and ran custom CUDA kernels on RTX 2050. Learned about parallel execution, compilation with ninja/nvcc, and discovered the 16384√ó16384 performance mystery.\nDeep dive ‚Üí"
  },
  {
    "objectID": "index.html#connect",
    "href": "index.html#connect",
    "title": "The Fire Hacker",
    "section": "Connect",
    "text": "Connect\n\nGitHub X/Twitter Contact"
  },
  {
    "objectID": "til.html",
    "href": "til.html",
    "title": "Today I Learned",
    "section": "",
    "text": "Deep dives into technical concepts I‚Äôm learning. From GPU programming to AI systems, these are comprehensive explorations of new technologies and techniques.\n\n\n\n\n\n\n\n\n\n\n\n\nMy First CUDA Kernel: Learning GPU Programming from Scratch\n\n\n\n\n\n\nThe Fire Hacker\n\n\nJan 16, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/timecapsule-slm.html",
    "href": "blog/timecapsule-slm.html",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "",
    "text": "TimeCapsule-SLM is an innovative AI-powered research and learning platform that‚Äôs revolutionizing how we discover knowledge and collaborate on research. Built with privacy-first principles and cutting-edge AI technology, it democratizes access to powerful research tools while keeping your data secure and local."
  },
  {
    "objectID": "blog/timecapsule-slm.html#introduction",
    "href": "blog/timecapsule-slm.html#introduction",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "",
    "text": "TimeCapsule-SLM is an innovative AI-powered research and learning platform that‚Äôs revolutionizing how we discover knowledge and collaborate on research. Built with privacy-first principles and cutting-edge AI technology, it democratizes access to powerful research tools while keeping your data secure and local."
  },
  {
    "objectID": "blog/timecapsule-slm.html#what-is-timecapsule-slm",
    "href": "blog/timecapsule-slm.html#what-is-timecapsule-slm",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "What is TimeCapsule-SLM?",
    "text": "What is TimeCapsule-SLM?\nTimeCapsule-SLM combines the power of Small Language Models with advanced research capabilities to create a comprehensive platform for:\n\nResearchers seeking AI-assisted discovery and pattern recognition\nStudents looking for adaptive, personalized learning experiences\nTeachers creating interactive educational content\nTeams collaborating on knowledge discovery\n\nThe platform addresses critical challenges in modern education and research: - Research fragmentation across multiple sources - Inefficient learning workflows - Privacy concerns with cloud-based AI - Limited AI integration in educational settings - Resource constraints in low-bandwidth environments"
  },
  {
    "objectID": "blog/timecapsule-slm.html#core-features",
    "href": "blog/timecapsule-slm.html#core-features",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Core Features",
    "text": "Core Features\n\nüß† DeepResearch TimeCapsule\nTransform your research workflow with multi-agent AI collaboration:\n\nAI-Powered Discovery: Generate novel research ideas and hypotheses\nPattern Recognition: Uncover hidden connections in your data\nMulti-Agent System: Leverage specialized AI agents for different research tasks\nCollaborative Intelligence: Combine human expertise with AI insights\n\n\n\nüé• AI-Frames Interactive Learning\nCreate immersive, adaptive learning experiences:\n\nSequential Learning Paths: Build structured knowledge journeys\nMultimodal Content: Integrate videos, documents, and interactive elements\nAI-Guided Explanations: Get personalized help when you need it\nSelf-Paced Progress: Learn at your own speed with AI support\n\n\n\nüìö In-Browser RAG (Retrieval-Augmented Generation)\nExperience the power of semantic search without compromising privacy:\n\nLocal Vector Store: All processing happens in your browser\nOffline Capability: Works without internet after initial model load\nSemantic Understanding: Find information based on meaning, not just keywords\nPrivacy-First Design: Your documents never leave your device"
  },
  {
    "objectID": "blog/timecapsule-slm.html#getting-started",
    "href": "blog/timecapsule-slm.html#getting-started",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Getting Started",
    "text": "Getting Started\n\nPrerequisites\nBefore installing TimeCapsule-SLM, ensure you have:\n# Node.js 18 or higher\nnode --version\n\n# npm or yarn package manager\nnpm --version\n\n# Git for cloning the repository\ngit --version\n\n\nInstallation\n\nClone the Repository:\n\ngit clone https://github.com/thefirehacker/TimeCapsule-SLM.git\ncd TimeCapsule-SLM\n\nInstall Dependencies:\n\nnpm install\n# or\nyarn install\n\nConfigure Environment:\n\ncp env.example .env.local\nEdit .env.local to configure your AI providers:\n# Optional: Add API keys for cloud models\nOPENAI_API_KEY=your_key_here\n\n# Local model configuration (Ollama)\nOLLAMA_HOST=http://localhost:11434\n\nStart the Development Server:\n\nnpm run dev\n# or\nyarn dev\nVisit http://localhost:3000 to access TimeCapsule-SLM!"
  },
  {
    "objectID": "blog/timecapsule-slm.html#setting-up-local-ai-models",
    "href": "blog/timecapsule-slm.html#setting-up-local-ai-models",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Setting Up Local AI Models",
    "text": "Setting Up Local AI Models\nFor the best privacy and offline experience, use local models with Ollama:\n\nInstall Ollama\n# macOS/Linux\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# Windows - Download from ollama.ai\n\n\nPull Recommended Models\n# For research and general tasks\nollama pull gemma:2b\n\n# For code and technical content\nollama pull qwen2.5:3b\n\n# For creative writing\nollama pull llama3.2:3b"
  },
  {
    "objectID": "blog/timecapsule-slm.html#using-timecapsule-slm",
    "href": "blog/timecapsule-slm.html#using-timecapsule-slm",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Using TimeCapsule-SLM",
    "text": "Using TimeCapsule-SLM\n\nCreating Your First Research Project\n\nInitialize a Knowledge Base:\n\nClick ‚ÄúNew Project‚Äù\nChoose your domain (Research, Education, Personal)\nSelect your preferred AI model\n\nImport Your Documents:\n\nDrag and drop PDFs, Word docs, or text files\nThe system will automatically index and embed them\nAll processing happens locally in your browser\n\nStart Researching:\n\nUse natural language queries to explore your knowledge base\nThe AI will surface relevant information and suggest connections\nGenerate summaries, insights, and new research directions\n\n\n\n\nBuilding AI-Frames for Learning\nCreate interactive learning experiences with AI-Frames:\n// Example AI-Frame configuration\n{\n  \"title\": \"Introduction to Quantum Computing\",\n  \"modules\": [\n    {\n      \"type\": \"video\",\n      \"content\": \"intro-video.mp4\",\n      \"ai_notes\": true\n    },\n    {\n      \"type\": \"interactive\",\n      \"content\": \"qubit-simulator\",\n      \"ai_guidance\": \"adaptive\"\n    },\n    {\n      \"type\": \"quiz\",\n      \"ai_generated\": true,\n      \"difficulty\": \"progressive\"\n    }\n  ]\n}\n\n\nCollaborative Features\nTimeCapsule-SLM supports real-time collaboration:\n\nShared Workspaces: Invite team members to research projects\nLive AI Sessions: Collaborate with AI assistance in real-time\nKnowledge Graphs: Visualize connections discovered by your team\nVersion Control: Track changes and contributions"
  },
  {
    "objectID": "blog/timecapsule-slm.html#architecture-technology",
    "href": "blog/timecapsule-slm.html#architecture-technology",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Architecture & Technology",
    "text": "Architecture & Technology\nTimeCapsule-SLM is built with modern, performant technologies:\n\nFrontend: Next.js 15, React 19, TypeScript\nAI Integration: Support for Ollama, OpenAI, and local models\nDatabase: RxDB for offline-first data persistence\nVector Store: In-browser embeddings with WebAssembly\nAuthentication: NextAuth.js for secure access"
  },
  {
    "objectID": "blog/timecapsule-slm.html#privacy-security",
    "href": "blog/timecapsule-slm.html#privacy-security",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Privacy & Security",
    "text": "Privacy & Security\nYour privacy is our priority:\n\nLocal-First: All sensitive processing happens on your device\nNo Telemetry: We don‚Äôt track your usage or collect data\nOpen Source: Audit the code yourself (Apache 2.0 License)\nEncryption: Local data is encrypted at rest\nControl: You decide what stays local vs.¬†what uses cloud services"
  },
  {
    "objectID": "blog/timecapsule-slm.html#use-cases",
    "href": "blog/timecapsule-slm.html#use-cases",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Use Cases",
    "text": "Use Cases\n\nFor Researchers\n\nLiterature reviews with AI-powered synthesis\nPattern discovery in research data\nHypothesis generation and validation\nCollaborative paper writing\n\n\n\nFor Students\n\nPersonalized study guides\nAI tutoring for complex topics\nInteractive learning paths\nExam preparation with adaptive quizzes\n\n\n\nFor Teachers\n\nCreate engaging course content\nBuild interactive lessons\nTrack student progress\nGenerate assessments automatically\n\n\n\nFor Teams\n\nKnowledge management\nCollaborative research\nTraining materials\nDocumentation with AI assistance"
  },
  {
    "objectID": "blog/timecapsule-slm.html#performance-tips",
    "href": "blog/timecapsule-slm.html#performance-tips",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Performance Tips",
    "text": "Performance Tips\nOptimize TimeCapsule-SLM for your hardware:\n\nModel Selection: Choose smaller models (2-3B parameters) for faster responses\nCaching: Enable browser caching for repeated queries\nBatch Processing: Process multiple documents simultaneously\nGPU Acceleration: Use WebGPU when available for faster inference"
  },
  {
    "objectID": "blog/timecapsule-slm.html#roadmap-future-features",
    "href": "blog/timecapsule-slm.html#roadmap-future-features",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Roadmap & Future Features",
    "text": "Roadmap & Future Features\nWe‚Äôre constantly improving TimeCapsule-SLM:\n\nMobile Apps: iOS and Android applications (Q2 2025)\nVoice Interface: Natural conversation with your knowledge base\nAdvanced Visualizations: 3D knowledge graphs and mind maps\nPlugin System: Extend functionality with custom modules\nFederated Learning: Collaborate without sharing raw data"
  },
  {
    "objectID": "blog/timecapsule-slm.html#contributing",
    "href": "blog/timecapsule-slm.html#contributing",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Contributing",
    "text": "Contributing\nTimeCapsule-SLM is open source and welcomes contributions:\n# Fork the repository\n# Create a feature branch\ngit checkout -b feature/amazing-feature\n\n# Make your changes\n# Run tests\nnpm test\n\n# Submit a pull request\nCheck our contribution guidelines for more details."
  },
  {
    "objectID": "blog/timecapsule-slm.html#community-support",
    "href": "blog/timecapsule-slm.html#community-support",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Community & Support",
    "text": "Community & Support\nJoin our growing community:\n\nGitHub Discussions: Technical questions and feature requests\nDiscord Server: Real-time chat with developers and users\nDocumentation: Comprehensive guides at timecapsule.bubblspace.com\nX/Twitter: Follow @thefirehacker for updates"
  },
  {
    "objectID": "blog/timecapsule-slm.html#conclusion",
    "href": "blog/timecapsule-slm.html#conclusion",
    "title": "Getting Started with TimeCapsule-SLM: AI-Powered Research & Learning Platform",
    "section": "Conclusion",
    "text": "Conclusion\nTimeCapsule-SLM represents a new paradigm in AI-assisted research and learning. By combining powerful AI capabilities with privacy-first design and local-first architecture, we‚Äôre making advanced research tools accessible to everyone.\nWhether you‚Äôre a researcher pushing the boundaries of knowledge, a student seeking personalized learning, or a teacher creating engaging content, TimeCapsule-SLM empowers you to work smarter, not harder.\nStart your journey today and experience the future of AI-powered research and learning!\n\nReady to transform your research and learning workflow? Get started with TimeCapsule-SLM or visit timecapsule.bubblspace.com for more information.\nHave questions or feedback? Reach out on X/Twitter or open an issue on GitHub."
  },
  {
    "objectID": "til/cuda-kernels-basics.html",
    "href": "til/cuda-kernels-basics.html",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "",
    "text": "Today I ran my first custom GPU code! Along with distributed pre-training AI models, I wanted to understand what‚Äôs happening at the low level. I was looking for a good resource to learn about kernel development for both inference & training. My interests were in MoE routing kernels, however I decided to start simple: compile and run kernels on local GPUs. I have a few gaming laptops and decided to run kernels on them.\nI decided to start with the simplest possible program: adding two vectors together. GPU Mode‚Äôs reference kernels are perfect to build a working end-to-end workflow."
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-beginning-why-learn-cuda",
    "href": "til/cuda-kernels-basics.html#the-beginning-why-learn-cuda",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "",
    "text": "Today I ran my first custom GPU code! Along with distributed pre-training AI models, I wanted to understand what‚Äôs happening at the low level. I was looking for a good resource to learn about kernel development for both inference & training. My interests were in MoE routing kernels, however I decided to start simple: compile and run kernels on local GPUs. I have a few gaming laptops and decided to run kernels on them.\nI decided to start with the simplest possible program: adding two vectors together. GPU Mode‚Äôs reference kernels are perfect to build a working end-to-end workflow."
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-program-overall-architecture",
    "href": "til/cuda-kernels-basics.html#the-program-overall-architecture",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "The Program: Overall Architecture",
    "text": "The Program: Overall Architecture\nBefore diving into the details, let me explain the complete program flow. We‚Äôll be working with the vectoradd_py problem from the reference-kernels repository.\n\nFile Structure & Purpose\nproblems/pmpp/vectoradd_py/\n‚îú‚îÄ‚îÄ run_local.py        # Main benchmark script\n‚îú‚îÄ‚îÄ submission.py       # Our custom CUDA kernel implementation\n‚îú‚îÄ‚îÄ reference.py        # Correctness checking\n‚îú‚îÄ‚îÄ task.py            # Data structure definitions\n‚îî‚îÄ‚îÄ README.md          # Problem description\nThe Execution Flow:\n\nrun_local.py - The orchestrator that:\n\nGenerates test data of various sizes\nCalls our custom kernel\nMeasures performance with CUDA events\nVerifies correctness against reference implementation\n\nsubmission.py - Contains our CUDA kernel using PyTorch‚Äôs inline compilation:\n\nCUDA C++ code written as Python strings\nCompiled on-the-fly using load_inline\nCreates a Python module we can call\n\nThe Magic: JIT Compilation Process\n\nWhen we use torch.utils.cpp_extension.load_inline, here‚Äôs what happens behind the scenes:\nYour Python Code\n        ‚Üì\ntorch.utils.cpp_extension.load_inline\n        ‚Üì\nGenerates .cpp and .cu source files\n        ‚Üì\nWrites build.ninja file\n        ‚Üì\nninja ‚Üí nvcc/g++ compile ‚Üí add_cuda.so\n        ‚Üì\ndlopen() loads .so into Python process\nThis is ahead-of-time compilation - once compiled, your kernel is fixed machine code running directly on the GPU!"
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-challenge-what-was-i-trying-to-achieve",
    "href": "til/cuda-kernels-basics.html#the-challenge-what-was-i-trying-to-achieve",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "The Challenge: What Was I Trying to Achieve?",
    "text": "The Challenge: What Was I Trying to Achieve?\nMy goal was simple but specific: 1. Write actual CUDA code that runs on my RTX 2050 laptop GPU 2. Understand how thousands of threads work together 3. Measure real performance and understand the numbers 4. Learn why GPUs are so powerful for AI workloads\nI found the perfect learning resource which I modified for my use: GPU Mode‚Äôs reference-kernels repository fork. It‚Äôs a collection of progressively harder GPU programming challenges, starting with vector addition."
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-big-picture-how-gpus-think-differently",
    "href": "til/cuda-kernels-basics.html#the-big-picture-how-gpus-think-differently",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "The Big Picture: How GPUs Think Differently",
    "text": "The Big Picture: How GPUs Think Differently\nBefore diving into code, here‚Äôs the mental shift that changed everything for me:\nCPU Thinking: ‚ÄúDo step 1, then step 2, then step 3‚Ä¶‚Äù GPU Thinking: ‚ÄúDo ALL the steps at once, everywhere!‚Äù\nImagine you need to paint 1000 fence posts. A CPU is like one very fast painter who paints each post perfectly, one after another. A GPU is like hiring 1000 amateur painters who each paint one post simultaneously. Even if each painter is slower, getting all posts done at once is way faster!\nFor vector addition (C = A + B), instead of:\nfor i in range(million):\n    C[i] = A[i] + B[i]  # One at a time\nThe GPU does:\nThread 0: C[0] = A[0] + B[0]\nThread 1: C[1] = A[1] + B[1]\nThread 2: C[2] = A[2] + B[2]\n... (all at the same time!)\nThread 999999: C[999999] = A[999999] + B[999999]"
  },
  {
    "objectID": "til/cuda-kernels-basics.html#setting-up-the-journey-to-hello-gpu",
    "href": "til/cuda-kernels-basics.html#setting-up-the-journey-to-hello-gpu",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "Setting Up: The Journey to ‚ÄúHello GPU‚Äù",
    "text": "Setting Up: The Journey to ‚ÄúHello GPU‚Äù\nGetting CUDA working on Windows with WSL2 was an adventure. Here‚Äôs what actually worked:\n\nStep 1: Install CUDA Toolkit\nFirst, I needed the CUDA compiler (nvcc) to turn my code into GPU instructions:\n# Get NVIDIA's official CUDA repository\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb\nsudo dpkg -i cuda-keyring_1.1-1_all.deb\nsudo apt-get update\nsudo apt-get install -y cuda-toolkit-12-1\n\n# Tell the system where CUDA lives\nexport CUDA_HOME=/usr/local/cuda-12.1\nexport PATH=$CUDA_HOME/bin:$PATH\n\n\nStep 2: PyTorch with CUDA Support\nPyTorch makes it easy to compile CUDA code on-the-fly:\npip install --index-url https://download.pytorch.org/whl/cu121 torch"
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-code-understanding-every-line",
    "href": "til/cuda-kernels-basics.html#the-code-understanding-every-line",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "The Code: Understanding Every Line",
    "text": "The Code: Understanding Every Line\nNow for the exciting part - the actual GPU code! Let me explain what each piece does and why it matters.\n\nThe GPU Kernel: Where the Magic Happens\ntemplate &lt;typename scalar_t&gt;\n__global__ void add_kernel(const scalar_t* __restrict__ A,\n                           const scalar_t* __restrict__ B,\n                           scalar_t* __restrict__ C,\n                           int N) {\n    // Who am I? Calculate my unique ID among thousands of threads\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // Am I responsible for a valid element?\n    if (idx &lt; N) {\n        C[idx] = A[idx] + B[idx];  // Do my one simple job\n    }\n}\nWhat‚Äôs happening here:\n\n__global__: This function runs on the GPU. It‚Äôs called from the CPU but executes on thousands of GPU cores simultaneously.\nThread Identity Crisis (solved!): Each thread needs to know which element to process. Think of it like a massive factory where each worker needs to know which item on the conveyor belt is theirs:\n\nthreadIdx.x: ‚ÄúI‚Äôm worker #5 in my team‚Äù\nblockIdx.x: ‚ÄúMy team is team #3‚Äù\nblockDim.x: ‚ÄúEach team has 256 workers‚Äù\nSo my global position is: 3 * 256 + 5 = 773 - I handle element 773!\n\nif (idx &lt; N): Safety first! We might launch more threads than we have data (for efficiency reasons), so each thread checks if it has real work to do.\n\n\n\nLaunching the Kernel: Mission Control\ntorch::Tensor add_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.numel();  // How many elements total?\n    auto C = torch::empty_like(A);  // Prepare output space\n\n    // Configure the thread army\n    const int threads = 256;  // Threads per block (team size)\n    const int blocks = (N + threads - 1) / threads;  // How many teams needed?\n\n    // LAUNCH! Send thousands of threads to work\n    add_kernel&lt;scalar_t&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(\n        A.data_ptr&lt;scalar_t&gt;(),\n        B.data_ptr&lt;scalar_t&gt;(),\n        C.data_ptr&lt;scalar_t&gt;(),\n        N\n    );\n}\nThe Strategy: - We organize threads into blocks (teams) of 256 threads each - Why 256? It‚Äôs a multiple of 32 (warp size - the GPU‚Äôs natural execution unit) - The &lt;&lt;&lt;blocks, threads&gt;&gt;&gt; syntax is CUDA‚Äôs special way to say ‚Äúlaunch this many blocks with this many threads each‚Äù\n\n\nThe Python Bridge: Making it Usable\nPyTorch‚Äôs load_inline is brilliant - it compiles CUDA code on-the-fly:\nadd_module = load_inline(\n    name='add_cuda',\n    cpp_sources=add_cpp_source,\n    cuda_sources=add_cuda_source,\n    functions=['add_cuda'],\n    verbose=True,  # Show me what's happening!\n)\nFirst time you run this, you‚Äôll see:\nDetected CUDA files, patching ldflags\nBuilding extension module add_cuda...\nninja: no work to do.\nLoading extension module add_cuda...\nThat‚Äôs nvcc compiling your GPU code into a Python module!"
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-benchmarking-measuring-reality",
    "href": "til/cuda-kernels-basics.html#the-benchmarking-measuring-reality",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "The Benchmarking: Measuring Reality",
    "text": "The Benchmarking: Measuring Reality\nThe run_local.py script does something clever - it automatically picks test sizes based on available GPU memory:\n# How much GPU memory is free?\nfree_bytes, _ = torch.cuda.mem_get_info()\nbudget = int(free_bytes * 0.8)  # Use 80% to be safe\n\n# For 2D matrices: need space for A, B, and C\ns_max = int(math.sqrt(budget / (3 * bytes_per_elem)))\nThis prevents the dreaded ‚ÄúCUDA out of memory‚Äù error!\n\nTiming GPU Code: It‚Äôs Tricky!\nYou can‚Äôt use regular Python timing for GPU code because GPU operations are asynchronous. The solution? CUDA Events:\nt0 = torch.cuda.Event(enable_timing=True)\nt1 = torch.cuda.Event(enable_timing=True)\n\nt0.record()              # Start timer ON THE GPU\ncustom_kernel(case)      # Run kernel\nt1.record()              # Stop timer ON THE GPU\ntorch.cuda.synchronize() # Wait for GPU to finish\nelapsed = t0.elapsed_time(t1)  # Get time in milliseconds"
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-results-what-i-learned-from-the-numbers",
    "href": "til/cuda-kernels-basics.html#the-results-what-i-learned-from-the-numbers",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "The Results: What I Learned from the Numbers",
    "text": "The Results: What I Learned from the Numbers\nRunning on my RTX 2050 (4GB VRAM):\nsize=4096:  mean=0.9877 ms    (67 million elements)\nsize=8192:  mean=3.8027 ms    (268 million elements)\nsize=12288: mean=8.5460 ms    (603 million elements)\nsize=16384: mean=150.3063 ms  (1.07 billion elements) ‚Üê WHAT?!\n\nThe Mystery of the Slow 16384\nWhy did 16384√ó16384 suddenly become 17x slower? This taught me a crucial lesson about GPU architecture:\nThe Problem: With 256 threads per block, processing 268,435,456 elements needs 1,048,576 blocks!\nThe GPU scheduler choked trying to manage over a million tiny work units. It‚Äôs like trying to manage a million separate construction crews for a project - the coordination overhead kills you!\nThe Solution: Grid-stride loops - have each thread process multiple elements:\nfor (int idx = blockIdx.x * blockDim.x + threadIdx.x;\n     idx &lt; N;\n     idx += blockDim.x * gridDim.x) {\n    C[idx] = A[idx] + B[idx];\n}\nNow you can cap blocks at a reasonable number (like 10,000) and each thread handles multiple elements.\n\n\nMemory Bandwidth: The Real Bottleneck\nFor the 8192√ó8192 case: - Data moved: 268M elements √ó 2 bytes √ó 3 arrays = 1.6 GB - Time: 3.8 ms - Bandwidth: 421 GB/s\nMy RTX 2050‚Äôs theoretical max is ~200 GB/s, so we‚Äôre doing great! Wait, how are we exceeding theoretical max? Cache! Some data gets reused from the GPU‚Äôs L2 cache."
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-revelations-what-changed-my-understanding",
    "href": "til/cuda-kernels-basics.html#the-revelations-what-changed-my-understanding",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "The Revelations: What Changed My Understanding",
    "text": "The Revelations: What Changed My Understanding\n\nGPUs are not fast CPUs - They‚Äôre a completely different beast. They‚Äôre terrible at complex branching logic but amazing at doing the same simple thing everywhere.\nMemory movement dominates - For simple operations like addition, you spend more time moving data than computing. This is why AI models use operations like matrix multiplication that do lots of compute per memory access.\nLaunch configuration matters hugely - Too many blocks? Scheduling overhead. Too few? Underutilization. It‚Äôs an art.\nThe power of parallel thinking - Once you start thinking ‚Äúwhat can happen simultaneously?‚Äù instead of ‚Äúwhat comes next?‚Äù, you see opportunities everywhere."
  },
  {
    "objectID": "til/cuda-kernels-basics.html#whats-next",
    "href": "til/cuda-kernels-basics.html#whats-next",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "What‚Äôs Next?",
    "text": "What‚Äôs Next?\nNow that I‚Äôve got basic kernels working, I‚Äôm excited to explore: - Shared memory: Using the 48KB of ultra-fast memory shared within each block - Warp-level operations: Leveraging the fact that 32 threads execute in lockstep - Reduction operations: How do you sum a billion numbers in parallel? - Matrix multiplication: The operation that powers all of deep learning"
  },
  {
    "objectID": "til/cuda-kernels-basics.html#resources-that-helped-me",
    "href": "til/cuda-kernels-basics.html#resources-that-helped-me",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "Resources That Helped Me",
    "text": "Resources That Helped Me\n\nReference Kernels Repo: Practice problems with increasing difficulty\nPMPP Book: ‚ÄúProgramming Massively Parallel Processors‚Äù - The theory behind it all\nGPU Mode Discord: Amazing community of people learning together\nCUDA Documentation: Surprisingly readable once you know the basics"
  },
  {
    "objectID": "til/cuda-kernels-basics.html#the-journey-continues",
    "href": "til/cuda-kernels-basics.html#the-journey-continues",
    "title": "My First CUDA Kernel: Learning GPU Programming from Scratch",
    "section": "The Journey Continues",
    "text": "The Journey Continues\nStarting with vector addition might seem trivial, but it opened the door to understanding how modern AI actually works at the hardware level. Every transformer model, every diffusion model, every neural network - they‚Äôre all built on these fundamental parallel operations.\nThe moment it clicked that my GPU was running 65,536 threads simultaneously, each doing their tiny part of the work, was magical. It‚Äôs not just faster computing - it‚Äôs a fundamentally different way of solving problems.\nNext week: I‚Äôm going to tackle matrix multiplication and see if I can beat PyTorch‚Äôs built-in implementation (spoiler: probably not, but I‚Äôll learn tons trying!).\n\nWant to try this yourself? Clone the reference-kernels repo and start with problems/pmpp/vectoradd_py/. The journey from CPU thinking to GPU thinking is worth it!"
  }
]