<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="The Fire Hacker">
<meta name="dcterms.date" content="2025-10-23">
<meta name="description" content="Learn why dictionary comprehensions in python elegantly transform HuggingFace data for models, how kwargs unpacking makes model(batch) ‚Äòjust work‚Äô, and why gradient averaging vs LR(Learning‚Äërate) scaling are equivalent in distributed training. Plus: build a mini DDP from scratch to see it all in action.">

<title>DDP from Scratch: a learner-friendly guide ‚Äì The Fire Hacker</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../assets/favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-a6e161b2431e1f94a14e0f5d32135a3c.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-fa3fe239c827a4974adb9ae70d601836.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-V1B8R98P79"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-V1B8R98P79', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">The Fire Hacker</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../til.html"> 
<span class="menu-text">Today I Learned</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About Me</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/thefirehacker"> <i class="bi bi-github" role="img" aria-label="GitHub">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/thefirehacker"> <i class="bi bi-twitter" role="img" aria-label="X (Twitter)">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#tldr" id="toc-tldr" class="nav-link active" data-scroll-target="#tldr">TL;DR</a></li>
  <li><a href="#visual-mental-model-of-distributed-training" id="toc-visual-mental-model-of-distributed-training" class="nav-link" data-scroll-target="#visual-mental-model-of-distributed-training">0) Visual mental model of distributed training</a></li>
  <li><a href="#seeding-making-model-replicas-identical" id="toc-seeding-making-model-replicas-identical" class="nav-link" data-scroll-target="#seeding-making-model-replicas-identical">1) Seeding: making model replicas identical</a>
  <ul class="collapse">
  <li><a href="#why-no-communication" id="toc-why-no-communication" class="nav-link" data-scroll-target="#why-no-communication">Why no communication?</a></li>
  </ul></li>
  <li><a href="#two-python-idioms-youll-see-everywhere" id="toc-two-python-idioms-youll-see-everywhere" class="nav-link" data-scroll-target="#two-python-idioms-youll-see-everywhere">2) Two Python idioms you‚Äôll see everywhere</a>
  <ul class="collapse">
  <li><a href="#dictionary-comprehension-why-we-need-this-pattern" id="toc-dictionary-comprehension-why-we-need-this-pattern" class="nav-link" data-scroll-target="#dictionary-comprehension-why-we-need-this-pattern">2.1 Dictionary comprehension ‚Äî Why we need this pattern</a></li>
  <li><a href="#kwargs-unpacking-with-the-huggingface-connection" id="toc-kwargs-unpacking-with-the-huggingface-connection" class="nav-link" data-scroll-target="#kwargs-unpacking-with-the-huggingface-connection">2.2 Kwargs unpacking with <code>**</code> ‚Äî The HuggingFace connection</a></li>
  <li><a href="#gradient-averaging-vs-learning-rate-scaling" id="toc-gradient-averaging-vs-learning-rate-scaling" class="nav-link" data-scroll-target="#gradient-averaging-vs-learning-rate-scaling">2.3 Gradient averaging vs learning-rate scaling ‚öñÔ∏è</a></li>
  </ul></li>
  <li><a href="#a-tiny-ddp-wrapper-teaching-version" id="toc-a-tiny-ddp-wrapper-teaching-version" class="nav-link" data-scroll-target="#a-tiny-ddp-wrapper-teaching-version">3) A tiny DDP wrapper (teaching version)</a></li>
  <li><a href="#minimal-distributed-training-loop" id="toc-minimal-distributed-training-loop" class="nav-link" data-scroll-target="#minimal-distributed-training-loop">4) Minimal distributed training loop</a>
  <ul class="collapse">
  <li><a href="#what-just-happened" id="toc-what-just-happened" class="nav-link" data-scroll-target="#what-just-happened">What just happened?</a></li>
  </ul></li>
  <li><a href="#why-broadcast-at-init-if-we-already-seed" id="toc-why-broadcast-at-init-if-we-already-seed" class="nav-link" data-scroll-target="#why-broadcast-at-init-if-we-already-seed">5) Why broadcast at init if we already seed?</a></li>
  <li><a href="#common-pitfalls-fixes" id="toc-common-pitfalls-fixes" class="nav-link" data-scroll-target="#common-pitfalls-fixes">6) Common pitfalls &amp; fixes</a></li>
  <li><a href="#from-toy-to-real-ddp" id="toc-from-toy-to-real-ddp" class="nav-link" data-scroll-target="#from-toy-to-real-ddp">7) From toy to real DDP</a></li>
  <li><a href="#exercises-recommended" id="toc-exercises-recommended" class="nav-link" data-scroll-target="#exercises-recommended">8) Exercises (recommended)</a></li>
  <li><a href="#cheatsheet" id="toc-cheatsheet" class="nav-link" data-scroll-target="#cheatsheet">9) Cheatsheet</a></li>
  <li><a href="#appendix-tiny-utilities" id="toc-appendix-tiny-utilities" class="nav-link" data-scroll-target="#appendix-tiny-utilities">10) Appendix: tiny utilities</a></li>
  <li><a href="#bonus-where-does-forward-come-from-with-automodel" id="toc-bonus-where-does-forward-come-from-with-automodel" class="nav-link" data-scroll-target="#bonus-where-does-forward-come-from-with-automodel">11) Bonus: Where does forward() come from with AutoModel?</a></li>
  <li><a href="#quick-reference-gradient-sync-patterns" id="toc-quick-reference-gradient-sync-patterns" class="nav-link" data-scroll-target="#quick-reference-gradient-sync-patterns">12) Quick Reference: Gradient sync patterns</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">DDP from Scratch: a learner-friendly guide</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Python basics</div>
    <div class="quarto-category">Today I Learned</div>
    <div class="quarto-category">Scratch to Scale</div>
    <div class="quarto-category">PyTorch</div>
    <div class="quarto-category">Distributed Training</div>
  </div>
  </div>

<div>
  <div class="description">
    Learn why dictionary comprehensions in python elegantly transform HuggingFace data for models, how <strong>kwargs unpacking makes model(</strong>batch) ‚Äòjust work‚Äô, and why gradient averaging vs LR(Learning‚Äërate) scaling are equivalent in distributed training. Plus: build a mini DDP from scratch to see it all in action.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>The Fire Hacker </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 23, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p>This note is part of the <strong>Scratch ‚Üí Scale</strong> series by <a href="https://x.com/TheZachMueller">Zachary Mueller</a> (<a href="https://maven.com/walk-with-code/scratch-to-scale">course link</a>). We‚Äôll implement a toy DDP wrapper, explain <em>why</em> it works, and demystify two Python idioms you‚Äôll see everywhere: dictionary comprehensions and <strong>kwargs</strong> (argument unpacking).</p>
</blockquote>
<section id="tldr" class="level2">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p><strong>üîë Core Python patterns explained:</strong></p>
<ul>
<li><strong>Dictionary comprehensions</strong>: Transform raw data (lists, ints) into model-ready tensors in one elegant line ‚Äî <code>{k: torch.tensor(v).to(device) for k, v in item.items()}</code> converts HuggingFace dataset samples to GPU tensors with proper shapes.</li>
<li><strong>Kwargs unpacking (<code>**</code>)</strong>: Unpack dictionaries into named function arguments ‚Äî <code>model(**batch)</code> automatically maps dict keys to HuggingFace model‚Äôs <code>forward()</code> parameters like <code>input_ids</code>, <code>attention_mask</code>, <code>labels</code>.</li>
<li><strong>Gradient averaging ‚öñÔ∏è learning rate scaling</strong>: Dividing gradients by <code>world_size</code> or scaling LR by <code>1/world_size</code> are mathematically equivalent ‚Äî the choice is where in your algorithm the division happens: before the optimizer step (average gradients) or after (scale learning rate).</li>
</ul>
<p><strong>üìã DDP essentials:</strong></p>
<ul>
<li>Seed <strong>every</strong> process the same way <strong>before</strong> you create the model.</li>
<li>Average grads with <code>dist.all_reduce(param.grad, op=SUM)</code> then divide by world size.</li>
<li>Use <code>**kwargs</code> to pass batches to models: <code>model(**batch)</code> works seamlessly with HuggingFace transformers.</li>
</ul>
<hr>
</section>
<section id="visual-mental-model-of-distributed-training" class="level2">
<h2 class="anchored" data-anchor-id="visual-mental-model-of-distributed-training">0) Visual mental model of distributed training</h2>
<pre><code>Rank 0 (GPU0)      Rank 1 (GPU1)      ...
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ forward      ‚îÇ   ‚îÇ forward      ‚îÇ  (same model weights)
‚îÇ loss.backward‚îÇ   ‚îÇ loss.backward‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ   grads            ‚îÇ   grads
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ all_reduce (SUM) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ (every rank gets sum of all grads)
                    ‚îÇ
              divide by world_size
                    ‚îÇ
                optimizer.step()</code></pre>
<hr>
</section>
<section id="seeding-making-model-replicas-identical" class="level2">
<h2 class="anchored" data-anchor-id="seeding-making-model-replicas-identical">1) Seeding: making model replicas identical</h2>
<p>Identical initialization across ranks is <strong>not optional</strong>. If rank 0 samples weights {W} and rank 1 samples different weights {W‚Äô}, averaging grads is meaningless. We seed each RNG <strong>per process</strong>, then construct the model.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> set_seed(seed: <span class="bu">int</span> <span class="op">=</span> <span class="dv">43</span>):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> random, numpy <span class="im">as</span> np, torch</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    random.seed(seed)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    np.random.seed(seed)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    torch.manual_seed(seed)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    torch.cuda.manual_seed_all(seed)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co"># In your entry point (each process runs this):</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>set_seed(<span class="dv">43</span>)            <span class="co"># must happen BEFORE model creation</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> build_model()   <span class="co"># identical on all ranks</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<section id="why-no-communication" class="level3">
<h3 class="anchored" data-anchor-id="why-no-communication">Why no communication?</h3>
<p>Each process runs the exact same Python code with the same seeds ‚Üí same random draws ‚Üí identical parameters. No dist.broadcast is <em>required</em> to make them equal, though you can use broadcast to <strong>enforce</strong> equality (see ¬ß3).</p>
<blockquote class="blockquote">
<p><strong>Pitfall</strong>: Seeding <strong>after</strong> constructing the model doesn‚Äôt retroactively change weights.</p>
</blockquote>
<hr>
</section>
</section>
<section id="two-python-idioms-youll-see-everywhere" class="level2">
<h2 class="anchored" data-anchor-id="two-python-idioms-youll-see-everywhere">2) Two Python idioms you‚Äôll see everywhere</h2>
<section id="dictionary-comprehension-why-we-need-this-pattern" class="level3">
<h3 class="anchored" data-anchor-id="dictionary-comprehension-why-we-need-this-pattern">2.1 Dictionary comprehension ‚Äî Why we need this pattern</h3>
<p>This line converts a HuggingFace dataset sample (lists/ints) into a batch dictionary of <strong>GPU tensors</strong> with an added batch dimension:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>item <span class="op">=</span> {k: torch.tensor(v).unsqueeze(<span class="dv">0</span>).to(device) <span class="cf">for</span> k, v <span class="kw">in</span> item.items()}</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Why this transformation is essential:</strong></p>
<p>HuggingFace datasets return items as <strong>Python dicts with lists and ints</strong>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>example <span class="op">=</span> {<span class="st">"input_ids"</span>: [<span class="dv">101</span>, <span class="dv">2023</span>, ...], <span class="st">"attention_mask"</span>: [<span class="dv">1</span>, <span class="dv">1</span>, ...], <span class="st">"labels"</span>: <span class="dv">0</span>}</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>But PyTorch models expect <strong>GPU tensors with batch dimensions</strong>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> {<span class="st">"input_ids"</span>: tensor([[<span class="dv">101</span>, <span class="dv">2023</span>, ...]], device<span class="op">=</span><span class="st">'cuda:0'</span>), ...}</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Why tensors are required:</strong></p>
<p>PyTorch models perform tensor operations (matrix multiplications, slicing, etc.) that require PyTorch tensor objects, not Python lists or integers. If you pass Python lists/ints directly, you‚Äôll get errors like: - <code>TypeError: expected Tensor as element 0 in argument 0, but got list</code> - <code>RuntimeError: Expected all tensors to be on the same device</code></p>
<p>The dictionary comprehension converts your data to the correct tensor format <strong>before</strong> passing it to the model. (See ¬ß2.2 for how these tensors flow through the model‚Äôs <code>forward()</code> method.)</p>
<p>The dictionary comprehension does <strong>three transformations in one line</strong>:</p>
<ol type="1">
<li><strong>Preserve structure</strong>: Keep the same dict keys (<code>input_ids</code>, <code>attention_mask</code>, etc.)</li>
<li><strong>Convert types</strong>: List/int ‚Üí PyTorch tensor ‚Üí GPU tensor</li>
<li><strong>Add batch dimension</strong>: Shape <code>(seq_len,)</code> ‚Üí <code>(1, seq_len)</code> for batching</li>
</ol>
<p><strong>Breakdown:</strong> * <code>for k, v in item.items()</code> ‚Üí iterates over each key-value pair * <code>torch.tensor(v)</code> ‚Üí converts list/int to tensor * <code>.unsqueeze(0)</code> ‚Üí adds batch dimension: <code>[a, b, c]</code> ‚Üí <code>[[a, b, c]]</code> * <code>.to(device)</code> ‚Üí moves to GPU</p>
<p><strong>Without</strong> this transformation, you‚Äôd pass Python lists/CPU arrays to the model, which would either error or require slow implicit conversion on each forward pass.</p>
<section id="alternative-generator-based-streaming-with-yield" class="level4">
<h4 class="anchored" data-anchor-id="alternative-generator-based-streaming-with-yield">Alternative: Generator-based streaming with <code>yield</code></h4>
<p>For <strong>large datasets</strong> or <strong>memory-constrained</strong> scenarios, dictionary comprehensions can be memory-intensive (they build the entire dict in memory). A better approach uses <strong>generators with <code>yield</code></strong> for lazy evaluation:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> stream_to_device(item, device):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Generator that yields tensors one at a time - memory efficient"""</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> item.items():</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">yield</span> k, torch.tensor(v).unsqueeze(<span class="dv">0</span>).to(device)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Usage: build dict lazily</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> <span class="bu">dict</span>(stream_to_device(example, device))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Why generators are better for large data:</strong> * <strong>Lazy evaluation</strong>: Tensors are created and moved to GPU one at a time, not all at once. * <strong>Lower memory footprint</strong>: Only one tensor exists in memory during transformation. * <strong>Scalable</strong>: Works with datasets that don‚Äôt fit in RAM.</p>
<p><strong>When to use each:</strong> * <strong>Dict comprehension</strong>: Small to medium batches, simple one-liners, readable code. * <strong>Generator with <code>yield</code></strong>: Large datasets, streaming data, memory-constrained environments, production pipelines.</p>
</section>
</section>
<section id="kwargs-unpacking-with-the-huggingface-connection" class="level3">
<h3 class="anchored" data-anchor-id="kwargs-unpacking-with-the-huggingface-connection">2.2 Kwargs unpacking with <code>**</code> ‚Äî The HuggingFace connection</h3>
<p>Given <code>item = {"input_ids": X, "attention_mask": Y, "labels": Z}</code>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> model(<span class="op">**</span>item)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co"># exactly the same as:</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> model(input_ids<span class="op">=</span>X, attention_mask<span class="op">=</span>Y, labels<span class="op">=</span>Z)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Why this matters for HuggingFace models:</strong></p>
<p>The <code>**</code> operator unpacks a dict into <strong>named arguments</strong> that match your model‚Äôs <code>forward()</code> signature. This is why HuggingFace workflows are so elegant:</p>
<ol type="1">
<li><strong>Dataset has standard keys</strong>: HuggingFace datasets/tokenizers output dicts with keys like <code>"input_ids"</code>, <code>"attention_mask"</code>, <code>"labels"</code>.</li>
<li><strong>Model expects those keys</strong>: All HuggingFace models have a <code>forward()</code> method that accepts these exact parameter names.</li>
<li><strong><code>**kwargs</code> bridges them</strong>: Instead of manually extracting each key, <code>model(**batch)</code> automatically maps dict keys to function parameters.</li>
</ol>
<p><strong>Without <code>**kwargs</code> (manual, verbose):</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> model(</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    input_ids<span class="op">=</span>batch[<span class="st">"input_ids"</span>],</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    attention_mask<span class="op">=</span>batch[<span class="st">"attention_mask"</span>],</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    labels<span class="op">=</span>batch[<span class="st">"labels"</span>]</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>With <code>**kwargs</code> (clean, scalable):</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> model(<span class="op">**</span>batch)  <span class="co"># Automatically maps all keys!</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This works because HuggingFace models define their <code>forward()</code> signature to match the standard dataset keys. It‚Äôs a deliberate design pattern that makes training code incredibly clean.</p>
<p><strong>Tracing the forward() call chain:</strong></p>
<p>When you call <code>model(**batch)</code>, the unpacked tensors flow through the model‚Äôs forward pass. Here‚Äôs the call chain for <code>AutoModelForSequenceClassification</code>:</p>
<pre><code>model(**batch)  # batch contains tensors: {"input_ids": tensor(...), ...}
    ‚Üì
AutoModelForSequenceClassification.from_pretrained(...)
    ‚Üì
SmolLM2ForSequenceClassification  # concrete architecture class
    ‚Üì
GenericForSequenceClassification.forward(**kwargs)
    ‚Üì
    # forward() signature receives unpacked kwargs:
    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):
        outputs = self.model(input_ids, attention_mask=attention_mask, **kwargs)
        #              ‚Üë **kwargs unpacking maps dict keys to these parameters
        pooled = outputs[0][:, 0, :]  # CLS token pooling
        logits = self.score(pooled)   # linear classifier head
        loss = self.loss_fn(logits, labels) if labels is not None else None
        return SequenceClassifierOutput(loss=loss, logits=logits, ...)</code></pre>
<p><strong>Key insight</strong>: The <code>**batch</code> unpacking automatically maps dictionary keys (<code>"input_ids"</code>, <code>"attention_mask"</code>, <code>"labels"</code>) to the <code>forward()</code> method‚Äôs parameter names. This is why <code>model(**batch)</code> works seamlessly ‚Äî the keys match the function signature exactly.</p>
</section>
<section id="gradient-averaging-vs-learning-rate-scaling" class="level3">
<h3 class="anchored" data-anchor-id="gradient-averaging-vs-learning-rate-scaling">2.3 Gradient averaging vs learning-rate scaling ‚öñÔ∏è</h3>
<p><strong>This is a key insight</strong>: When training on N GPUs, you have two mathematically equivalent options for combining gradients:</p>
<section id="option-a-average-gradients-most-common" class="level4">
<h4 class="anchored" data-anchor-id="option-a-average-gradients-most-common">Option A: Average gradients (most common)</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># After backward on each rank</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>dist.all_reduce(param.grad, op<span class="op">=</span>SUM)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>param.grad <span class="op">/=</span> world_size  <span class="co"># Average the gradients</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimizer update with normal LR</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>optimizer.step()  <span class="co"># uses original learning rate</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="option-b-sum-gradients-scale-learning-rate" class="level4">
<h4 class="anchored" data-anchor-id="option-b-sum-gradients-scale-learning-rate">Option B: Sum gradients, scale learning rate</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># After backward on each rank  </span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>dist.all_reduce(param.grad, op<span class="op">=</span>SUM)  <span class="co"># Keep summed gradients</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimizer update with scaled LR</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> param <span class="kw">in</span> model.parameters():</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    param.data <span class="op">-=</span> (lr <span class="op">/</span> world_size) <span class="op">*</span> param.grad</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Why they‚Äôre equivalent:</strong></p>
<p><span class="math display">\[
\text{param} - \text{lr} \times \frac{\text{grad}}{N} = \text{param} - \frac{\text{lr}}{N} \times \text{grad}
\]</span></p>
<p><strong>Real-world implications:</strong> * <strong>PyTorch DDP</strong>: Uses Option A (averages gradients), so you keep your learning rate unchanged. * <strong>Some frameworks</strong> (Horovod, older examples): Use Option B (sum gradients), expecting you to scale LR by <code>1/world_size</code>. * <strong>The division can happen in two places</strong>: before the optimizer step (average gradients during sync) or after (scale learning rate during optimizer step) ‚Äî same math, different location in the algorithm.</p>
<p><strong>Practical tip:</strong> The instructor‚Äôs comment ‚Äúit depends where in the algorithm you want the averaging‚Äù refers to this choice. Most modern code averages gradients (Option A) because it‚Äôs cleaner and doesn‚Äôt require you to remember to scale the learning rate.</p>
<hr>
</section>
</section>
</section>
<section id="a-tiny-ddp-wrapper-teaching-version" class="level2">
<h2 class="anchored" data-anchor-id="a-tiny-ddp-wrapper-teaching-version">3) A tiny DDP wrapper (teaching version)</h2>
<p>This wrapper <strong>(a)</strong> verifies parameter equality at init (optionally enforces it) and <strong>(b)</strong> averages gradients after <code>backward()</code>.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.distributed <span class="im">as</span> dist</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MiniDDP:</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model: torch.nn.Module, enforce_broadcast: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>):</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.world_size <span class="op">=</span> dist.get_world_size() <span class="cf">if</span> dist.is_initialized() <span class="cf">else</span> <span class="dv">1</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># --- verify / enforce identical params across ranks ---</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.model.parameters():</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>            <span class="co"># create a rank0 copy to compare/broadcast</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>            rank0_buf <span class="op">=</span> p.detach().clone()</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>            dist.broadcast(rank0_buf, src<span class="op">=</span><span class="dv">0</span>)     <span class="co"># everyone receives rank0's tensor</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> enforce_broadcast:</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>                p.data.copy_(rank0_buf)          <span class="co"># enforce equality (optional)</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="kw">not</span> torch.equal(p.data, rank0_buf):</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">raise</span> <span class="pp">ValueError</span>(</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>                        <span class="st">"Parameters differ at init. Seed all ranks BEFORE model construction, "</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>                        <span class="st">"or set enforce_broadcast=True."</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>                    )</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, <span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.model(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> average_grads(<span class="va">self</span>):</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.world_size <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.model.parameters():</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> p.grad <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>            dist.all_reduce(p.grad, op<span class="op">=</span>dist.ReduceOp.SUM)</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>            p.grad.div_(<span class="va">self</span>.world_size)</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># convenience passthroughs</span></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> train(<span class="va">self</span>):</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model.train()</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="bu">eval</span>(<span class="va">self</span>):</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model.<span class="bu">eval</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Understanding <code>enforce_broadcast</code>:</strong></p>
<p>The <code>enforce_broadcast</code> parameter controls how parameter synchronization is handled at initialization:</p>
<ol type="1">
<li><p><strong><code>enforce_broadcast=False</code> (default)</strong>: <strong>Verifies</strong> that all ranks already have identical parameters (e.g., via seeding). If parameters differ, it raises an error. This is the ‚Äútrust but verify‚Äù approach ‚Äî you‚Äôre responsible for ensuring equality (via seeding), and the wrapper checks that you did it correctly.</p></li>
<li><p><strong><code>enforce_broadcast=True</code></strong>: <strong>Forces</strong> all ranks to use rank 0‚Äôs parameters by overwriting each rank‚Äôs parameters with rank 0‚Äôs values. This is the ‚Äúbelt and suspenders‚Äù approach ‚Äî even if seeding failed or parameters diverged, everyone gets rank 0‚Äôs exact state.</p></li>
</ol>
<p><strong>Why this mirrors PyTorch‚Äôs official DDP:</strong></p>
<p>PyTorch‚Äôs <code>DistributedDataParallel</code> <strong>always</strong> performs parameter synchronization at initialization (like <code>enforce_broadcast=True</code>), but it does so <strong>internally, automatically, and efficiently</strong>: - It broadcasts parameters from rank 0 to all other ranks during construction - It handles buffers (like BatchNorm running stats) as well - It uses optimized communication patterns (coalesced broadcasts, bucketing)</p>
<p>This initial synchronization is a core part of DDP‚Äôs design to ensure all model replicas start with identical weights. As documented in the <a href="https://docs.pytorch.org/docs/stable/notes/ddp.html">PyTorch DDP notes</a>: <em>‚ÄúWhen a model is wrapped with DDP, the constructor synchronizes the model‚Äôs parameters across all processes. This is achieved by broadcasting the parameters from the process with rank 0 to all other processes.‚Äù</em></p>
<p><strong>Key difference</strong>: In PyTorch‚Äôs DDP, this synchronization happens <strong>automatically in the constructor</strong> ‚Äî there‚Äôs no user-facing parameter to control it. It‚Äôs an internal implementation detail that ensures correctness.</p>
<p>In <code>MiniDDP</code>, we make this synchronization <strong>explicit and optional</strong> so you can: - See exactly what‚Äôs happening (educational value) - Choose to verify vs.&nbsp;enforce (learning about seeding) - Understand the tradeoffs between verification and enforcement</p>
<blockquote class="blockquote">
<p>This mirrors what PyTorch‚Äôs official <code>DistributedDataParallel</code> does conceptually, but without bucketing, overlap, or autograd hooks. Perfect for learning; use the real DDP for production.</p>
</blockquote>
<hr>
</section>
<section id="minimal-distributed-training-loop" class="level2">
<h2 class="anchored" data-anchor-id="minimal-distributed-training-loop">4) Minimal distributed training loop</h2>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># torchrun --nproc_per_node=2 train.py</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os, torch, torch.distributed <span class="im">as</span> dist</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.optim <span class="im">import</span> Adam</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForSequenceClassification, AutoTokenizer</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>MODEL <span class="op">=</span> <span class="st">"HuggingFaceTB/SmolLM2-360M-Instruct"</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> set_seed(seed<span class="op">=</span><span class="dv">43</span>):</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> random, numpy <span class="im">as</span> np</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    random.seed(seed)<span class="op">;</span> np.random.seed(seed)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    torch.manual_seed(seed)<span class="op">;</span> torch.cuda.manual_seed_all(seed)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main():</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    dist.init_process_group(<span class="st">"nccl"</span>)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    rank  <span class="op">=</span> dist.get_rank()<span class="op">;</span>  local_rank <span class="op">=</span> <span class="bu">int</span>(os.environ.get(<span class="st">"LOCAL_RANK"</span>, <span class="dv">0</span>))</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> torch.device(<span class="ss">f"cuda:</span><span class="sc">{</span>local_rank<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>    set_seed(<span class="dv">43</span>)  <span class="co"># same on every process BEFORE creating the model</span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>    tok <span class="op">=</span> AutoTokenizer.from_pretrained(MODEL)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> AutoModelForSequenceClassification.from_pretrained(</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>        MODEL, num_labels<span class="op">=</span><span class="dv">2</span>, torch_dtype<span class="op">=</span>torch.bfloat16</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>    ).to(device)</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>    ddp <span class="op">=</span> MiniDDP(model, enforce_broadcast<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>    opt <span class="op">=</span> Adam(ddp.model.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>)</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>    ds <span class="op">=</span> load_dataset(<span class="st">"glue"</span>, <span class="st">"mrpc"</span>)</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> encode(ex):</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> tok(ex[<span class="st">"sentence1"</span>], ex[<span class="st">"sentence2"</span>], padding<span class="op">=</span><span class="va">True</span>, truncation<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>    ds <span class="op">=</span> ds.<span class="bu">map</span>(encode, batched<span class="op">=</span><span class="va">True</span>).rename_columns({<span class="st">"label"</span>: <span class="st">"labels"</span>})</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># toy per-rank sample (one example per rank to show divergence if not averaged)</span></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>    example <span class="op">=</span> ds[<span class="st">"train"</span>][rank]</span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>    batch <span class="op">=</span> {k: torch.tensor(v).unsqueeze(<span class="dv">0</span>).to(device) <span class="cf">for</span> k, v <span class="kw">in</span> example.items()}</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>    ddp.train()</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> ddp(<span class="op">**</span>batch)         <span class="co"># kwargs unpacking</span></span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a>    out.loss.backward()</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>    ddp.average_grads()        <span class="co"># &lt;‚Äî key! average across ranks</span></span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a>    opt.step()<span class="op">;</span> opt.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> rank <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"step ok; loss:"</span>, out.loss.item())</span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a>    dist.destroy_process_group()</span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a>    main()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<section id="what-just-happened" class="level3">
<h3 class="anchored" data-anchor-id="what-just-happened">What just happened?</h3>
<ul>
<li>Both ranks ran the same code and created identical models (thanks to seeding).</li>
<li>Each rank used a <strong>different</strong> example (rank index) ‚Üí losses differ initially.</li>
<li><code>average_grads()</code> made every GPU apply the <strong>same averaged update</strong>, keeping replicas in lock‚Äëstep.</li>
</ul>
<hr>
</section>
</section>
<section id="why-broadcast-at-init-if-we-already-seed" class="level2">
<h2 class="anchored" data-anchor-id="why-broadcast-at-init-if-we-already-seed">5) Why broadcast at init if we already seed?</h2>
<p>Seeding should guarantee equality. The <code>broadcast</code> operation (when <code>enforce_broadcast=True</code>) is a belt‚Äëand‚Äësuspenders option:</p>
<ul>
<li><strong>Protect against forgotten seeds</strong>: If you forgot to seed on some ranks, broadcast ensures everyone still starts identical.</li>
<li><strong>Handle divergent code paths</strong>: If different ranks take different initialization paths, broadcast syncs them.</li>
<li><strong>Deal with non‚Äëdeterministic ops</strong>: Some operations (e.g., certain CUDA kernels) may not be fully deterministic even with seeds.</li>
<li><strong>Enable joining late ranks</strong>: If a rank joins after initialization, broadcast can sync it to the current state from rank 0.</li>
</ul>
<p><strong>In practice</strong>: With proper seeding (see ¬ß1), <code>enforce_broadcast=False</code> (verify mode) is usually sufficient. Use <code>enforce_broadcast=True</code> only if you <em>intend</em> to force‚Äësync weights at init or are debugging initialization issues.</p>
<p><strong>Note</strong>: PyTorch‚Äôs official DDP always performs this synchronization automatically (equivalent to <code>enforce_broadcast=True</code>), but hides it from you. <code>MiniDDP</code> makes it explicit so you can learn about the mechanism.</p>
<hr>
</section>
<section id="common-pitfalls-fixes" class="level2">
<h2 class="anchored" data-anchor-id="common-pitfalls-fixes">6) Common pitfalls &amp; fixes</h2>
<ul>
<li><strong>Different seeds / seeding too late</strong> ‚Üí parameters differ. <em>Fix</em>: call <code>set_seed()</code> before <code>build_model()</code> on every rank.</li>
<li><strong>Forgetting to divide after <code>all_reduce(SUM)</code></strong> ‚Üí LR effectively <code>√ó world_size</code>. <em>Fix</em>: divide grads (or use <code>op=AVG</code> on newer APIs like <code>reduce_scatter_tensor</code>).</li>
<li><strong>Grad is <code>None</code></strong>: layers not used in the forward didn‚Äôt receive gradients. <em>Fix</em>: check the graph; guard <code>if p.grad is None: continue</code>.</li>
<li><strong>CPU tensors in batch</strong>: model expects CUDA tensors. <em>Fix</em>: dictionary comprehension that moves tensors to <code>device</code>.</li>
<li><strong>Shape mismatches across ranks</strong>: ensure each rank‚Äôs micro‚Äëbatch has identical shapes (padding or a proper <code>DistributedSampler</code>).</li>
<li><strong>NCCL init errors</strong>: set <code>MASTER_ADDR/PORT</code>, unique <code>RANK</code>, correct <code>CUDA_VISIBLE_DEVICES</code>.</li>
</ul>
<hr>
</section>
<section id="from-toy-to-real-ddp" class="level2">
<h2 class="anchored" data-anchor-id="from-toy-to-real-ddp">7) From toy to real DDP</h2>
<p>What we built is the core idea. Production <code>torch.nn.parallel.DistributedDataParallel</code> adds:</p>
<ul>
<li>gradient bucketing and overlap with communication;</li>
<li>parameter and buffer broadcast on construction (with versioning);</li>
<li>autograd hooks for exact timing;</li>
<li>mixed precision, static graph optimizations, etc.</li>
</ul>
<p><strong>Upgrade path</strong>: once you grasp the flow above, swap <code>MiniDDP</code> for <code>DistributedDataParallel(model, device_ids=[local_rank])</code> and use <code>DistributedSampler</code> in your <code>DataLoader</code>.</p>
<hr>
</section>
<section id="exercises-recommended" class="level2">
<h2 class="anchored" data-anchor-id="exercises-recommended">8) Exercises (recommended)</h2>
<ol type="1">
<li><strong>Fail fast</strong>: comment out <code>set_seed(43)</code> and watch the init check throw. Then set <code>enforce_broadcast=True</code> and observe it succeed.</li>
<li><strong>Batching</strong>: replace the single‚Äëexample hack with a <code>DataLoader</code> + <code>DistributedSampler</code>. Verify all ranks consume disjoint shards.</li>
<li><strong>Reduce‚Äëscatter</strong>: re‚Äëimplement <code>average_grads()</code> with <code>reduce_scatter_tensor</code> + <code>all_gather_into_tensor</code> to mimic optimizer sharding.</li>
<li><strong>Kwargs drill</strong>: write a wrapper that logs which kwargs are passed through (<code>*args</code>, <code>**kwargs</code>) and rejects unknown keys.</li>
<li><strong>Determinism</strong>: enable CUDA deterministic flags and compare speed/behavior.</li>
</ol>
<hr>
</section>
<section id="cheatsheet" class="level2">
<h2 class="anchored" data-anchor-id="cheatsheet">9) Cheatsheet</h2>
<ul>
<li><code>item = {k: f(v) for k, v in d.items()}</code> ‚Üí dictionary comprehension.</li>
<li><code>model(**d)</code> ‚Üí unpack <code>d</code> into named arguments to <code>forward</code>.</li>
<li><code>dist.all_reduce(t, SUM); t /= world_size</code> ‚Üí average a tensor across ranks.</li>
<li>Seed <em>before</em> model creation on <strong>every</strong> process.</li>
<li>If in doubt, force-sync params once with <code>broadcast</code>.</li>
</ul>
<hr>
</section>
<section id="appendix-tiny-utilities" class="level2">
<h2 class="anchored" data-anchor-id="appendix-tiny-utilities">10) Appendix: tiny utilities</h2>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> recursively_apply(func, data):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(data, (<span class="bu">tuple</span>, <span class="bu">list</span>)):</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">type</span>(data)(recursively_apply(func, x) <span class="cf">for</span> x <span class="kw">in</span> data)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(data, <span class="bu">dict</span>):</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {k: recursively_apply(func, v) <span class="cf">for</span> k, v <span class="kw">in</span> data.items()}</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> func(data)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: move a nested batch to device</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> recursively_apply(<span class="kw">lambda</span> t: t.to(device) <span class="cf">if</span> <span class="bu">isinstance</span>(t, torch.Tensor) <span class="cf">else</span> t, batch)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="bonus-where-does-forward-come-from-with-automodel" class="level2">
<h2 class="anchored" data-anchor-id="bonus-where-does-forward-come-from-with-automodel">11) Bonus: Where does forward() come from with AutoModel?</h2>
<p>When we wrote:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels<span class="op">=</span><span class="dv">2</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>that helper inspects the model config and dispatches to the architecture‚Äëspecific <code>...ForSequenceClassification</code> class. For the SmolLM family, that class inherits a generic head that already implements <code>forward()</code>.</p>
<p>Call chain at runtime (conceptual):</p>
<ul>
<li>AutoModelForSequenceClassification ‚Üí ArchitectureForSequenceClassification ‚Üí GenericForSequenceClassification.forward(**kwargs) ‚Üí ArchitectureModel.forward(‚Ä¶) ‚Üí CLS pooling ‚Üí classifier head (<code>self.score</code>) ‚Üí loss (if labels)</li>
</ul>
<p>Minimal shape of that <code>forward()</code>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GenericForSequenceClassification(PreTrainedModel):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, input_ids<span class="op">=</span><span class="va">None</span>, attention_mask<span class="op">=</span><span class="va">None</span>, labels<span class="op">=</span><span class="va">None</span>, <span class="op">**</span>kwargs):</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> <span class="va">self</span>.model(input_ids, attention_mask<span class="op">=</span>attention_mask, <span class="op">**</span>kwargs)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>        pooled <span class="op">=</span> outputs[<span class="dv">0</span>][:, <span class="dv">0</span>, :]</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.score(pooled)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="va">self</span>.loss_fn(logits, labels) <span class="cf">if</span> labels <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> SequenceClassifierOutput(loss<span class="op">=</span>loss, logits<span class="op">=</span>logits, hidden_states<span class="op">=</span>outputs.hidden_states)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This is why <code>model(**batch)</code> (see ¬ß2.2) ‚Äújust works‚Äù: the dict keys map to the generic <code>forward()</code> signature, which calls the backbone‚Äôs <code>forward()</code> under the hood.</p>
<hr>
<p>Happy scaling! If you‚Äôre following the course, tag this post as <strong>TIL/DDP‚Äëfrom‚Äëscratch</strong> and iterate from here. üß™üöÄ</p>
</section>
<section id="quick-reference-gradient-sync-patterns" class="level2">
<h2 class="anchored" data-anchor-id="quick-reference-gradient-sync-patterns">12) Quick Reference: Gradient sync patterns</h2>
<p><strong>Summary of the two equivalent approaches</strong> (see ¬ß2.3 for full explanation):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Pattern A: Average gradients (PyTorch DDP default)</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>dist.all_reduce(param.grad, op<span class="op">=</span>SUM)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>param.grad <span class="op">/=</span> world_size</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>param <span class="op">-=</span> lr <span class="op">*</span> param.grad  <span class="co"># Original LR</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Pattern B: Sum gradients, scale LR (Horovod-style)</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>dist.all_reduce(param.grad, op<span class="op">=</span>SUM)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>param <span class="op">-=</span> (lr <span class="op">/</span> world_size) <span class="op">*</span> param.grad  <span class="co"># Scaled LR</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Key takeaway:</strong> Both produce identical updates. Choose Pattern A for cleaner code that matches PyTorch DDP defaults.</p>


</section>

</main> <!-- /main -->
<!-- Enhanced Google Analytics 4 Implementation for The Fire Hacker -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-V1B8R98P79"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  
  // Initialize GA4 with enhanced configuration
  gtag('js', new Date());
  gtag('config', 'G-V1B8R98P79', {
    // Enhanced ecommerce and engagement tracking
    send_page_view: true,
    page_title: document.title,
    page_location: window.location.href,
    
    // Enhanced measurement features
    enhanced_measurement: {
      scrolls: true,
      outbound_clicks: true,
      site_search: true,
      video_engagement: true,
      file_downloads: true
    },
    
    // Custom parameters for The Fire Hacker
    custom_map: {
      'dimension1': 'page_type',
      'dimension2': 'content_category',
      'dimension3': 'user_engagement_level'
    },
    
    // Debug mode (remove in production if needed)
    debug_mode: false
  });

  // Set custom dimensions based on page
  const pageType = window.location.pathname.includes('/blog/') ? 'blog_post' :
                   window.location.pathname.includes('/til/') ? 'til_post' :
                   window.location.pathname === '/' || window.location.pathname === '/index.html' ? 'homepage' :
                   window.location.pathname.includes('/about') ? 'about' :
                   'other';

  const contentCategory = window.location.pathname.includes('/blog/') ? 'blog' :
                         window.location.pathname.includes('/til/') ? 'today_i_learned' :
                         'main_site';

  // Send custom dimensions
  gtag('config', 'G-V1B8R98P79', {
    'custom_map.dimension1': pageType,
    'custom_map.dimension2': contentCategory
  });

  // Enhanced page view with custom parameters
  gtag('event', 'page_view', {
    'page_type': pageType,
    'content_category': contentCategory,
    'site_name': 'The Fire Hacker',
    'author': 'Fire Hacker',
    'contact_email': 'firehacker@bubblspace.com'
  });

  // Track mailto clicks to firehacker@bubblspace.com
  document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('a[href^="mailto:firehacker@bubblspace.com"]').forEach(function(link) {
      link.addEventListener('click', function() {
        gtag('event', 'email_contact', {
          'contact_email': 'firehacker@bubblspace.com',
          'contact_method': 'mailto_link'
        });
      });
    });
  });
</script>

<!-- Load enhanced tracking script -->
<script src="analytics.js"></script>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/thefirehacker\.github\.io");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // default icon
            link.classList.add("external");
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>